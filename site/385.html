
<!DOCTYPE html>
<html class="scroll-smooth" lang="en-US" data-theme="light">
<head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, shrink-to-fit=no" name="viewport">
    <meta content="IE=edge" http-equiv="X-UA-Compatible">
    <title>Page 385 â€¢ Relatively General .NET</title>
    <link href="favicon.ico" rel="icon" sizes="any">
    <link href="images/apple-touch-icon.png" rel="apple-touch-icon">
    <meta content="hsl()" name="theme-color">
    <meta content="website" property="og:type">
    <meta content="Home" property="og:title">
    <link rel="stylesheet" href="css/styles.css">
    <link rel="stylesheet" href="pagefind/pagefind-ui.css">
</head>
<body class="mx-auto flex min-h-screen max-w-3xl flex-col bg-bgColor px-4 pt-16 font-mono text-sm font-normal text-textColor antialiased sm:px-8">

<a class="sr-only focus:not-sr-only focus:fixed focus:start-1 focus:top-1.5" href="#main">
    skip to content
</a>
<header class="group relative mb-28 flex items-center sm:ps-[4.5rem]" id="main-header">
    <div class="flex sm:flex-col">
        <a aria-current="page" class="inline-flex items-center hover:filter-none sm:relative sm:inline-block"
           href="index.html">
            <img class="me-3 sm:absolute sm:start-[-4.5rem] sm:me-0 sm:h-16 sm:w-16 w-16" src="images/giphy.gif"
                 alt=""/>
            <span class="text-xl font-bold sm:text-2xl">Relatively General .NET</span>
        </a>
        <nav aria-label="Main menu"
             class="absolute -inset-x-4 top-14 hidden flex-col items-end gap-y-4 rounded-md bg-bgColor/[.85] py-4 text-accent shadow backdrop-blur group-[.menu-open]:z-50 group-[.menu-open]:flex sm:static sm:z-auto sm:-ms-4 sm:mt-1 sm:flex sm:flex-row sm:items-center sm:divide-x sm:divide-dashed sm:divide-accent sm:rounded-none sm:bg-transparent sm:py-0 sm:shadow-none sm:backdrop-blur-none"
             id="navigation-menu">
            <a aria-current="page" class="px-4 py-4 underline-offset-2 sm:py-0 sm:hover:underline"
               href="index.html"> Home </a><a
                class="px-4 py-4 underline-offset-2 sm:py-0 sm:hover:underline" href="/about/">
                About </a>
        </nav>
    </div>
    <site-search class="ms-auto" id="search">
        <button id="open-search"
                class="flex h-9 w-9 items-center justify-center rounded-md ring-zinc-400 transition-all hover:ring-2"
                data-open-modal="">
            <svg aria-label="search" class="h-7 w-7" fill="none" height="16" stroke="currentColor"
                 stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 24 24" width="16"
                 xmlns="http://www.w3.org/2000/svg">
                <path d="M0 0h24v24H0z" stroke="none"></path>
                <path d="M3 10a7 7 0 1 0 14 0 7 7 0 1 0-14 0M21 21l-6-6"></path>
            </svg>
        </button>
        <dialog aria-label="search"
                class="h-full max-h-full w-full max-w-full border border-zinc-400 bg-bgColor shadow backdrop:backdrop-blur sm:mx-auto sm:mb-auto sm:mt-16 sm:h-max sm:max-h-[calc(100%-8rem)] sm:min-h-[15rem] sm:w-5/6 sm:max-w-[48rem] sm:rounded-md">
            <div class="dialog-frame flex flex-col gap-4 p-6 pt-12 sm:pt-6">
                <button id="close-search"
                        class="ms-auto cursor-pointer rounded-md bg-zinc-200 p-2 font-semibold dark:bg-zinc-700"
                        data-close-modal="">Close
                </button>
                <div class="search-container">
                    <div id="cactus__search"/>
                </div>
            </div>
        </dialog>
    </site-search>
    <theme-toggle class="ms-2 sm:ms-4">
        <button id="theme-toggle" class="relative h-9 w-9 rounded-md p-2 ring-zinc-400 transition-all hover:ring-2"
                type="button">
            <span class="sr-only">Dark Theme</span>
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-7 w-7 -translate-x-1/2 -translate-y-1/2 scale-100 opacity-100 transition-all dark:scale-0 dark:opacity-0"
                 fill="none" focusable="false" id="sun-svg" stroke-width="1.5" viewBox="0 0 24 24"
                 xmlns="http://www.w3.org/2000/svg">
                <path
                    d="M12 18C15.3137 18 18 15.3137 18 12C18 8.68629 15.3137 6 12 6C8.68629 6 6 8.68629 6 12C6 15.3137 8.68629 18 12 18Z"
                    stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M22 12L23 12" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M12 2V1" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M12 23V22" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M20 20L19 19" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M20 4L19 5" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M4 20L5 19" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M4 4L5 5" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M1 12L2 12" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
            </svg>
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-7 w-7 -translate-x-1/2 -translate-y-1/2 scale-0 opacity-0 transition-all dark:scale-100 dark:opacity-100"
                 fill="none" focusable="false" id="moon-svg" stroke="currentColor" stroke-width="1.5"
                 viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M0 0h24v24H0z" fill="none" stroke="none"></path>
                <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z"></path>
                <path d="M17 4a2 2 0 0 0 2 2a2 2 0 0 0 -2 2a2 2 0 0 0 -2 -2a2 2 0 0 0 2 -2"></path>
                <path d="M19 11h2m-1 -1v2"></path>
            </svg>
        </button>
    </theme-toggle>
    <mobile-button>
        <button aria-expanded="false" aria-haspopup="menu" aria-label="Open main menu"
                class="group relative ms-4 h-7 w-7 sm:invisible sm:hidden" id="toggle-navigation-menu" type="button">
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-full w-full -translate-x-1/2 -translate-y-1/2 transition-all group-aria-expanded:scale-0 group-aria-expanded:opacity-0"
                 fill="none" focusable="false" id="line-svg" stroke="currentColor" stroke-width="1.5"
                 viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M3.75 9h16.5m-16.5 6.75h16.5" stroke-linecap="round" stroke-linejoin="round"></path>
            </svg>
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-full w-full -translate-x-1/2 -translate-y-1/2 scale-0 text-accent opacity-0 transition-all group-aria-expanded:scale-100 group-aria-expanded:opacity-100"
                 fill="none" focusable="false" id="cross-svg" stroke="currentColor" stroke-width="1.5"
                 viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M6 18L18 6M6 6l12 12" stroke-linecap="round" stroke-linejoin="round"></path>
            </svg>
        </button>
    </mobile-button>
</header>
<main id="main" data-pagefind-body>
    <section aria-label="Blog post list">
        <article id="article-3841">
            <a href="https://ayende.com/blog/165123/big-data-search-setting-up" target="_blank">
                <h2 class="title mb-6" id="article-3841">Big Data Search</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: January 17, 2014
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">The interesting thing about this problem is that I was very careful in how I phrased things. I said what I wanted to happen, but didn&#x2019;t specify what needs to be done. That was quite intentional. For that matter, the fact that I am posting about what is going to be our acceptance criteria is also intentional. The idea is to have a non trivial task, but something that should be very well understood and easy to research. It also means that the candidate needs to be able to write some non trivial code. And I can tell a lot about a dev from such a project. At the same time, this is a very self contained scenario. The idea is that this is something that you can do in a short amount of time. The reason that this is an interesting exercise is that this is actually at least two totally different but related problems. First, in a 15TB file, we obviously cannot rely on just scanning the entire file. That means that we have to have an index. And that mean that we have to build it. Interestingly enough, an index being a sorted structure, that means that we have to solve the problem of sorting more data than can fit in main memory. The second problem is probably easier, since it is just an implementation of external sort, and there are plenty of algorithms around to handle that. Note that I am not really interested in actual efficiencies for this particular scenario. I care about being able to see the code. See that it works, etc. My solution, for example, is a single threaded system that make no attempt at parallelism or I/O optimizations. It clocks at over 1 GB / minute and the memory consumption is at under 150MB. Queries for a unique value return the result in 0.0004 seconds. Queries that returned 153K results completed in about 2 seconds. When increasing the used memory to about 650MB, there isn&#x2019;t really any difference in performance, which surprised me a bit. Then again, the entire code is probably highly inefficient. But that is good enough for now. The process is kicked off with indexing:      1: var options = new DirectoryExternalStorageOptions(&quot;/path/to/index/files&quot;);   2: var input = File.OpenRead(@&quot;/path/to/data/Crimes_-_2001_to_present.csv&quot;);   3: var sorter = new ExternalSorter(input, options, new int[]   4: {   5:     1,// case number   6:     4, // ICHR   7:&#xA0;    8: });   9:&#xA0;   10: sorter.Sort();&#xA;I am actually using the Chicago crime data for this. This is a 1GB file that I downloaded from the Chicago city portal in CSV format. This is what the data looks like:&#xA;&#xA;The ExternalSorter will read and parse the file, and start reading it into a buffer. When it gets to a certain size (about 64MB of source data, usually), it will sort the values in memory and output them into temporary files.&#xA;Those file looks like this:&#xA;&#xA;Initially, I tried to do that with binary data, but it turns out that that was too complex to be easy, and writing this in a human readable format made it much easier to work with. The format is pretty simple, you have the value of the left, and on the right you have start position of the row for this value. &#xA;We generate about 17 such temporary files for the 1GB file. One temporary file per each 64 MB of the original file. This lets us keep our actual memory consumption very low, but for larger data sets, we&#x2019;ll probably want to actually do the sort every 1 GB or maybe more. Our test machine has 16 GB of RAM, so doing a sort and outputting a temporary file every 8 GB can be a good way to handle things. But that is beside the point.&#xA;The end result is that we have multiple sorted files, but they aren&#x2019;t sequential. In other words, in file #1 we have values 1,4,6,8 and in file #2 we have 1,2,6,7. We need to merge all of them together. Luckily, this is easy enough to do. We basically have a heap that we feed entries from the files into. And that pretty much takes care of this. See merge sort if you want more details about this.&#xA;The end result of merging all of those files is&#x2026; another file, just like them, that contains all of the data sorted. Then it is time to actually handle the other issue, actually searching the data.&#xA;We can do that using simple binary search, with the caveat that because this is a text file, and there is no fixed size records or pages, it is actually a big hard to figure out where to start reading.&#xA;In effect, what I am doing is to select an arbitrary byte position, then walk backward until I find a &#x2018;\n&#x2019;. Once I found the new line character, I can read the full line, check the value, and decide where I need to look next. Assuming that I actually found my value, I can now go to the byte position of the value in the original file and read the original line, giving it to the user.&#xA;Assuming an indexing rate of 1 GB / minute a 15 TB file would take about 10 days to index. But there are ways around that as well, but I&#x2019;ll touch on them in my next post. What all of this did was bring home just how much we usually don&#x2019;t have to worry about such things. But I consider this research well spent, we&#x2019;ll be using this in the future.</p>
        </article>
        <article id="article-3842">
            <a href="https://ayende.com/blog/165122/big-data-search" target="_blank">
                <h2 class="title mb-6" id="article-3842">Big Data Search</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: January 16, 2014
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">I got tired of the old questions that we were asking candidates, so I decided to add a new one. This one is usually something that we&#x2019;ll give the candidates to do at home, at their leisure. Let us imagine the following file:      1: &quot;first_name&quot;,&quot;last_name&quot;,&quot;company_name&quot;,&quot;address&quot;,&quot;city&quot;,&quot;county&quot;,&quot;state&quot;,&quot;zip&quot;,&quot;phone1&quot;,&quot;phone2&quot;,&quot;email&quot;,&quot;web&quot;   2: &quot;James&quot;,&quot;Butt&quot;,&quot;Benton, John B Jr&quot;,&quot;6649 N Blue Gum St&quot;,&quot;New Orleans&quot;,&quot;Orleans&quot;,&quot;LA&quot;,70116,&quot;504-621-8927&quot;,&quot;504-845-1427&quot;,&quot;jbutt@gmail.com&quot;,&quot;http://www.bentonjohnbjr.com&quot;   3: &quot;Josephine&quot;,&quot;Darakjy&quot;,&quot;Chanay, Jeffrey A Esq&quot;,&quot;4 B Blue Ridge Blvd&quot;,&quot;Brighton&quot;,&quot;Livingston&quot;,&quot;MI&quot;,48116,&quot;810-292-9388&quot;,&quot;810-374-9840&quot;,&quot;josephine_darakjy@darakjy.org&quot;,&quot;http://www.chanayjeffreyaesq.com&quot;   4: &quot;Art&quot;,&quot;Venere&quot;,&quot;Chemel, James L Cpa&quot;,&quot;8 W Cerritos Ave #54&quot;,&quot;Bridgeport&quot;,&quot;Gloucester&quot;,&quot;NJ&quot;,&quot;08014&quot;,&quot;856-636-8749&quot;,&quot;856-264-4130&quot;,&quot;art@venere.org&quot;,&quot;http://www.chemeljameslcpa.com&quot;   5: &quot;Lenna&quot;,&quot;Paprocki&quot;,&quot;Feltz Printing Service&quot;,&quot;639 Main St&quot;,&quot;Anchorage&quot;,&quot;Anchorage&quot;,&quot;AK&quot;,99501,&quot;907-385-4412&quot;,&quot;907-921-2010&quot;,&quot;lpaprocki@hotmail.com&quot;,http://www.feltzprintingservice.com&#xA;As you can see, this is a pretty trivial CSV file. However, let assume that it is a small example of a CSV file that is 15 TB in size. The requirement is to be able to query on that file. We need to be able to query by email or all the people with in a particular zip code. Because of the size, the solution can be composed of two parts, a prepare part (which can run for as long as it is needed) and answer to queries part. Maximum time to answer any query must be under 30 seconds.&#xA;&#xA;You can assume that the file never changes, and that once the prepare part is done, it will never need to be run again. &#xA;The answer to a query is the full CSV row.&#xA;You can assume a machine with a single machine 100TB disk, 16 GB RAM and 8 CPU cores. &#xA;The solution cannot use any existing databases.&#xA;The solution needs to include explanation of the various options that were available and why this specific solution was chosen.&#xA;After the prepare phase is done, the solution has to take less than 30TB of data (including the original file).&#xA;The solution should be easy to apply to different CSV file.&#xA;I decided that it wouldn&#x2019;t be fair to ask candidates to do something like that without doing it myself. Mostly because the fact that I have a good idea about how to do something doesn&#x2019;t meant that I understand the actual implementation issues that might pop up.&#xA;I actually gave myself a somewhat harder task, do the above mention task, but do it without access to any library other than the BCL and do so with a minimal amount of memory usage. The entire thing took less than a day, and it solves the problem quite a bit more efficiently than I actually anticipated.&#xA;But I&#x2019;ll discuss the details of this in my next post.</p>
        </article>
        <article id="article-3843">
            <a href="https://www.meziantou.net/stockage-des-mots-de-passe.htm" target="_blank">
                <h2 class="title mb-6" id="article-3843">Stockage des mots de passe</h2>
            </a>
            <p class="mb-2">by G&#xE9;rald Barr&#xE9;</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: January 16, 2014
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">J&#x27;ai &#xE9;crit un article sur un autre blog:Il s&#x27;agit de stocker des informations permettant d&#x27;authentifier une personne sur une application. Il faut garder &#xE0; l&#x27;esprit que ces donn&#xE9;es sont sensibles et doivent donc &#xEA;tre s&#xE9;curis&#xE9;es.On voit souvent des news indiquant que des mots de passe ont fuit&#xE9;. Cela</p>
        </article>
        <article id="article-3844">
            <a href="https://ayende.com/blog/165313/the-cost-of-working-with-strings" target="_blank">
                <h2 class="title mb-6" id="article-3844">The cost of working with strings</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: January 15, 2014
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">Following my last post, I decided that it might be better to actually show what the difference is between direct string manipulation and working at lower levels. I generated a sample CSV file with 10 million lines and 6 columns. The file size was 658MB. I then wrote the simplest code that I could possibly think of:      1: public class TrivialCsvParser   2: {   3:     private readonly string _path;   4:&#xA0;    5:     public TrivialCsvParser(string path)   6:     {   7:         _path = path;   8:     }   9:&#xA0;   10:     public IEnumerable&lt;string[]&gt; Parse()  11:     {  12:         using (var reader = new StreamReader(_path))  13:         {  14:             while (true)  15:             {  16:                 var line = reader.ReadLine();  17:                 if (line == null)  18:                     break;  19:                 var fields = line.Split(&#x27;,&#x27;);  20:                 yield return fields;  21:             }  22:         }  23:     }  24: }&#xA;This run in 8.65 seconds (with a no-op action) and kept the memory utilization at about 7MB. &#xA;Then next thing to try was just reading through the file without doing any parsing. So I wrote this:&#xA;&#xA;&#xA;   1: public class NoopParser   2: {   3:     private readonly string _path;   4:&#xA0;    5:     public NoopParser(string path)   6:     {   7:         _path = path;   8:     }   9:&#xA0;   10:     public IEnumerable&lt;object&gt; Parse()  11:     {  12:         var buffer = new byte[1024];  13:         using (var stream = new FileStream(_path,FileMode.Open, FileAccess.Read))  14:         {  15:             while (true)  16:             {  17:                 var result = stream.Read(buffer, 0, buffer.Length);  18:                 if (result == 0)  19:                     break;  20:                 yield return null; // noop  21:             }  22:         }  23:     }  24: }&#xA;Note that this isn&#x2019;t actually doing anything. But this took 0.83 seconds, so we see a pretty important big difference here. By the way, the amount of memory used isn&#x2019;t noticeably different here. Both use about 7 MB. Probably because we aren&#x2019;t actually holding up to any of the data in any meaningful way.&#xA;I have run the results using release build, and I run it multiple times, so the file is probably all in the OS cache. So I/O cost is pretty minimal here. However, note that we aren&#x2019;t doing a lot of stuff that is being done by the TrivialCsvParser. For example, doing line searches, splitting the string to fields, etc. But interestingly enough, just removing the split will reduce the cost from 8.65 seconds to 3.55 seconds.</p>
        </article>
        <article id="article-3845">
            <a href="https://ayende.com/blog/165121/without-strings-it-is-a-dark-cold-place" target="_blank">
                <h2 class="title mb-6" id="article-3845">Without strings, it is a dark, cold place&#x2026;</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: January 14, 2014
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">So I set out to do some non trivial stuff with file parsing. The file format is CSV, and I am explicitly trying to do it with as few string allocations as possible. In effect, I am basically relying on a char array that I manually manage. But as it turns out, this is not so easy. To start with, 65279 should be taken out and shot. That is the BOM marker (U&#x2B;FEFF), and it is has a very nasty habit of showing up when you are mixing StreamWriter and reading from a byte stream, even when I made sure to use the UTF8 encoding anyway. It is possible, as I said, but it is anything but nice. I set out to do non trivial stuff using this approach, but I wonder how useful this actually is. From experience, this can kill a system performance. This has been more than just my experience: http://joeduffyblog.com/2012/10/30/beware-the-string Of course, the moment that you start dealing with your own string type, it is all back in the good bad days of C&#x2B;&#x2B; and BSTR vs cstr vs std::string vs. MyString vs OmgStr. For example, how do you look at the value during debug&#x2026; I am pretty sure that in general, that isn&#x2019;t something that you&#x2019;ll want to do. In my spike, quite a lot of the issues that came up were directly associated with this. On the other hand, this did let me do things like string pooling, efficient parsing with no allocations, etc. But I&#x2019;ll talk about that specific project in my next post.</p>
        </article>
        <article id="article-3846">
            <a href="https://ayende.com/blog/165091/strings-are-annoying" target="_blank">
                <h2 class="title mb-6" id="article-3846">Strings are annoying</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: January 13, 2014
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">I hate a love/hate/hate relationship with .NET strings. That is because they are both incredibly convenient and horribly inefficient in a bad way. Let us look at the following file:      1: &quot;first_name&quot;,&quot;last_name&quot;,&quot;company_name&quot;,&quot;address&quot;,&quot;city&quot;,&quot;county&quot;,&quot;state&quot;,&quot;zip&quot;,&quot;phone1&quot;,&quot;phone2&quot;,&quot;email&quot;,&quot;web&quot;   2: &quot;James&quot;,&quot;Butt&quot;,&quot;Benton, John B Jr&quot;,&quot;6649 N Blue Gum St&quot;,&quot;New Orleans&quot;,&quot;Orleans&quot;,&quot;LA&quot;,70116,&quot;504-621-8927&quot;,&quot;504-845-1427&quot;,&quot;jbutt@gmail.com&quot;,&quot;http://www.bentonjohnbjr.com&quot;   3: &quot;Josephine&quot;,&quot;Darakjy&quot;,&quot;Chanay, Jeffrey A Esq&quot;,&quot;4 B Blue Ridge Blvd&quot;,&quot;Brighton&quot;,&quot;Livingston&quot;,&quot;MI&quot;,48116,&quot;810-292-9388&quot;,&quot;810-374-9840&quot;,&quot;josephine_darakjy@darakjy.org&quot;,&quot;http://www.chanayjeffreyaesq.com&quot;   4: &quot;Art&quot;,&quot;Venere&quot;,&quot;Chemel, James L Cpa&quot;,&quot;8 W Cerritos Ave #54&quot;,&quot;Bridgeport&quot;,&quot;Gloucester&quot;,&quot;NJ&quot;,&quot;08014&quot;,&quot;856-636-8749&quot;,&quot;856-264-4130&quot;,&quot;art@venere.org&quot;,&quot;http://www.chemeljameslcpa.com&quot;&#xA;Reading this is a simple matter of writing something like this:&#xA;&#xA;&#xA;   1: var headerLine = reader.ReadLine();   2: var headers = headerLine.Split(&#x27;,&#x27;).Select(h=&gt;h.Trim(&#x27;&quot;&#x27;)).ToArray();   3:&#xA0;    4: while(reader.EndOfStream == false)   5: {   6:     var line = reader.ReadLine();   7:     var columns = line.Split(&quot;,&quot;);   8:     var dic = new Dictionary&lt;string,string&gt;();   9:     for(var i=0;i&lt;headers.Length;i&#x2B;&#x2B;)  10:     {  11:         dic[headers[i]] = columns[i].Trim(&#x27;&quot;&#x27;);  12:     }  13:     yield return dic;  14: }&#xA;Now, let us look at the same code again, but this time, I marked places where we are doing string allocation:&#xA;&#xA;&#xA;   1: var headerLine = reader.ReadLine();   2: var headers = headerLine.Split(&#x27;,&#x27;).Select(h=&gt;h.Trim(&#x27;&quot;&#x27;)).ToArray();   3:&#xA0;    4: while(reader.EndOfStream == false)   5: {   6:     var line = reader.ReadLine();   7:     var columns = line.Split(&quot;,&quot;);   8:     var dic = new Dictionary&lt;string,string&gt;();   9:     for(var i=0;i&lt;headers.Length;i&#x2B;&#x2B;)  10:     {  11:         dic[headers[i]] = columns[i].Trim(&#x27;&quot;&#x27;);  12:     }  13:     yield return dic;  14: }&#xA;&#xA;Those are a lot of strings that we are allocating. And if we are reading a large file, that can very quickly turn into a major performance issue. If I was writing the same in C, for example, I would be re-using the allocated string multiple times, but here we&#x2019;ve to allocate and discard them pretty much continuously.&#xA;The really sad thing about it, it is incredibly easy to do this, usually without paying any attention. But even if you know what you are doing, you pretty much have to roll your own everything to get it to work. And that sucks quite badly.</p>
        </article>
        <article id="article-3847">
            <a href="https://benfoster.io/blog/aspnet-mvc-custom-error-pages/" target="_blank">
                <h2 class="title mb-6" id="article-3847">Custom error pages in ASP.NET MVC. Easy, right?</h2>
            </a>
            <p class="mb-2">by Ben Foster</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: January 11, 2014
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">If you&#x2019;ve ever struggled to configure custom error pages in ASP.NET MVC then this post is for you.</p>
        </article>
        <article id="article-3848">
            <a href="https://ayende.com/blog/165090/early-lock-release-transactions-and-errors" target="_blank">
                <h2 class="title mb-6" id="article-3848">Early lock release, transactions and errors</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: January 10, 2014
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">There has been quite a lot of research on the notion of early lock release as a way to improve database concurrency. For a while, Voron actually supported that, mostly because I thought that this is a good idea. Lately, however, I decided that it is anything but for modern databases. In short, this is how transactions behave:    Standard transaction Early Lock Release Transaction    Take locks Perform work Commit transaction in memory Flush transaction to log file Release locks Notify user that the transaction successfully completed   Take locks  Perform work  Commit transaction in memory  Flush transaction to log file async Release locks  Notify user that the transaction successfully completed when the log flush competes As you note, the major difference is when we release the locks. In the case of ELR, we release the locks immediately when start flushing the transaction state to log. The idea is that we don&#x2019;t need to wait for potentially length I/O to allow the next transaction to start running. However, we don&#x2019;t report the transaction as completed before we flushed the transaction. There is a tiny complication in there, the next transaction cannot start doing the async flushing to log before our transaction is also over, but that is fine and expected, anyway. However, what about error conditions? What happens if we&#x2019;ve started to flush the transaction to disk in an async manner, then we got an error. Well, the obvious thing to do here (and as called out in the paper) is to abort the transaction, and then abort any transaction that has started after the failed transaction released its locks. This is usually okay, because in general, that is no big deal at all. Aside from something like out of disk space errors (which can be resolved by pre-allocating the data), there aren&#x2019;t really any non catastrophic disk errors. So usually if the disk give you a hard time, it pretty much time to give up and go home. However, with the use of cloud computing, it is actually pretty common (however much it is a bad idea) to have a &#x201C;networked disk&#x201D;. This means that it can certainly be the case that a I/O request you made to the disk can get lost, delayed and just appear to fail (but actually go through). It is the last scenario in particular that worries me. If you actually wrote to the log, but you think that you didn&#x2019;t, what is your state now? And while I can handle that in a case where I can fail the transaction and rollback all the previous state, it is much harder to do that if I&#x2019;ve already committed the transaction in memory, since we might need to do a memory only rollback, and that isn&#x2019;t something that we are actually setup to do. In short, we&#x2019;ll be rolling back early lock release in Voron, it isn&#x2019;t worth the complexity involved, especially since we already have better ways to handle concurrency.</p>
        </article>
        <article id="article-3849">
            <a href="https://ayende.com/blog/165057/voron-time-series-data" target="_blank">
                <h2 class="title mb-6" id="article-3849">Voron &amp; time series data</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: January 09, 2014
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">One of the things that Voron does very well is the ability to read a lot of data fast. One of the interesting scenarios we deal with is when we want to deal with time series data. For example, let us say that we have a bunch of sensors reporting on the temperature metrics within an area (said while the heaviest storm in 5 decades is blowing outside). Every minute, we have some data coming in. For fun, we will make the following assumptions:  We have do deal with late writes (a sensor sending us updates from 1 hour ago because of communication update). Dates aren&#x2019;t unique. All queries will take into account the dates. First, let me show you the full code for that, then we can talk about how it works:      1: public class DateTimeSeries : IDisposable   2: {   3:     private readonly JsonSerializer _serializer = new JsonSerializer();   4:     private readonly StorageEnvironment _storageEnvironment;   5:     private long _last;   6:     private readonly Slice _lastKey;   7:&#xA0;    8:     public DateTimeSeries(string path)   9:     {  10:         _lastKey = &quot;last-key&quot;;  11:         _storageEnvironment = new StorageEnvironment(StorageEnvironmentOptions.ForPath(path));  12:         using (var tx = _storageEnvironment.NewTransaction(TransactionFlags.ReadWrite))  13:         {  14:             _storageEnvironment.CreateTree(tx, &quot;data&quot;);  15:             var read = tx.State.Root.Read(tx, _lastKey);  16:&#xA0;   17:             _last = read != null ? read.Reader.ReadInt64() : 1;  18:&#xA0;   19:             tx.Commit();  20:         }  21:     }  22:&#xA0;   23:     public void AddRange&lt;T&gt;(IEnumerable&lt;KeyValuePair&lt;DateTime, T&gt;&gt; values)  24:     {  25:         using (var tx = _storageEnvironment.NewTransaction(TransactionFlags.ReadWrite))  26:         {  27:             var data = tx.GetTree(&quot;data&quot;);  28:             var buffer = new byte[16];  29:             var key = new Slice(buffer);  30:             var ms = new MemoryStream();  31:             foreach (var kvp in values)  32:             {  33:                 var date = kvp.Key;  34:                 EndianBitConverter.Big.CopyBytes(date.ToBinary(), buffer, 0);  35:                 EndianBitConverter.Big.CopyBytes(_last&#x2B;&#x2B;, buffer, 8);  36:                 ms.SetLength(0);  37:                 _serializer.Serialize(new StreamWriter(ms), kvp.Value);  38:                 ms.Position = 0;  39:&#xA0;   40:                 data.Add(tx, key, ms);  41:             }  42:&#xA0;   43:             tx.State.Root.Add(tx, _lastKey, new MemoryStream(BitConverter.GetBytes(_last)));  44:             tx.Commit();  45:         }  46:     }  47:&#xA0;   48:     public IEnumerable&lt;T&gt; ScanRange&lt;T&gt;(DateTime start, DateTime end)  49:     {  50:         using (var tx = _storageEnvironment.NewTransaction(TransactionFlags.Read))  51:         {  52:             var data = tx.GetTree(&quot;data&quot;);  53:             var startBuffer = new byte[16];  54:             EndianBitConverter.Big.CopyBytes(start.ToBinary(), startBuffer, 0);  55:             var startKey = new Slice(startBuffer);  56:&#xA0;   57:             using (var it = data.Iterate(tx))  58:             {  59:                 var endBuffer = new byte[16];  60:                 EndianBitConverter.Big.CopyBytes(end.ToBinary(), endBuffer, 0);  61:                 EndianBitConverter.Big.CopyBytes(long.MaxValue, endBuffer, 8);  62:&#xA0;   63:                 it.MaxKey = new Slice(endBuffer);  64:                 if (it.Seek(startKey) == false)  65:                     yield break;  66:                 do  67:                 {  68:                     var reader = it.CreateReaderForCurrent();  69:                     using (var stream = reader.AsStream())  70:                     {  71:                         yield return _serializer.Deserialize&lt;T&gt;(new JsonTextReader(new StreamReader(stream)));  72:                     }  73:                 } while (it.MoveNext());  74:             }  75:         }  76:               77:     }  78:&#xA0;   79:     public void Dispose()  80:     {  81:         _storageEnvironment.Dispose();  82:     }  83: }&#xA;In line 14, we create the data tree, which will hold the actual time series data, and the last-key, which I&#x2019;ll explain in a bit.&#xA;The AddRange method in line 23 is probably the most interesting. We create a key that is composed of the date of the entry, and an incrementing number. Note that we use big endian encoding because that allow easy byte string sorting. The implications of this sort of key is that the values are actually sorted by the date, but if we have multiple values for the same millisecond, we don&#x2019;t overwrite the data. Along with adding the actual data, we record the change in the incrementing counter ,so if we need to restart, we&#x2019;ll continue from where we left off.&#xA;Finally, we have the actual ScanRange method. Here we basically start from the minimum value for the start date, and set the MaxKey as the stop condition for the maximum value for the end date. And then it is just getting the values out.&#xA;Pretty simple, I think.</p>
        </article>
        <article id="article-3850">
            <a href="https://ayende.com/blog/164963/working-with-the-freedb-dataset-in-voron" target="_blank">
                <h2 class="title mb-6" id="article-3850">Working with the FreeDB dataset in Voron</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: January 08, 2014
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">Reminder, the FreeDB data set is a 3.32 million records. Containing most of the albums that came out in the past few decades. We created the following Voron database to handle it:      1: _storageEnvironment = new StorageEnvironment(StorageEnvironmentOptions.ForPath(&quot;FreeDB&quot;));   2: using (var tx = _storageEnvironment.NewTransaction(TransactionFlags.ReadWrite))   3: {   4:     _storageEnvironment.CreateTree(tx, &quot;albums&quot;);   5:     _storageEnvironment.CreateTree(tx, &quot;ix_diskids&quot;);   6:     _storageEnvironment.CreateTree(tx, &quot;ix_artists&quot;);   7:     _storageEnvironment.CreateTree(tx, &quot;ix_titles&quot;);   8:     tx.Commit();   9: }&#xA;The albums tree contains the actual information about the album, as a json string. And the ix_* trees contains back references to it. They are our indexes. For what it is worth, you might want to note that this is pretty much how most RDBMS implements their indexing. We&#x2019;re now working at a pretty low level.&#xA;Note also that we need to define those trees whenever we start the database.&#xA;Now, let us do some queries, shall we?&#xA;We are going to start with the simplest option, given a disk id, give me the matching disk. Because disk ids are only nearly unique, we have the possibility of multiple results returning.&#xA0; &#xA;&#xA;Side note, the DB now is 9.00 GB in size.&#xA;Let us see how we query it. It pains me to write it, but I created a &#x201C;repository like&#x201D; interface, because Voron is way too low level for us to expose to user code. This is actually one of the few places where a repository like interface is good. Because it hides the extra complexity and the rigidity of the structure is justified.&#xA;The interface looks like: &#xA;&#xA;&#xA;Now, let us see how we actually implement this guy. We&#x2019;ll start from the simplest thing, doing a search by a disk id, which is a nearly unique value that identify a disk.&#xA;&#xA;&#xA;   1: public IEnumerable&lt;Disk&gt; FindByDiskId(string diskId)   2: {   3:     using (var tx = _storageEnvironment.NewTransaction(TransactionFlags.Read))   4:     {   5:         var dix = tx.GetTree(&quot;ix_diskids&quot;);   6:         var albums = tx.GetTree(&quot;albums&quot;);   7:&#xA0;    8:         using (var it = dix.MultiRead(tx, diskId))   9:         {  10:             if (it.Seek(Slice.BeforeAllKeys) == false)  11:                 yield break;  12:             do  13:             {  14:                 var readResult = albums.Read(tx, it.CurrentKey);  15:                 using (var stream = readResult.Reader.AsStream())  16:                 {  17:                     yield return _serializer.Deserialize&lt;Disk&gt;(new JsonTextReader(new StreamReader(stream)));  18:                 }  19:             } while (it.MoveNext());  20:         }  21:&#xA0;   22:         tx.Commit();  23:     }  24: }&#xA;Let us go over this code in detail. We are using a read transaction, because we aren&#x2019;t doing any writes.&#xA;We are using two trees here, the ix_diskids, which is the index on the disks, and the albums tree, which contains the actual data.&#xA;On line 8, we do a multi read. This is done because a single disk id value may belong to multiple albums.&#xA;Lines 10 and 11 are needed in case there are no results. And the line 14 is where the important thing happen. The result of the MultiRead is an iterator that contains all the ids of the albums with this disk id. We then read it from the albums tree, deserialize it and hand it to the user. Pretty simple, overall.&#xA;Now, let us go to the more complex scenario, where we want to do a search by artist or album title.&#xA;&#xA;&#xA;   1: public IEnumerable&lt;Disk&gt; FindByArtist(string prefix)   2: {   3:     return FindByMultiValueIterator(prefix, &quot;ix_artists&quot;);   4: }   5:&#xA0;    6: public IEnumerable&lt;Disk&gt; FindByAlbumTitle(string prefix)   7: {   8:     return FindByMultiValueIterator(prefix, &quot;ix_titles&quot;);   9: }  10:&#xA0;   11: private IEnumerable&lt;Disk&gt; FindByMultiValueIterator(string prefix, string treeIndexName)  12: {  13:     using (var tx = _storageEnvironment.NewTransaction(TransactionFlags.Read))  14:     {  15:         var dix = tx.GetTree(treeIndexName);  16:         var albums = tx.GetTree(&quot;albums&quot;);  17:&#xA0;   18:         using (var multiValueIterator = dix.Iterate(tx))  19:         {  20:             multiValueIterator.RequiredPrefix = prefix.ToLower();  21:             if (multiValueIterator.Seek(multiValueIterator.RequiredPrefix) == false)  22:                 yield break;  23:             do  24:             {  25:                 using (var albumsIterator = multiValueIterator.CreateMutliValueIterator())  26:                 {  27:                     if (albumsIterator.Seek(Slice.BeforeAllKeys) == false)  28:                         continue;  29:                     do  30:                     {  31:                         var readResult = albums.Read(tx, albumsIterator.CurrentKey);  32:                         using (var stream = readResult.Reader.AsStream())  33:                         {  34:                             yield return _serializer.Deserialize&lt;Disk&gt;(new JsonTextReader(new StreamReader(stream)));  35:                         }  36:                     } while (albumsIterator.MoveNext());  37:                 }  38:             } while (multiValueIterator.MoveNext());  39:         }  40:&#xA0;   41:         tx.Commit();  42:     }  43: }&#xA;You can see that in both cases, we handle it the same, because the actual behavior is the same. We don&#x2019;t want to do just an exact match. If we wanted that, we could use the exact same logic as we did in FindByDiskId. But we want to do something more, we want to be able to search by prefix, not just by exact match. That means that we have to iterate over the tree. The only difference between FindByAlbumTitle and FindByArtist is the tree index that they use.&#xA;We start out as before, iterating over the index (line 18). Note that in line 20, we defined a required prefix, and we use the lower case form of the prefix. We also entered that to the index as lower case. This is how we are able to get case insensitive searches.&#xA;Line 21 actually take us to the beginning of all the entries greater or equal to the prefix, and by setting RequiredPrefix we ensure that we can&#x2019;t go to any entry that is doesn&#x2019;t have this prefix. This is an interesting example, because we are now iterating over all the index entries that have the same prefix. But the values of the index is also a list. That is why we do in line 25. Get all the values that match a particular entry with that particular prefix.&#xA;The value of that is the actual id that we use to go into the albums tree. And there you have in, non trivial search with Voron.</p>
        </article>
        <div class="button flex justify-between">
            <a href="384.html"><span class="back arrow"></span></a>

            <a href="386.html"><span class="next arrow"></span></a>
        </div>
    </section>
</main>
<footer
    class="mt-auto flex w-full flex-col items-center justify-center gap-y-2 pb-4 pt-20 text-center align-top font-semibold text-gray-600 dark:text-gray-400 sm:flex-row sm:justify-between sm:text-xs">
    <div class="me-0 sm:me-4">
        <div class="flex flex-wrap items-end gap-x-2">
            <ul class="flex flex-1 items-center gap-x-2 sm:flex-initial">
                <li class="flex">
                    <p class="flex items-end gap-2 justify-center flex-wrap	">Â© Relatively General
                        .NET 2024<span
                            class="inline-block">&nbsp;ðŸš€&nbsp;Theme: Astro Cactus</span>

                        <a class="inline-block sm:hover:text-link" href="https://github.com/chrismwilliams/astro-cactus"
                           rel="noopener noreferrer " target="_blank">
                            <svg width="1em" height="1em" viewBox="0 0 24 24" aria-hidden="true" class="h-6 w-6"
                                 focusable="false" data-icon="mdi:github">
                                <symbol id="ai:mdi:github">
                                    <path fill="currentColor"
                                          d="M12 2A10 10 0 0 0 2 12c0 4.42 2.87 8.17 6.84 9.5c.5.08.66-.23.66-.5v-1.69c-2.77.6-3.36-1.34-3.36-1.34c-.46-1.16-1.11-1.47-1.11-1.47c-.91-.62.07-.6.07-.6c1 .07 1.53 1.03 1.53 1.03c.87 1.52 2.34 1.07 2.91.83c.09-.65.35-1.09.63-1.34c-2.22-.25-4.55-1.11-4.55-4.92c0-1.11.38-2 1.03-2.71c-.1-.25-.45-1.29.1-2.64c0 0 .84-.27 2.75 1.02c.79-.22 1.65-.33 2.5-.33s1.71.11 2.5.33c1.91-1.29 2.75-1.02 2.75-1.02c.55 1.35.2 2.39.1 2.64c.65.71 1.03 1.6 1.03 2.71c0 3.82-2.34 4.66-4.57 4.91c.36.31.69.92.69 1.85V21c0 .27.16.59.67.5C19.14 20.16 22 16.42 22 12A10 10 0 0 0 12 2"></path>
                                </symbol>
                                <use xlink:href="#ai:mdi:github"></use>
                            </svg>
                            <span class="sr-only">Github</span>
                        </a>
                    </p>
                </li>
            </ul>
        </div>
    </div>
    <nav aria-label="More on this site" class="flex gap-x-2 sm:gap-x-0 sm:divide-x sm:divide-gray-500">
        <a class="px-4 py-2 sm:py-0 sm:hover:text-textColor sm:hover:underline" href="index.html"> Home </a><a
            class="px-4 py-2 sm:py-0 sm:hover:text-textColor sm:hover:underline" href="/about/"> About </a>
    </nav>
</footer>
<script src="js/script.js?id=af8f4559935e7bf5bf6015373793411d"></script>
<script src="pagefind/pagefind-ui.js"></script>
</body>
</html>