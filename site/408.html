
<!DOCTYPE html>
<html class="scroll-smooth" lang="en-US" data-theme="light">
<head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, shrink-to-fit=no" name="viewport">
    <meta content="IE=edge" http-equiv="X-UA-Compatible">
    <title>Page 408 â€¢ Relatively General .NET</title>
    <link href="favicon.ico" rel="icon" sizes="any">
    <link href="images/apple-touch-icon.png" rel="apple-touch-icon">
    <meta content="hsl()" name="theme-color">
    <meta content="website" property="og:type">
    <meta content="Home" property="og:title">
    <link rel="stylesheet" href="css/styles.css">
    <link rel="stylesheet" href="pagefind/pagefind-ui.css">
    <!-- Google Analytics -->
    <script>
        // Only load GA if consent is given
        function loadGA() {
            const script = document.createElement('script');
            script.src = 'https://www.googletagmanager.com/gtag/js?id=G-MDFXJY3FCY';
            script.async = true;
            document.head.appendChild(script);

            window.dataLayer = window.dataLayer || [];

            function gtag() {
                dataLayer.push(arguments);
            }

            gtag('js', new Date());
            gtag('config', 'G-MDFXJY3FCY');
        }

        // Check if consent was previously given
        if (localStorage.getItem('cookieConsent') === 'accepted') {
            loadGA();
        }
    </script>
    <!-- End Google Analytics -->
</head>
<body class="mx-auto flex min-h-screen max-w-3xl flex-col bg-bgColor px-4 pt-16 font-mono text-sm font-normal text-textColor antialiased sm:px-8">

<a class="sr-only focus:not-sr-only focus:fixed focus:start-1 focus:top-1.5" href="#main">
    skip to content
</a>
<header class="group relative mb-28 flex items-center sm:ps-[4.5rem]" id="main-header">
    <div class="flex sm:flex-col">
        <a aria-current="page" class="inline-flex items-center hover:filter-none sm:relative sm:inline-block"
           href="index.html">
            <img class="me-3 sm:absolute sm:start-[-4.5rem] sm:me-0 sm:h-16 sm:w-16 w-16" src="images/giphy.gif"
                 alt=""/>
            <span class="text-xl font-bold sm:text-2xl">Relatively General .NET</span>
        </a>
        <nav aria-label="Main menu"
             class="absolute -inset-x-4 top-14 hidden flex-col items-end gap-y-4 rounded-md bg-bgColor/[.85] py-4 text-accent shadow backdrop-blur group-[.menu-open]:z-50 group-[.menu-open]:flex sm:static sm:z-auto sm:-ms-4 sm:mt-1 sm:flex sm:flex-row sm:items-center sm:divide-x sm:divide-dashed sm:divide-accent sm:rounded-none sm:bg-transparent sm:py-0 sm:shadow-none sm:backdrop-blur-none"
             id="navigation-menu">
            <a aria-current="page" class="px-4 py-4 underline-offset-2 sm:py-0 sm:hover:underline"
               href="index.html"> Home </a><a
                class="px-4 py-4 underline-offset-2 sm:py-0 sm:hover:underline" href="/about/">
                About </a>
        </nav>
    </div>
    <site-search class="ms-auto" id="search">
        <button id="open-search"
                class="flex h-9 w-9 items-center justify-center rounded-md ring-zinc-400 transition-all hover:ring-2"
                data-open-modal="">
            <svg aria-label="search" class="h-7 w-7" fill="none" height="16" stroke="currentColor"
                 stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 24 24" width="16"
                 xmlns="http://www.w3.org/2000/svg">
                <path d="M0 0h24v24H0z" stroke="none"></path>
                <path d="M3 10a7 7 0 1 0 14 0 7 7 0 1 0-14 0M21 21l-6-6"></path>
            </svg>
        </button>
        <dialog aria-label="search"
                class="h-full max-h-full w-full max-w-full border border-zinc-400 bg-bgColor shadow backdrop:backdrop-blur sm:mx-auto sm:mb-auto sm:mt-16 sm:h-max sm:max-h-[calc(100%-8rem)] sm:min-h-[15rem] sm:w-5/6 sm:max-w-[48rem] sm:rounded-md">
            <div class="dialog-frame flex flex-col gap-4 p-6 pt-12 sm:pt-6">
                <button id="close-search"
                        class="ms-auto cursor-pointer rounded-md bg-zinc-200 p-2 font-semibold dark:bg-zinc-700"
                        data-close-modal="">Close
                </button>
                <div class="search-container">
                    <div id="cactus__search"/>
                </div>
            </div>
        </dialog>
    </site-search>
    <theme-toggle class="ms-2 sm:ms-4">
        <button id="theme-toggle" class="relative h-9 w-9 rounded-md p-2 ring-zinc-400 transition-all hover:ring-2"
                type="button">
            <span class="sr-only">Dark Theme</span>
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-7 w-7 -translate-x-1/2 -translate-y-1/2 scale-100 opacity-100 transition-all dark:scale-0 dark:opacity-0"
                 fill="none" focusable="false" id="sun-svg" stroke-width="1.5" viewBox="0 0 24 24"
                 xmlns="http://www.w3.org/2000/svg">
                <path
                    d="M12 18C15.3137 18 18 15.3137 18 12C18 8.68629 15.3137 6 12 6C8.68629 6 6 8.68629 6 12C6 15.3137 8.68629 18 12 18Z"
                    stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M22 12L23 12" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M12 2V1" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M12 23V22" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M20 20L19 19" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M20 4L19 5" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M4 20L5 19" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M4 4L5 5" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M1 12L2 12" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
            </svg>
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-7 w-7 -translate-x-1/2 -translate-y-1/2 scale-0 opacity-0 transition-all dark:scale-100 dark:opacity-100"
                 fill="none" focusable="false" id="moon-svg" stroke="currentColor" stroke-width="1.5"
                 viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M0 0h24v24H0z" fill="none" stroke="none"></path>
                <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z"></path>
                <path d="M17 4a2 2 0 0 0 2 2a2 2 0 0 0 -2 2a2 2 0 0 0 -2 -2a2 2 0 0 0 2 -2"></path>
                <path d="M19 11h2m-1 -1v2"></path>
            </svg>
        </button>
    </theme-toggle>
    <mobile-button>
        <button aria-expanded="false" aria-haspopup="menu" aria-label="Open main menu"
                class="group relative ms-4 h-7 w-7 sm:invisible sm:hidden" id="toggle-navigation-menu" type="button">
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-full w-full -translate-x-1/2 -translate-y-1/2 transition-all group-aria-expanded:scale-0 group-aria-expanded:opacity-0"
                 fill="none" focusable="false" id="line-svg" stroke="currentColor" stroke-width="1.5"
                 viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M3.75 9h16.5m-16.5 6.75h16.5" stroke-linecap="round" stroke-linejoin="round"></path>
            </svg>
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-full w-full -translate-x-1/2 -translate-y-1/2 scale-0 text-accent opacity-0 transition-all group-aria-expanded:scale-100 group-aria-expanded:opacity-100"
                 fill="none" focusable="false" id="cross-svg" stroke="currentColor" stroke-width="1.5"
                 viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M6 18L18 6M6 6l12 12" stroke-linecap="round" stroke-linejoin="round"></path>
            </svg>
        </button>
    </mobile-button>
</header>
<main id="main" data-pagefind-body>
    <section aria-label="Blog post list">
        <article id="article-4071">
            <a href="https://ayende.com/blog/162915/so-what-have-you-been-learning-lately" target="_blank">
                <h2 class="title mb-6" id="article-4071">So, what have YOU been learning lately?</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: September 24, 2013
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">One of the worst things that can happen to you professionally is stagnation. You know what you are doing, you know how it works, and you can coast along very easily. Unfortunately, there is the old, it isn&#x2019;t what we know that we don&#x2019;t know that is going to hurt us. It is what we don&#x2019;t know that we don&#x2019;t know that is going to bite us in the end. One of the reasons that I have routinely been going out and searching for difficult codebases to read has been to avoid that. I know that I don&#x2019;t know a lot, I just don&#x2019;t know what I don&#x2019;t know. So I go into an unfamiliar codebase and try to figure out how things work over there. I have been doing that for quite some time now. And I am not talking about looking at some sample project a poo schlump put out to show how you can do CQRS with 17 projects to create a ToDo app. I am talking about production code, and usually in areas or languages that I am not familiar with. A short list of the stuff that I have been gone over:  CouchDB (to learn Erlang, actually, but that got me to do DB stuff). LevelDB LMDB NServiceBus Mass Transit SignalR Hibernate Hibernate Search Those are codebases that do interesting things that I wanted to learn from. Indeed, I have learned from each of those. Some people can learn by reading academic papers, I find that I learn best from having a vague idea about what is going on, then diving into the implementation details and seeing how it all fits together.  But the entire post so far was a preface to the question I wanted to ask. If you are reading this post, I am pretty sure that you are a professional developer. Doctors, lawyers and engineers (to name a few) have to recertify every so often, to make sure that they are current. But I have seen all too many developers stagnate to the point where they are very effective in their chosen field (building web apps with jQuery Mobile on ASP.Net WebForms 3.5) and nearly useless otherwise.  So, how are you keeping your skills sharp and your knowledge current? What have you been learning lately? It can be a course, or a book or a side project or just reading code. But, in my opinion, it cannot be something passive. If you were going to answer: &#x201C;I read your blog&#x201D; as the answer to that question, that is not sufficient, flatterer. Although, you might want to go a bit further and consider that imitation is the sincerest form of flattery, so go ahead and do something.</p>
        </article>
        <article id="article-4072">
            <a href="https://ayende.com/blog/163777/hibernating-rhinos-and-managed-designs-announce-enterprise-partnership-related-to-ravendb" target="_blank">
                <h2 class="title mb-6" id="article-4072">Hibernating Rhinos and Managed Designs announce enterprise partnership related to RavenDB</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: September 23, 2013
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">September 23, 2013 Deal will allow Hibernating Rhinos customers to get premium level consulting and support services provided by Managed Designs across Europe  Milan, Italy and Sede Izhak, Israel &#x2013; September 23, 2013. Hibernating Rhinos and Managed Designs today announced a partnership that will allow Hibernating Rhinos customers to get premium level consulting, support and training services appointing Managed Designs as its official partner for the following European countries: &#xB7; West Europe Countries: Portugal, Spain (including Andorra), France (including Monaco); &#xB7; Central Europe Countries: Luxemburg, Belgium, Germany, Switzerland, Nederland, United Kingdom and Ireland, Denmark, Sweden, Norway, Finland, Austria and Italy (including San Marino and Vatican City) &#xB7; East Europe Countries: Czech Republic, Poland, Hungary, Slovakia, Slovenia, Bosnia Herzegovina, Croatia, Serbia, Albania and Greece, Romania and Bulgaria As per this partnership &#x201C;Hibernating Rhinos is committed on developing and marketing a first class document database and wants its customers to get the best experience out of it, so we&#x2019;re glad having Managed Designs assisting them&#x201D; said Oren Eini, CEO of Hibernating Rhinos.  &#x201C;Managed Designs has been enjoying RavenDB for years now, and we&#x2019;re excited to have been engaged by Hibernating Rhinos in order to have their customers getting the best experience out of the product&#x201D;, said Andrea Saltarello, CEO of Managed Designs. About Hibernating Rhinos Hibernating Rhinos LTD is an Israeli based company, focused on delivering products and services in the database infrastructure field. For more information about Hibernating Rhinos, visit http://www.hibernatingrhinos.com About Managed Designs Managed Designs provides consulting, education and software development services helping customers to find solutions to their business problems. For more information about Managed Designs, visit http://www.manageddesigns.it RavenDB is a registered trademark of Hibernating Rhinos and/or its affiliates. Other names may be trademarks of their respective owners.</p>
        </article>
        <article id="article-4073">
            <a href="https://ayende.com/blog/163267/raven-storage-vorons-performance" target="_blank">
                <h2 class="title mb-6" id="article-4073">Raven Storage&#x2013;Voron&#x2019;s Performance</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: September 20, 2013
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">Voron is the code name for another storage engine that we are currently trying, based on LMDB. After taking that for a spin for a while, it is not pretty complete, and I decided to give it some perf test runs. So far, there has been zero performance work. All the usual caveat applies here, with regards to short test runs, etc.&#xA;&#xA;&#x9;Just like before, we found that this is horribly slow on the first run. The culprit was our debug code that verified the entire structure whenever we added or removed something. One we run it in release mode we started getting more interesting results.&#xA;&#xA;&#x9;Here is the test code:&#xA;&#xA;&#x9;using (var env = new StorageEnvironment(new MemoryMapPager(&quot;bench.data&quot;, flushMode)))&#xA;{&#xA;    using (var tx = env.NewTransaction(TransactionFlags.ReadWrite))&#xA;    {&#xA;        var value = new byte[100];&#xA;        new Random().NextBytes(value);&#xA;        var ms = new MemoryStream(value);&#xA;        for (long i = 0; i &lt; Count; i&#x2B;&#x2B;)&#xA;        {&#xA;            ms.Position = 0;&#xA;            env.Root.Add(tx, i.ToString(&quot;0000000000000000&quot;), ms);&#xA;        }&#xA;&#xA;        tx.Commit();&#xA;    }&#xA;     using (var tx = env.NewTransaction(TransactionFlags.ReadWrite))&#xA;     {&#xA;         DebugStuff.RenderAndShow(tx, tx.GetTreeInformation(env.Root).Root);&#xA;&#xA;         tx.Commit();&#xA;     }&#xA;}&#xA;&#x9;&#xA;&#x9;&#xA;&#xA;&#xA;&#x9;We write 1 million entries with 100 bytes in size and 8 bytes of the key. We run it in three mode:&#xA;&#xA;&#x9;fill seq none &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0;: &#xA0; &#xA0; &#xA0;9,578 ms &#xA0; &#xA0;104,404 ops / sec&#xA;&#xA;&#x9;fill seq buff &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0;: &#xA0; &#xA0; 10,802 ms &#xA0; &#xA0; 92,575 ops / sec&#xA;&#xA;&#x9;fill seq sync &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0;: &#xA0; &#xA0; &#xA0;9,387 ms &#xA0; &#xA0;106,528 ops / sec&#xA;&#xA;&#xA;&#xA;&#x9;None means no flushing to disk, let the OS deals with that completely. Buffers means flush to the OS, but not to disk, and full means do a full fsync.&#xA;&#xA;&#x9;Now, this is pretty stupid way to go about it, I&#x2019;ve to say. This is doing everything in a single transaction, and we are actually counting the time to close &amp; open the db here as well, but that is okay for now. We aren&#x2019;t interested in real numbers, just some rough ideas.&#xA;&#xA;&#x9;Now, let us see how we read it?&#xA;&#xA;&#x9;using (var env = new StorageEnvironment(new MemoryMapPager(&quot;bench.data&quot;)))&#xA;{&#xA;    using (var tx = env.NewTransaction(TransactionFlags.Read))&#xA;    {&#xA;        var ms = new MemoryStream(100);&#xA;        for (int i = 0; i &lt; Count; i&#x2B;&#x2B;)&#xA;        {&#xA;            var key = i.ToString(&quot;0000000000000000&quot;);&#xA;            using (var stream = env.Root.Read(tx, key))&#xA;            {&#xA;                ms.Position = 0;&#xA;                stream.CopyTo(ms);&#xA;            }&#xA;        }&#xA;&#xA;        tx.Commit();&#xA;    }&#xA;}&#xA;&#x9;&#xA;&#xA;&#xA;&#x9;And this gives us:&#xA;&#xA;&#x9;read seq &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; : &#xA0; &#xA0; &#xA0;3,289 ms &#xA0; &#xA0;304,032 ops / sec&#xA;&#xA;&#x9;And again, this is with opening &amp; closing the entire db.&#xA;&#xA;&#x9;We could do better with pre-allocation of space on the disk, etc. But I wanted to keep things realistic and to allow us to grow.&#xA;&#xA;&#x9;Next, I wanted to see how much we would gain by parallelizing everything, so I wrote the following code:&#xA;&#xA;&#x9;using (var env = new StorageEnvironment(new MemoryMapPager(&quot;bench.data&quot;)))&#xA;{&#xA;    var countdownEvent = new CountdownEvent(parts);&#xA;    for (int i = 0; i &lt; parts; i&#x2B;&#x2B;)&#xA;    {&#xA;        var currentBase = i;&#xA;        ThreadPool.QueueUserWorkItem(state =&gt;&#xA;        {&#xA;            using (var tx = env.NewTransaction(TransactionFlags.Read))&#xA;            {&#xA;                var ms = new MemoryStream(100);&#xA;                for (int j = 0; j &lt; Count / parts; j&#x2B;&#x2B;)&#xA;                {&#xA;                    var current = j * currentBase;&#xA;                    var key = current.ToString(&quot;0000000000000000&quot;);&#xA;                    using (var stream = env.Root.Read(tx, key))&#xA;                    {&#xA;                        ms.Position = 0;&#xA;                        stream.CopyTo(ms);&#xA;                    }&#xA;                }&#xA;&#xA;                tx.Commit();&#xA;            }&#xA;&#xA;            countdownEvent.Signal();&#xA;        });&#xA;    }&#xA;    countdownEvent.Wait();&#xA;}&#xA;&#x9;&#xA;&#xA;&#xA;&#x9;I then run it with 1 &#x2013; 16 parts, so see how it behaves. Here are the details for this machine:&#xA;&#xA;&#x9;&#xA;&#xA;&#x9;And the results pretty much match what I expected:&#xA;&#xA;&#x9;read seq &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; : &#xA0; &#xA0; &#xA0;3,317 ms &#xA0; &#xA0;301,424 ops / sec&#xA;&#xA;&#x9;read parallel 1 &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0;: &#xA0; &#xA0; &#xA0;2,539 ms &#xA0; &#xA0;393,834 ops / sec&#xA;&#xA;&#x9;read parallel 2 &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0;: &#xA0; &#xA0; &#xA0;1,950 ms &#xA0; &#xA0;512,711 ops / sec&#xA;&#xA;&#x9;read parallel 4 &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0;: &#xA0; &#xA0; &#xA0;2,201 ms &#xA0; &#xA0;454,172 ops / sec&#xA;&#xA;&#x9;read parallel 8 &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0;: &#xA0; &#xA0; &#xA0;2,139 ms &#xA0; &#xA0;467,387 ops / sec&#xA;&#xA;&#x9;read parallel 16 &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; : &#xA0; &#xA0; &#xA0;2,010 ms &#xA0; &#xA0;497,408 ops / sec&#xA;&#xA;&#x9;We get a 2x perf improvement from running on two cores, running on 4 threads require some dancing around, which caused some perf drop, then we see more time spent in thread switching than anything else, pretty much. As you can see, we see a really pretty jump in performance the more cores we use, until we reach the actual machine limitations.&#xA;&#xA;&#x9;Note that I made sure to clear the OS buffer cache before each test. If we don&#x27;t do that, we get:&#xA;&#xA;&#x9;read seq &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; : &#xA0; &#xA0; &#xA0;2,562 ms &#xA0; &#xA0;390,291 ops / sec&#xA;&#xA;&#x9;read parallel 1 &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0;: &#xA0; &#xA0; &#xA0;2,608 ms &#xA0; &#xA0;383,393 ops / sec&#xA;&#xA;&#x9;read parallel 2 &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0;: &#xA0; &#xA0; &#xA0;1,868 ms &#xA0; &#xA0;535,220 ops / sec&#xA;&#xA;&#x9;read parallel 4 &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0;: &#xA0; &#xA0; &#xA0;1,646 ms &#xA0; &#xA0;607,283 ops / sec&#xA;&#xA;&#x9;read parallel 8 &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0;: &#xA0; &#xA0; &#xA0;1,673 ms &#xA0; &#xA0;597,557 ops / sec&#xA;&#xA;&#x9;read parallel 16 &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; : &#xA0; &#xA0; &#xA0;1,581 ms &#xA0; &#xA0;632,309 ops / sec&#xA;&#xA;&#x9;So far, I am pretty happy with those numbers. What I am not happy is the current API, but I&#x2019;ll talk about this in a separate post.</p>
        </article>
        <article id="article-4074">
            <a href="https://ayende.com/blog/163457/optimizing-writes-in-voron" target="_blank">
                <h2 class="title mb-6" id="article-4074">Optimizing writes in Voron</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: September 19, 2013
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">As I mentioned, one of the things that I have been working on with Voron is optimizing the sad case of random writes.&#xA0; I discussed some of the issues that we had already, and now I want to explain how we approach resolving them. With LMDB, free space occur on every write, because we don&#x2019;t make modifications in place, instead, we make modifications to a copy, and free the existing page to be reclaimed later. The way the free space reclamation work, a new page can be allocated anywhere on the file. That can lead to a lot of seeks. With Voron, we used a more complex policy. The file is divided in 4 MB sections. And we will aggregate free space in each section. When we need more space, we will find a section with enough free space and use that, and we will continue to use that for as long as we can. The end result is that we tend to be much more local in the way we are reusing space. Here are the original results: Flush     1 with  12 pages   - 48 kb writes and 1  seeks   (11 leaves, 1 branches, 0 overflows)&#xA;Flush     2 with  13 pages   - 52 kb writes and 1  seeks   (12 leaves, 1 branches, 0 overflows)&#xA;Flush     3 with  21 pages   - 84 kb writes and 1  seeks   (20 leaves, 1 branches, 0 overflows)&#xA; &#xA;Flush    27 with  76 pages   - 304 kb writes and 1 seeks  (75 leaves,  1 branches, 0 overflows)&#xA;Flush    28 with  73 pages   - 292 kb writes and 1 seeks  (72 leaves,  1 branches, 0 overflows)&#xA;Flush    29 with  84 pages   - 336 kb writes and 1 seeks  (80 leaves,  4 branches, 0 overflows)&#xA; &#xA;Flush 1,153 with 158 pages - 632 kb writes and 67  seeks (107 leaves, 51 branches, 0 overflows)&#xA;Flush 1,154 with 168 pages - 672 kb writes and 65  seeks (113 leaves, 55 branches, 0 overflows)&#xA;Flush 1,155 with 165 pages - 660 kb writes and 76  seeks (113 leaves, 52 branches, 0 overflows)&#xA; &#xA;Flush 4,441 with 199 pages - 796 kb writes and 146 seeks (111 leaves, 88 branches, 0 overflows)&#xA;Flush 4,442 with 198 pages - 792 kb writes and 133 seeks (113 leaves, 85 branches, 0 overflows)&#xA;Flush 4,443 with 196 pages - 784 kb writes and 146 seeks (109 leaves, 87 branches, 0 overflows)&#xA; &#xA;Flush 7,707 with 209 pages - 836 kb writes and 170 seeks (111 leaves, 98 branches, 0 overflows)&#xA;Flush 7,708 with 217 pages - 868 kb writes and 169 seeks (119 leaves, 98 branches, 0 overflows)&#xA;Flush 7,709 with 197 pages - 788 kb writes and 162 seeks (108 leaves, 89 branches, 0 overflows)&#xA; &#xA;Flush 9,069 with 204 pages - 816 kb writes and 170 seeks (108 leaves, 96 branches, 0 overflows)&#xA;Flush 9,070 with 206 pages - 824 kb writes and 166 seeks (112 leaves, 94 branches, 0 overflows)&#xA;Flush 9,071 with 203 pages - 812 kb writes and 169 seeks (105 leaves, 98 branches, 0 overflows)&#xA;&#xA;&#xA;And here are the improved results:&#xA;Flush      1 with   2 pages -     8 kb writes and   1 seeks (  2 leaves,   0 branches,   0 overflows)&#xA;Flush      2 with   8 pages -    32 kb writes and   1 seeks (  7 leaves,   1 branches,   0 overflows)&#xA;Flush      3 with  10 pages -    40 kb writes and   1 seeks (  9 leaves,   1 branches,   0 overflows)&#xA;  &#xA;Flush     27 with  73 pages -   292 kb writes and   1 seeks ( 72 leaves,   1 branches,   0 overflows)&#xA;Flush     28 with  72 pages -   288 kb writes and   1 seeks ( 71 leaves,   1 branches,   0 overflows)&#xA;Flush     29 with  71 pages -   284 kb writes and   1 seeks ( 70 leaves,   1 branches,   0 overflows)&#xA;  &#xA;Flush  1,153 with 157 pages -   628 kb writes and  11 seeks (105 leaves,  52 branches,   0 overflows)&#xA;Flush  1,154 with 159 pages -   636 kb writes and   2 seeks (107 leaves,  52 branches,   0 overflows)&#xA;Flush  1,155 with 167 pages -   668 kb writes and  17 seeks (111 leaves,  56 branches,   0 overflows)&#xA;  &#xA;Flush  4,441 with 210 pages -   840 kb writes and  11 seeks (121 leaves,  86 branches,   3 overflows)&#xA;Flush  4,442 with 215 pages -   860 kb writes and   1 seeks (124 leaves,  88 branches,   3 overflows)&#xA;Flush  4,443 with 217 pages -   868 kb writes and   9 seeks (126 leaves,  89 branches,   2 overflows)&#xA;  &#xA;Flush  7,707 with 231 pages -   924 kb writes and   7 seeks (136 leaves,  93 branches,   2 overflows)&#xA;Flush  7,708 with 234 pages -   936 kb writes and   9 seeks (136 leaves,  97 branches,   1 overflows)&#xA;Flush  7,709 with 241 pages -   964 kb writes and  13 seeks (140 leaves,  97 branches,   4 overflows)&#xA;&#xA;Flush  9,069 with 250 pages - 1,000 kb writes and   6 seeks (144 leaves, 101 branches,   5 overflows)&#xA;Flush  9,070 with 250 pages - 1,000 kb writes and  13 seeks (145 leaves,  98 branches,   7 overflows)&#xA;Flush  9,071 with 248 pages -   992 kb writes and  12 seeks (143 leaves,  99 branches,   6 overflows)&#xA;&#xA;&#xA;Let us plot this in a chart, so we can get a better look at things:&#xA;&#xA;As you can see, this is a pretty major improvement. But it came at a cost, let us see the cost of size per transaction&#x2026;&#xA;&#xA;So we improved on the seeks / tx, but got worse on the size / tx. That is probably because of the overhead of keeping the state around, but it also relates to some tunable configuration that we added (the amount of free space in a section that will make it eligible for use.&#xA;Annoyingly, after spending quite a bit of time &amp; effort on this, we don&#x2019;t see a major perf boost here. But I am confident that it&#x2019;ll come.</p>
        </article>
        <article id="article-4075">
            <a href="https://ayende.com/blog/163426/field-tokenization-a-problem-analysis" target="_blank">
                <h2 class="title mb-6" id="article-4075">Field Tokenization: A problem analysis</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: September 18, 2013
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">I was asked about the problem of repeating property names in RavenDB. In particular, let us consider the following document:      1: {    2:     &quot;FirstName&quot;:&quot;John&quot;,   3:     &quot;LastName&quot;:&quot;Doe&quot;,   4:     &quot;BirthDay&quot;: &quot;1980-01-01&quot;,   5:     &quot;LastLoggedInAt&quot;: &quot;2013-08-10&quot;   6: }&#xA;There are 40 chars in this document that are dedicated solely for field names, and just 28 chars dedicate to actual data. Now, 40 vs 28 means that if we have 1 million of those documents, we are losing a lot of space for no good reason. Surely the database can do better on this, right? It can tokenize those things so instead of storing 40 bytes for fields, it would store only 16 (assuming int32 for token ids). And for larger documents, you would see an even more massive saving.&#xA;I think that I talked about this before, oh yes, here it is: http://ayende.com/blog/4669/you-saved-5-cents-and-your-code-is-not-readable-congrats, in which you manually (or via config at the client side) change the document to look like this:&#xA;&#xA;&#xA;   1: {    2:  &quot;FN&quot;:&quot;John&quot;,   3:  &quot;LN&quot;:&quot;Doe&quot;,   4:  &quot;BD&quot;: &quot;1980-01-01&quot;,   5:  &quot;LLD&quot;: &quot;2013-08-10&quot;   6: }&#xA;This is pretty horrible and I cover exactly why this is bad in the blob post above. Now, for 1 million documents, those 40 chars represent about 38MB of disk space. Not really a major issue, I think.&#xA;The actual question I was asked was (by Justin):&#xA;&#xA;Repeating keys is so silly MongoDB has a highly voted issue, marked major with a plan to fix. And many client libraries have code built that tries to shorten names for you:&#xA;https://jira.mongodb.org/browse/SERVER-863&#xA;I personally think reading and writing 2x or more data then necessary is not a silly issue, it impacts all aspects of performance and database management. Disk are cheap sure, then again if your database is smaller you might have been able to put it on a ssd or faster but smaller drive, or just transferred less bytes from that cheap disk to accomplish the same thing. Are you really going to tell me every byte doesn&#x27;t make a difference after examining these c code bases that do their best to save every byte for performance reasons?&#xA;The ugly solution shown above is a manual approach, but surely the db can do better, right? Interning property names is relatively simple conceptually, but a lot more complex in practice.&#xA;For example, if you want to do interning, you probably want to do it end to end. Which means that the client libs need to understand the server tokens. That means that you need some way of informing the client when there is a new interned property added, otherwise they wouldn&#x27;t be able to figure out the real property name. Then there is the issue of how you are going to handle sharding or replication. Is each server going to have its unique copy of the intern table? Is each client going to have to navigate that? Or are you going to try to have a single intern table, and somehow manage to agree on all the positions in that table among a distributed set of nodes? &#xA;Imagine what going to happen during failover, you suddenly need to get the intern table for each of the replicas. And making sure that you aren&#x27;t using the wrong intern table is going to be fun. Not to mention what happens when you have different clients talking to different servers, especially if you had a network split.&#xA;Even assuming that you don&#x27;t care for that, and only want this for saving disk I/O on the server, you are going to run into issue with concurrency management with regards to the itern list. If you have two transaction adding values that both require modifying the intern list, you are in an interesting problem. Not least of which because if you save just the property name id to the data store, instead of the full key, you have to be able to say that this is there if the transaction is committed, because otherwise you have an unknown (or wrong) property name when you read it back. So now you have concurrency issues, and the potential for conflicts because two transactions may be modifying the values of the intern table at the same time. For fun, assume that they both have documents with the same property name that needs to be added, as well as other property names.&#xA;And that doesn&#x27;t even take into account that it is actually pretty common to have dynamic property names. For example, dates &amp; times. You have 1 million documents, each of them have a property with the names like: &quot;2013-08-03T19:57:32.7060000Z&quot;&#xA0; A common scenario where this occurs is tracking things by time. For instance, if you want to track the amount of hours you worked on a bug. The document might look like this:&#xA;&#xA;   1: {   2:   &quot;2013-08-03T19:57:32.7060000Z&quot;:{   3:      &quot;Project&quot;:&quot;RavenDB&quot;,   4:      &quot;RavenDB-1248&quot;:&quot;Fixed&quot;,   5:      &quot;RavenDB-1212&quot;:&quot;Progress made&quot;   6:   }   7: }&#xA;As you can see, we actually have multiple properties with dynamic names here. It would be a waste to try to intern them. In fact, it is likely going to cause us to fall over and die. And wait for that to happen when you have to pass this to the client... fun &amp; joy all around.&#xA;But you know what, I can still probably go and figure out what the most 10,000 common field names are, and have a fixed table of those in place. This would give me the ability to handle things like Name, FirstName, Date, etc. Because it is a fixed list, it would means that a lot of the problems listed above don&#x2019;t matter. It would mean that if you wanted to call your property FirstNames, you might need to call it Names, because the first one would be in the list and won&#x2019;t be interned. &#xA;And that leads to a lot of interesting potential issues with backward and forward compatibility. How do I add more terms to the list? Older clients wouldn&#x2019;t be able to understand the tokens, and that leads to additional complication just managing the versioning story. And I can assure you that you&#x2019;ll have a lot of people clamoring and wanting to add their own unique properties to that list. (What do you mean &#x2018;PensionFundTarget&#x2019; isn&#x2019;t on the intern list, is a a really common term and we could really use the perf boost of that!).&#xA;And then we have the issue of debugging. Right now, you can watch what is going on over the wire using Fiddler very easily, and the values are easily understood. Do you really want to try to figure out data like this?&#xA;&#xA;&#xA;   1: {    2:  &quot;231&quot;:&quot;John&quot;,   3:  &quot;432&quot;:&quot;Doe&quot;,   4:  &quot;127&quot;: &quot;1980-01-01&quot;,   5:  &quot;5841&quot;: &quot;2013-08-10&quot;   6: }&#xA;In pretty much every aspect you can think of, this is a really crazy hard feature, and the benefits that it brings aren&#x2019;t really that important.&#xA;Sure, you can reduce the amount of data that you read from the disk. But that data is already sitting in the same place. And there is absolutely no difference between reading a 30 bytes value and a 200 bytes value (reads/writes are always done at a minimum of 512 bytes). So reading a bit of extra bytes means pretty much nothing to us. We don&#x2019;t have to seek to it, it is already there. And anyway, we have caches to alleviate that problem for us. &#xA;And reading from the disk isn&#x2019;t really our major problem, that is relatively cheap compare to sending the data to the user over the network. But wait, we actually have found a solution for that, we use gzip compression on the wire, a supported and well known method that you can easily see through with tools like Fiddler. That usually gives you the best of all worlds. You get small response size, you still use readable format, you don&#x2019;t have to deal with all of the issues mentioned above. Happy happy joy joy!&#xA;Hm&#x2026; can we not apply the same method internally on the server side? Sure we can. We can compress &amp; decompress data on the fly when we write / read to the disk, further reducing the amount of data that we write. &#xA;In RavenDB, we do both. Data is compressed on the wire, and can be compressed to the disk as well. &#xA;As for the MongoDB issue, it may be highly voted, it may be marked as major, but it has been sitting in the issue tracker for the past 3&#x2B; years. I would say that they aren&#x2019;t in any rush to resolve this. Probably for much the same reasons as I outlined here. It is cheap to vote for an issue, and usually the people setting the issue severity are the people creating it. I wouldn&#x2019;t put too much stock on anything like that.&#xA;Put simply, trying to solve this is a really hard problem, with a lot of cascading implication and dangerous side effects. Conversely there are more effective solutions that are cheaper, simpler and are orthogonal to everything else.&#xA;I think you can guess what I would pick.</p>
        </article>
        <article id="article-4076">
            <a href="https://ayende.com/blog/163425/json-packing-text-based-formats-and-other-stuff-that-come-to-mind-at-5-am" target="_blank">
                <h2 class="title mb-6" id="article-4076">JSON Packing, Text Based Formats and other stuff that come to mind at 5 AM</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: September 17, 2013
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">This post was written at 5:30AM, I run into this while doing research for another post, and I couldn&#x2019;t really let it go. XML as a text base format is really wasteful in space. But that wasn&#x2019;t what really made it lose its shine. That was when it became so complex that it stopped being human readable. For example, I give you:      1: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;   2:  &lt;SOAP-ENV:Envelope   3:   xmlns:xsi=&quot;http://www.w3.org/1999/XMLSchema-instance&quot;   4:   xmlns:xsd=&quot;http://www.w3.org/1999/XMLSchema&quot;   5:   xmlns:SOAP-ENV=&quot;http://schemas.xmlsoap.org/soap/envelope/&quot;&gt;   6:    &lt;SOAP-ENV:Body&gt;   7:        &lt;ns1:getEmployeeDetailsResponse   8:         xmlns:ns1=&quot;urn:MySoapServices&quot;   9:         SOAP-ENV:encodingStyle=&quot;http://schemas.xmlsoap.org/soap/encoding/&quot;&gt;  10:            &lt;return xsi:type=&quot;ns1:EmployeeContactDetail&quot;&gt;  11:                &lt;employeeName xsi:type=&quot;xsd:string&quot;&gt;Bill Posters&lt;/employeeName&gt;  12:                &lt;phoneNumber xsi:type=&quot;xsd:string&quot;&gt;&#x2B;1-212-7370194&lt;/phoneNumber&gt;  13:                &lt;tempPhoneNumber  14:                 xmlns:ns2=&quot;http://schemas.xmlsoap.org/soap/encoding/&quot;  15:                 xsi:type=&quot;ns2:Array&quot;  16:                 ns2:arrayType=&quot;ns1:TemporaryPhoneNumber[3]&quot;&gt;  17:                    &lt;item xsi:type=&quot;ns1:TemporaryPhoneNumber&quot;&gt;  18:                        &lt;startDate xsi:type=&quot;xsd:int&quot;&gt;37060&lt;/startDate&gt;  19:                        &lt;endDate xsi:type=&quot;xsd:int&quot;&gt;37064&lt;/endDate&gt;  20:                        &lt;phoneNumber xsi:type=&quot;xsd:string&quot;&gt;&#x2B;1-515-2887505&lt;/phoneNumber&gt;  21:                    &lt;/item&gt;  22:                    &lt;item xsi:type=&quot;ns1:TemporaryPhoneNumber&quot;&gt;  23:                        &lt;startDate xsi:type=&quot;xsd:int&quot;&gt;37074&lt;/startDate&gt;  24:                        &lt;endDate xsi:type=&quot;xsd:int&quot;&gt;37078&lt;/endDate&gt;  25:                        &lt;phoneNumber xsi:type=&quot;xsd:string&quot;&gt;&#x2B;1-516-2890033&lt;/phoneNumber&gt;  26:                    &lt;/item&gt;  27:                    &lt;item xsi:type=&quot;ns1:TemporaryPhoneNumber&quot;&gt;  28:                        &lt;startDate xsi:type=&quot;xsd:int&quot;&gt;37088&lt;/startDate&gt;  29:                        &lt;endDate xsi:type=&quot;xsd:int&quot;&gt;37092&lt;/endDate&gt;  30:                        &lt;phoneNumber xsi:type=&quot;xsd:string&quot;&gt;&#x2B;1-212-7376609&lt;/phoneNumber&gt;  31:                    &lt;/item&gt;  32:                &lt;/tempPhoneNumber&gt;  33:            &lt;/return&gt;  34:        &lt;/ns1:getEmployeeDetailsResponse&gt;  35:    &lt;/SOAP-ENV:Body&gt;  36: /SOAP-ENV:Envelope&gt;&#xA;After XML was thrown out of the company of respectable folks, we had JSON show up and entertain us. It is smaller and more concise than XML, and so far has resisted the efforts to make it into some sort of a uber complex enterprisiey tool. &#xA;But today I run into quite a few effort to do strange things to JSON. I am talking about things like JSON DB (a compressed json format, not actual json database), JSONH, json.hpack, and friends. All of those attempt to reduce the size of JSON documents.&#xA;Let us take an example. the following is a JSON document representing one of RavenDB builds:&#xA;&#xA;&#xA;   1: {   2:   &quot;BuildName&quot;: &quot;RavenDB Unstable v2.5&quot;,   3:   &quot;IsUnstable&quot;: true,   4:   &quot;Version&quot;: &quot;2509-Unstable&quot;,   5:   &quot;PublishedAt&quot;: &quot;2013-02-26T12:06:12.0000000&quot;,   6:   &quot;DownloadsIds&quot;: [],   7:   &quot;Changes&quot;: [   8:     {   9:       &quot;Commiter&quot;: {  10:         &quot;Email&quot;: &quot;david@davidwalker.org&quot;,  11:         &quot;Name&quot;: &quot;David Walker&quot;  12:       },  13:       &quot;Version&quot;: &quot;17c661cb158d5e3c528fe2c02a3346305f0234a3&quot;,  14:       &quot;Href&quot;: &quot;/app/rest/changes/id:21039&quot;,  15:       &quot;TeamCityId&quot;: 21039,  16:       &quot;Username&quot;: &quot;david walker&quot;,  17:       &quot;Comment&quot;: &quot;Do not save Has-Api-Key header to metadata\n&quot;,  18:       &quot;Date&quot;: &quot;2013-02-20T23:22:43.0000000&quot;,  19:       &quot;Files&quot;: [  20:         &quot;Raven.Abstractions/Extensions/MetadataExtensions.cs&quot;  21:       ]  22:     },  23:     {  24:       &quot;Commiter&quot;: {  25:         &quot;Email&quot;: &quot;david@davidwalker.org&quot;,  26:         &quot;Name&quot;: &quot;David Walker&quot;  27:       },  28:       &quot;Version&quot;: &quot;5ffb4d61ad9102696948f6678bbecac88e1dc039&quot;,  29:       &quot;Href&quot;: &quot;/app/rest/changes/id:21040&quot;,  30:       &quot;TeamCityId&quot;: 21040,  31:       &quot;Username&quot;: &quot;david walker&quot;,  32:       &quot;Comment&quot;: &quot;Do not save IIS Application Request Routing headers to metadata\n&quot;,  33:       &quot;Date&quot;: &quot;2013-02-20T23:23:59.0000000&quot;,  34:       &quot;Files&quot;: [  35:         &quot;Raven.Abstractions/Extensions/MetadataExtensions.cs&quot;  36:       ]  37:     },  38:     {  39:       &quot;Commiter&quot;: {  40:         &quot;Email&quot;: &quot;ayende@ayende.com&quot;,  41:         &quot;Name&quot;: &quot;Ayende Rahien&quot;  42:       },  43:       &quot;Version&quot;: &quot;5919521286735f50f963824a12bf121cd1df4367&quot;,  44:       &quot;Href&quot;: &quot;/app/rest/changes/id:21035&quot;,  45:       &quot;TeamCityId&quot;: 21035,  46:       &quot;Username&quot;: &quot;ayende rahien&quot;,  47:       &quot;Comment&quot;: &quot;Better disposal\n&quot;,  48:       &quot;Date&quot;: &quot;2013-02-26T10:16:45.0000000&quot;,  49:       &quot;Files&quot;: [  50:         &quot;Raven.Client.WinRT/MissingFromWinRT/ThreadSleep.cs&quot;  51:       ]  52:     },  53:     {  54:       &quot;Commiter&quot;: {  55:         &quot;Email&quot;: &quot;ayende@ayende.com&quot;,  56:         &quot;Name&quot;: &quot;Ayende Rahien&quot;  57:       },  58:       &quot;Version&quot;: &quot;c93264e2a94e2aa326e7308ab3909aa4077bc3bb&quot;,  59:       &quot;Href&quot;: &quot;/app/rest/changes/id:21036&quot;,  60:       &quot;TeamCityId&quot;: 21036,  61:       &quot;Username&quot;: &quot;ayende rahien&quot;,  62:       &quot;Comment&quot;: &quot;Will ensure that the value is always positive or zero (never negative).\nWhen using numeric calc, will div by 1,024 to get more concentration into buckets.\n&quot;,  63:       &quot;Date&quot;: &quot;2013-02-26T10:17:23.0000000&quot;,  64:       &quot;Files&quot;: [  65:         &quot;Raven.Database/Indexing/IndexingUtil.cs&quot;  66:       ]  67:     },  68:     {  69:       &quot;Commiter&quot;: {  70:         &quot;Email&quot;: &quot;ayende@ayende.com&quot;,  71:         &quot;Name&quot;: &quot;Ayende Rahien&quot;  72:       },  73:       &quot;Version&quot;: &quot;7bf51345d39c3993fed5a82eacad6e74b9201601&quot;,  74:       &quot;Href&quot;: &quot;/app/rest/changes/id:21037&quot;,  75:       &quot;TeamCityId&quot;: 21037,  76:       &quot;Username&quot;: &quot;ayende rahien&quot;,  77:       &quot;Comment&quot;: &quot;Fixing a bug where we wouldn&#x27;t decrement reduce stats for an index when multiple values from the same bucket are removed\n&quot;,  78:       &quot;Date&quot;: &quot;2013-02-26T10:53:01.0000000&quot;,  79:       &quot;Files&quot;: [  80:         &quot;Raven.Database/Indexing/MapReduceIndex.cs&quot;,  81:         &quot;Raven.Database/Storage/Esent/StorageActions/MappedResults.cs&quot;,  82:         &quot;Raven.Database/Storage/IMappedResultsStorageAction.cs&quot;,  83:         &quot;Raven.Database/Storage/Managed/MappedResultsStorageAction.cs&quot;,  84:         &quot;Raven.Tests/Issues/RavenDB_784.cs&quot;,  85:         &quot;Raven.Tests/Storage/MappedResults.cs&quot;,  86:         &quot;Raven.Tests/Views/ViewStorage.cs&quot;  87:       ]  88:     },  89:     {  90:       &quot;Commiter&quot;: {  91:         &quot;Email&quot;: &quot;ayende@ayende.com&quot;,  92:         &quot;Name&quot;: &quot;Ayende Rahien&quot;  93:       },  94:       &quot;Version&quot;: &quot;ff2c5b43eba2a8a2206152658b5e76706e12945c&quot;,  95:       &quot;Href&quot;: &quot;/app/rest/changes/id:21038&quot;,  96:       &quot;TeamCityId&quot;: 21038,  97:       &quot;Username&quot;: &quot;ayende rahien&quot;,  98:       &quot;Comment&quot;: &quot;No need for so many repeats\n&quot;,  99:       &quot;Date&quot;: &quot;2013-02-26T11:27:49.0000000&quot;, 100:       &quot;Files&quot;: [ 101:         &quot;Raven.Tests/Bugs/MultiOutputReduce.cs&quot; 102:       ] 103:     }, 104:     { 105:       &quot;Commiter&quot;: { 106:         &quot;Email&quot;: &quot;ayende@ayende.com&quot;, 107:         &quot;Name&quot;: &quot;Ayende Rahien&quot; 108:       }, 109:       &quot;Version&quot;: &quot;0620c74e51839972554fab3fa9898d7633cfea6e&quot;, 110:       &quot;Href&quot;: &quot;/app/rest/changes/id:21041&quot;, 111:       &quot;TeamCityId&quot;: 21041, 112:       &quot;Username&quot;: &quot;ayende rahien&quot;, 113:       &quot;Comment&quot;: &quot;Merge branch &#x27;master&#x27; of https://github.com/cloudbirdnet/ravendb into 2.1\n&quot;, 114:       &quot;Date&quot;: &quot;2013-02-26T11:41:39.0000000&quot;, 115:       &quot;Files&quot;: [ 116:         &quot;Raven.Abstractions/Extensions/MetadataExtensions.cs&quot; 117:       ] 118:     } 119:   ], 120:   &quot;ResolvedIssues&quot;: [], 121:   &quot;Contributors&quot;: [ 122:     { 123:       &quot;FullName&quot;: &quot;Ayende Rahien&quot;, 124:       &quot;Email&quot;: &quot;ayende@ayende.com&quot;, 125:       &quot;EmailHash&quot;: &quot;730a9f9186e14b8da5a4e453aca2adfe&quot; 126:     }, 127:     { 128:       &quot;FullName&quot;: &quot;David Walker&quot;, 129:       &quot;Email&quot;: &quot;david@davidwalker.org&quot;, 130:       &quot;EmailHash&quot;: &quot;4e5293ab04bc1a4fdd62bd06e2f32871&quot; 131:     } 132:   ], 133:   &quot;BuildTypeId&quot;: &quot;bt8&quot;, 134:   &quot;Href&quot;: &quot;/app/rest/builds/id:588&quot;, 135:   &quot;ProjectName&quot;: &quot;RavenDB&quot;, 136:   &quot;TeamCityId&quot;: 588, 137:   &quot;ProjectId&quot;: &quot;project3&quot;, 138:   &quot;Number&quot;: 2509 139: }&#xA;This document is 4.52KB in size. Running this through JSONH gives us the following:&#xA;&#xA;&#xA;   1: [   2:     14,   3:     &quot;BuildName&quot;,   4:     &quot;IsUnstable&quot;,   5:     &quot;Version&quot;,   6:     &quot;PublishedAt&quot;,   7:     &quot;DownloadsIds&quot;,   8:     &quot;Changes&quot;,   9:     &quot;ResolvedIssues&quot;,  10:     &quot;Contributors&quot;,  11:     &quot;BuildTypeId&quot;,  12:     &quot;Href&quot;,  13:     &quot;ProjectName&quot;,  14:     &quot;TeamCityId&quot;,  15:     &quot;ProjectId&quot;,  16:     &quot;Number&quot;,  17:     &quot;RavenDB Unstable v2.5&quot;,  18:     true,  19:     &quot;2509-Unstable&quot;,  20:     &quot;2013-02-26T12:06:12.0000000&quot;,  21:     [  22:     ],  23:     [  24:         {  25:             &quot;Commiter&quot;: {  26:                 &quot;Email&quot;: &quot;david@davidwalker.org&quot;,  27:                 &quot;Name&quot;: &quot;David Walker&quot;  28:             },  29:             &quot;Version&quot;: &quot;17c661cb158d5e3c528fe2c02a3346305f0234a3&quot;,  30:             &quot;Href&quot;: &quot;/app/rest/changes/id:21039&quot;,  31:             &quot;TeamCityId&quot;: 21039,  32:             &quot;Username&quot;: &quot;david walker&quot;,  33:             &quot;Comment&quot;: &quot;Do not save Has-Api-Key header to metadata\n&quot;,  34:             &quot;Date&quot;: &quot;2013-02-20T23:22:43.0000000&quot;,  35:             &quot;Files&quot;: [  36:                 &quot;Raven.Abstractions/Extensions/MetadataExtensions.cs&quot;  37:             ]  38:         },  39:         {  40:             &quot;Commiter&quot;: {  41:                 &quot;Email&quot;: &quot;david@davidwalker.org&quot;,  42:                 &quot;Name&quot;: &quot;David Walker&quot;  43:             },  44:             &quot;Version&quot;: &quot;5ffb4d61ad9102696948f6678bbecac88e1dc039&quot;,  45:             &quot;Href&quot;: &quot;/app/rest/changes/id:21040&quot;,  46:             &quot;TeamCityId&quot;: 21040,  47:             &quot;Username&quot;: &quot;david walker&quot;,  48:             &quot;Comment&quot;: &quot;Do not save IIS Application Request Routing headers to metadata\n&quot;,  49:             &quot;Date&quot;: &quot;2013-02-20T23:23:59.0000000&quot;,  50:             &quot;Files&quot;: [  51:                 &quot;Raven.Abstractions/Extensions/MetadataExtensions.cs&quot;  52:             ]  53:         },  54:         {  55:             &quot;Commiter&quot;: {  56:                 &quot;Email&quot;: &quot;ayende@ayende.com&quot;,  57:                 &quot;Name&quot;: &quot;Ayende Rahien&quot;  58:             },  59:             &quot;Version&quot;: &quot;5919521286735f50f963824a12bf121cd1df4367&quot;,  60:             &quot;Href&quot;: &quot;/app/rest/changes/id:21035&quot;,  61:             &quot;TeamCityId&quot;: 21035,  62:             &quot;Username&quot;: &quot;ayende rahien&quot;,  63:             &quot;Comment&quot;: &quot;Better disposal\n&quot;,  64:             &quot;Date&quot;: &quot;2013-02-26T10:16:45.0000000&quot;,  65:             &quot;Files&quot;: [  66:                 &quot;Raven.Client.WinRT/MissingFromWinRT/ThreadSleep.cs&quot;  67:             ]  68:         },  69:         {  70:             &quot;Commiter&quot;: {  71:                 &quot;Email&quot;: &quot;ayende@ayende.com&quot;,  72:                 &quot;Name&quot;: &quot;Ayende Rahien&quot;  73:             },  74:             &quot;Version&quot;: &quot;c93264e2a94e2aa326e7308ab3909aa4077bc3bb&quot;,  75:             &quot;Href&quot;: &quot;/app/rest/changes/id:21036&quot;,  76:             &quot;TeamCityId&quot;: &quot;...bug where we wouldn&#x27;t decrement reduce stats for an index when multiple values from the same bucket are removed\n&quot;,  77:             &quot;Date&quot;: &quot;2013-02-26T10:53:01.0000000&quot;,  78:             &quot;Files&quot;: [  79:                 &quot;Raven.Database/Indexing/MapReduceIndex.cs&quot;,  80:                 &quot;Raven.Database/Storage/Esent/StorageActions/MappedResults.cs&quot;,  81:                 &quot;Raven.Database/Storage/IMappedResultsStorageAction.cs&quot;,  82:                 &quot;Raven.Database/Storage/Managed/MappedResultsStorageAction.cs&quot;,  83:                 &quot;Raven.Tests/Issues/RavenDB_784.cs&quot;,  84:                 &quot;Raven.Tests/Storage/MappedResults.cs&quot;,  85:                 &quot;Raven.Tests/Views/ViewStorage.cs&quot;  86:             ]  87:         },  88:         {  89:             &quot;Commiter&quot;: {  90:                 &quot;Email&quot;: &quot;ayende@ayende.com&quot;,  91:                 &quot;Name&quot;: &quot;Ayende Rahien&quot;  92:             },  93:             &quot;Version&quot;: &quot;ff2c5b43eba2a8a2206152658b5e76706e12945c&quot;,  94:             &quot;Href&quot;: &quot;/app/rest/changes/id:21038&quot;,  95:             &quot;TeamCityId&quot;: 21038,  96:             &quot;Username&quot;: &quot;ayende rahien&quot;,  97:             &quot;Comment&quot;: &quot;No need for so many repeats\n&quot;,  98:             &quot;Date&quot;: &quot;2013-02-26T11:27:49.0000000&quot;,  99:             &quot;Files&quot;: [ 100:                 &quot;Raven.Tests/Bugs/MultiOutputReduce.cs&quot; 101:             ] 102:         }, 103:         { 104:             &quot;Commiter&quot;: { 105:                 &quot;Email&quot;: &quot;ayende@ayende.com&quot;, 106:                 &quot;Name&quot;: &quot;Ayende Rahien&quot; 107:             }, 108:             &quot;Version&quot;: &quot;0620c74e51839972554fab3fa9898d7633cfea6e&quot;, 109:             &quot;Href&quot;: &quot;/app/rest/changes/id:21041&quot;, 110:             &quot;TeamCityId&quot;: 21041, 111:             &quot;Username&quot;: &quot;ayende rahien&quot;, 112:             &quot;Comment&quot;: &quot;Merge branch &#x27;master&#x27; of https://github.com/cloudbirdnet/ravendb into 2.1\n&quot;, 113:             &quot;Date&quot;: &quot;2013-02-26T11:41:39.0000000&quot;, 114:             &quot;Files&quot;: [ 115:                 &quot;Raven.Abstractions/Extensions/MetadataExtensions.cs&quot; 116:             ] 117:         } 118:     ], 119:     [ 120:     ], 121:     [ 122:         { 123:             &quot;FullName&quot;: &quot;Ayende Rahien&quot;, 124:             &quot;Email&quot;: &quot;ayende@ayende.com&quot;, 125:             &quot;EmailHash&quot;: &quot;730a9f9186e14b8da5a4e453aca2adfe&quot; 126:         }, 127:         { 128:             &quot;FullName&quot;: &quot;David Walker&quot;, 129:             &quot;Email&quot;: &quot;david@davidwalker.org&quot;, 130:             &quot;EmailHash&quot;: &quot;4e5293ab04bc1a4fdd62bd06e2f32871&quot; 131:         } 132:     ], 133:     &quot;bt8&quot;, 134:     &quot;/app/rest/builds/id:588&quot;, 135:     &quot;RavenDB&quot;, 136:     588, 137:     &quot;project3&quot;, 138:     2509 139: ]&#xA;It reduced the document size to 2.93KB! Awesome, nearly half of the size was gone. Except: This is actually generating utterly unreadable mess. I mean, can you look at this and figure out what the hell is going on. &#xA;I thought not. At this point, we might as well use a binary format. I happen to have a zip tool at my disposal, so I checked what would happen if I threw this through that. The end result was a file that was 1.42KB. And I had no more loss of readability than I have with the JSONH stuff.&#xA;To be frank, I just don&#x2019;t get efforts like this. JSON is a text base human readable format. If you lose the human readable portion of the format, you might as well drop directly to binary. It is likely to be more efficient and you don&#x2019;t lose anything by it.&#xA;And if you want to compress your data, it is probably better to use something like a compression tool. HTTP Compression, for example, is practically free, since all servers and clients should be able to consume it now. And any tool that you use should be able to inspect through it. And it is likely to generate much better results on your JSON documents than if you will try a clever format like this.</p>
        </article>
        <article id="article-4077">
            <a href="https://ayende.com/blog/163394/new-interview-question" target="_blank">
                <h2 class="title mb-6" id="article-4077">New interview question</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: September 16, 2013
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">So, I think that I run through enough string sorting and tax calculations. My next interview question is going to be:  Given a finite set of unique numbers, find all the runs in the set. Runs are 1 or more consecutive numbers. That is, given {1,59,12,43,4,58,5,13,46,3,6}, the output should be: {1}, {3,4,5,6}, {12,13}, {43}, {46},{58,59}. Note that the size of the set may be very large.  This seems pretty simple, as a question, and should introduce interesting results.  Just to give you some idea, this is a real world problem we run into. We need to find runs in a sequence of freed pages so we can optimize sequential I/O.</p>
        </article>
        <article id="article-4078">
            <a href="https://ayende.com/blog/163393/the-storage-wars-shadow-paging-log-structured-merge-and-write-ahead-logging" target="_blank">
                <h2 class="title mb-6" id="article-4078">The storage wars: Shadow Paging, Log Structured Merge and Write Ahead Logging</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: September 13, 2013
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">I&#x2019;ve been doing a lot of research lately on storage. And in general, it seems that the most popular ways of writing to disk today are divide into the following categories.  Write Ahead Logging (WAL)&#x2013;&#xA0; Many databases use some sort of variant on that.&#xA0; PostgreSQL, SQLite, MongoDB, SQL Server, etc. Oracle has Redo Log, which seems similar, but I didn&#x2019;t check too deeply. Log Structured Merge&#xA0; (LSM)&#x2013; a lot of NoSQL databases use this method. Cassandra, Riak, LevelDB, SQLite 4, etc. Shadow Paging &#x2013; was quite popular a long time ago (80s), but still somewhat in use. LMDB, Tokyo Cabinet, CoucbDB (sort of). WAL came into being for a very simple reason, it is drastically faster to write sequentially than it is to do random writes. Let us assume that you store the data on disk using some sort of a tree, when you need to insert / update something in that tree, the record can be anywhere. That means that you would need to do random writes, and have to suffer the perf issues associated with that. Instead, you can write to the log and have some sort of a background process that would update the on disk data.  It also means that you really only have to update in memory data, flush the log and you are safe. The recovery procedure is going to be pretty complex, but it gives you some nice performance. Note that you write everything at least twice, once for the log, and once for the read data file. The log writes are sequential, the data writes are random. LSM also take advantage of sequential write speeds, but it takes it even further, instead of updating the actual data, you will wait until the log gets to a certain size, at which point you are going to merge it with the current data file(s). That means that you you will usually write things multiple times, in LevelDB, for example, a lot of the effort has actually gone into eradicating this cost. The cost of compacting your data. Because what ended up happening is that you have user writes competing with the compaction writes. Shadow Paging is not actually trying to optimize sequential writes. Well, that is not really fair. Shadow Paging &amp; sequential writes are just not related. The reason I said CouchDB is sort of using shadow paging is that it is using the exact same mechanics as other shadow paging system, but it always write at the end of the file. That means that is has excellent write speed, but it also means that it needs some way to reduce space. And that means it uses compaction, which brings you right back to the competing write story. For our purposes, we will ignore the way CouchDB work and focus on systems that works like LMDB. In those sort of systems, instead of modifying the data directly, we create a shadow page (copy on write) and modify that. Because the shadow page is only wired up to the rest of the pages on commit, this is absolutely atomic. It also means that modifying a page is going to use one page, and leave another free (the old page). And that, in turn, means that you need to have some way of scavenging for free space. CouchDB does that by creating a whole new file.  LMDB does that by recording the free space and reusing that in the next transaction. That means that writes to LMDB can happen anywhere. We can apply policies on top of that to mitigate that, but that is beside the point. Let us go back to another important aspect that we have to deal with in databases. Backups. As it turn out, it is actually really simple for most LSM / WAL systems to implement that, because you can just use the logs. For LMDB, you can create a backup really easily (in fact, since we are using shadow paging, you pretty much get it for free). However, one feature that I don&#x2019;t think would be possible with LMDB would be incremental backups. WAL/LSM make it easy, just take the logs since a given point. But with LMDB style dbs, I don&#x2019;t think that this would be possible.</p>
        </article>
        <article id="article-4079">
            <a href="https://ayende.com/blog/163362/seek-and-you-shall-find-or-maybe-delay-a-lot" target="_blank">
                <h2 class="title mb-6" id="article-4079">Seek, and you shall find, or maybe delay a lot</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: September 12, 2013
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">I have been doing a lot more analysis about the actual disk access patterns that we saw in Voron. In the beginning, I strongly suspected that the problem was with how I was using memory mapped files. In particular, some experiments with using my better knowledge of the environment has led to substantial performance gains. Instead of calling FlushViewOfFile(0, fileLen), I was able to call FlushViewOfFile on just the ranges that I knew changed. That helped, but it wasn&#x2019;t nearly enough. So I run some quick tests using file stream, and realized that the fault was obviously with the broken way Windows is using Memory Mapped files. So I decided to (no, I didn&#x2019;t write my own mmap impl, thank you very much) to take manual care of how Voron is writing to disk. I used WriteFile with all the bells and whistles, even had async I/O for a while there. I was able to directly pinpoint the exact locations where we needed to write, pass that to Windows in an efficient manner, and be done with it. It required me to write malloc implementation in managed code, but it was quite efficient looking code. And then I run it. Just to give you some perspective, the scenario under question here is 10,000 transactions doing 100 random writes each. Using the memory map approach, after the FlushViewOfFile range optimization, I got roughly 7,000 operations / second. Using my own hand written, optimized I/O, I got&#x2026; 262 operations / second. Okaaaay&#x2026; so maybe I need to go back to the drawing board. I sat down and started working on figuring out what is actually going on. I looked at the actual output that we had, in particular, how many writes did we have per transaction? I sat down to analyze what is going on. We are writing 100 records with 16 bytes key and 100 bytes value. That means that the absolute minimum amount we can write would be 11,600 bytes. However, we are writing in 4Kb pages, which bring us to 3 pages and 12,288 bytes per transaction. Of course, this ignore things like the writing of branch pages in the B&#x2B;Tree, so let us see what the real numbers are.      1: Flush     1 with  12 pages   - 48 kb writes and 1  seeks   (11 leaves, 1 branches, 0 overflows)   2: Flush     2 with  13 pages   - 52 kb writes and 1  seeks   (12 leaves, 1 branches, 0 overflows)   3: Flush     3 with  21 pages   - 84 kb writes and 1  seeks   (20 leaves, 1 branches, 0 overflows)   4:     5: Flush    27 with  76 pages   - 304 kb writes and 1 seeks  (75 leaves,  1 branches, 0 overflows)   6: Flush    28 with  73 pages   - 292 kb writes and 1 seeks  (72 leaves,  1 branches, 0 overflows)   7: Flush    29 with  84 pages   - 336 kb writes and 1 seeks  (80 leaves,  4 branches, 0 overflows)   8:&#xA0;    9: Flush 1,153 with 158 pages - 632 kb writes and 67  seeks (107 leaves, 51 branches, 0 overflows)  10: Flush 1,154 with 168 pages - 672 kb writes and 65  seeks (113 leaves, 55 branches, 0 overflows)  11: Flush 1,155 with 165 pages - 660 kb writes and 76  seeks (113 leaves, 52 branches, 0 overflows)  12:&#xA0;   13: Flush 4,441 with 199 pages - 796 kb writes and 146 seeks (111 leaves, 88 branches, 0 overflows)  14: Flush 4,442 with 198 pages - 792 kb writes and 133 seeks (113 leaves, 85 branches, 0 overflows)  15: Flush 4,443 with 196 pages - 784 kb writes and 146 seeks (109 leaves, 87 branches, 0 overflows)  16:&#xA0;   17: Flush 7,707 with 209 pages - 836 kb writes and 170 seeks (111 leaves, 98 branches, 0 overflows)  18: Flush 7,708 with 217 pages - 868 kb writes and 169 seeks (119 leaves, 98 branches, 0 overflows)  19: Flush 7,709 with 197 pages - 788 kb writes and 162 seeks (108 leaves, 89 branches, 0 overflows)  20:&#xA0;   21: Flush 9,069 with 204 pages - 816 kb writes and 170 seeks (108 leaves, 96 branches, 0 overflows)  22: Flush 9,070 with 206 pages - 824 kb writes and 166 seeks (112 leaves, 94 branches, 0 overflows)  23: Flush 9,071 with 203 pages - 812 kb writes and 169 seeks (105 leaves, 98 branches, 0 overflows)&#xA;The very first transactions are already showing something very interesting, we are actually writing 12 - 21 pages, or 48&#xA0; - 84 Kb of data, instead of 12 Kb. Why do we write 4 times as much data as we wanted?&#xA;The answer is that we are writing data that is random in nature, so it can&#x2019;t all sit in the same page, we get a lot of page splits and very quickly we end up with a lot of pages. This is pretty much inevitable, since this is how trees work. But look at what happens down the road. In particular look at lines 9 &#x2013; 10. You can see that we are now at a pretty stable state. We are writing ~160 pages per transaction. And since we write random data, we tend to touch about 100 leaf pages per transaction (the stuff after 100 is usually page splits). But something that is much more interesting can be seen in the count of seeks. &#xA;The way LMDB works, we use copy-on-write, so whenever we modify a page, we are actually modify a copy and mark the actual page as available for future transaction to re-use. This has the great advantage in that we don&#x2019;t ever need to do compaction, but it also means that when we do want to do writes, we have to make them pretty much all over place. And it actually gets worse as more times goes by.&#xA;Now, you have to realize that this is pretty much the worst case scenario, a transaction that does a lot of writes all over the place. But it means that toward the end, we are really hurting.&#xA;You already know the numbers, right? What about just straight out writing 1MB? What is the cost of seeks? I wrote the following code:&#xA;&#xA;&#xA;   1: var buffer = new byte[1024*1024];   2: var random = new Random();   3: random.NextBytes(buffer);   4: if(File.Exists(&quot;test.bin&quot;))   5:     File.Delete(&quot;test.bin&quot;);   6: using (var fs = new FileStream(&quot;test.bin&quot;, FileMode.CreateNew, FileAccess.ReadWrite))   7: {   8:     fs.SetLength(1024 * 1024 * 768);   9:     // warm up  10:     for (int i = 0; i &lt; 200; i&#x2B;&#x2B;)  11:     {  12:         fs.Position = random.Next(0, (int)fs.Length);  13:         fs.Write(buffer,0, random.Next(0, 1024));  14:     }  15:&#xA0;   16:     var sp = Stopwatch.StartNew();  17:     for (int i = 0; i &lt; 200; i&#x2B;&#x2B;)  18:     {  19:           fs.Position = random.Next(0, (int)fs.Length);  20:           fs.WriteByte(1);  21:     }  22:     fs.Flush(true);  23:     sp.Stop();  24:     Console.WriteLine(&quot;200 seeks &amp; 200 bytes {0:#,#} ms&quot;, sp.ElapsedMilliseconds);  25:&#xA0;   26:     sp = Stopwatch.StartNew();  27:     fs.Position = random.Next(0, (int)fs.Length);  28:     fs.Write(buffer, 0, buffer.Length);  29:     fs.Flush(true);  30:     sp.Stop();  31:     Console.WriteLine(&quot;1 MB write {0:#,#} ms&quot;, sp.ElapsedMilliseconds);  32: }&#xA;The results are quite interesting:&#xA;&#xA;&#xA;   1: 200 seeks &amp; 200 bytes 146 ms   2: 1 MB write 6 ms&#xA;Just to note, this is when running on SSD, the numbers are supposed to be a lot worse when running on HDD.&#xA;In other words, it ain&#x2019;t the size of the write, but how you spread it around that really matters. Just to compare, here are the numbers for when we are doing sequential writes:&#xA;&#xA;&#xA;   1: Flush      1 with   6 pages -  24 kb writes and   1 seeks (  5 leaves,   1 branches,   0 overflows)   2: Flush      2 with   6 pages -  24 kb writes and   1 seeks (  5 leaves,   1 branches,   0 overflows)   3: Flush      3 with   6 pages -  24 kb writes and   1 seeks (  5 leaves,   1 branches,   0 overflows)   4:&#xA0;    5: Flush    159 with   7 pages -  28 kb writes and   3 seeks (  5 leaves,   2 branches,   0 overflows)   6: Flush    160 with   8 pages -  32 kb writes and   3 seeks (  6 leaves,   2 branches,   0 overflows)   7: Flush    161 with   7 pages -  28 kb writes and   3 seeks (  5 leaves,   2 branches,   0 overflows)   8: Flush    162 with   7 pages -  28 kb writes and   3 seeks (  5 leaves,   2 branches,   0 overflows)   9:&#xA0;   10: Flush  1,320 with   8 pages -  32 kb writes and   3 seeks (  6 leaves,   2 branches,   0 overflows)  11: Flush  1,321 with   7 pages -  28 kb writes and   3 seeks (  5 leaves,   2 branches,   0 overflows)  12: Flush  1,322 with   7 pages -  28 kb writes and   3 seeks (  5 leaves,   2 branches,   0 overflows)  13:&#xA0;   14: Flush  4,316 with   7 pages -  28 kb writes and   3 seeks (  5 leaves,   2 branches,   0 overflows)  15: Flush  4,317 with   7 pages -  28 kb writes and   2 seeks (  5 leaves,   2 branches,   0 overflows)  16: Flush  4,318 with   8 pages -  32 kb writes and   3 seeks (  6 leaves,   2 branches,   0 overflows)  17:&#xA0;   18: Flush  7,409 with   8 pages -  32 kb writes and   4 seeks (  5 leaves,   3 branches,   0 overflows)  19: Flush  7,410 with   9 pages -  36 kb writes and   2 seeks (  6 leaves,   3 branches,   0 overflows)  20: Flush  7,411 with   8 pages -  32 kb writes and   4 seeks (  5 leaves,   3 branches,   0 overflows)  21:&#xA0;   22: Flush  9,990 with   8 pages -  32 kb writes and   3 seeks (  5 leaves,   3 branches,   0 overflows)  23: Flush  9,991 with   9 pages -  36 kb writes and   4 seeks (  6 leaves,   3 branches,   0 overflows)  24: Flush  9,992 with   8 pages -  32 kb writes and   3 seeks (  5 leaves,   3 branches,   0 overflows)  25: Flush  9,993 with   8 pages -  32 kb writes and   4 seeks (  5 leaves,   3 branches,   0 overflows)&#xA;Because we are always writing at the end, we only need to touch very few pages. Note that even with the small number of pages, we still need to do quite a bit of seeks, relative to the number of pages we write.&#xA;I have some ideas about this, but they are still unformed. Mostly we need to balance between the amount of free space that is used to the number of seeks allowed. I think we can get there by being smart about tracking the number of pages modified by a transaction, and waiting until free space become available in sufficient numbers to be relevant. Something like that would allow auto tuning of the amount of garbage we accumulate vs. the number of seeks we require.</p>
        </article>
        <article id="article-4080">
            <a href="https://ayende.com/blog/163713/ravendb-2-5-webinar-is-live-and-some-date-stuff" target="_blank">
                <h2 class="title mb-6" id="article-4080">RavenDB 2.5 Webinar is live (and some date stuff)</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: September 11, 2013
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">Here is the recording for our RavenDB 2.5 Webinar from Monday. You can start watching it right now, or you can continue reading about the timing snafu we had below.  Basically, we run into a timing error because of daylight savings. Unlike the common error, where we forgot to account for daylight savings taking effect, here we forgot to take into account daylight saving not coming into effect. For a lot of really reasons, daylight savings in Israel is a contentious issue, involving the cross roads of politics, religion and whole bunch of other stuff. As a result, until recently we didn&#x2019;t have a fixed date for daylight saving changes. A few years ago it changed, and that date was supposed to be last week. Then politics happened, and the date moved. We didn&#x2019;t account for a lot of software still thinking that the daylight savings time actually happening on time, and that meant that we actually had the webinar an hour early. I apologize for the mistake, and hopefully you&#x2019;ll still enjoy the recording.</p>
        </article>
        <div class="button flex justify-between">
            <a href="407.html"><span class="back arrow"></span></a>

            <a href="409.html"><span class="next arrow"></span></a>
        </div>
    </section>
</main>
<footer
    class="mt-auto flex w-full flex-col items-center justify-center gap-y-2 pb-4 pt-20 text-center align-top font-semibold text-gray-600 dark:text-gray-400 sm:flex-row sm:justify-between sm:text-xs">
    <div class="me-0 sm:me-4">
        <div class="flex flex-wrap items-end gap-x-2">
            <ul class="flex flex-1 items-center gap-x-2 sm:flex-initial">
                <li class="flex">
                    <p class="flex items-end gap-2 justify-center flex-wrap	">Â© Relatively General
                        .NET 2025<span
                            class="inline-block">&nbsp;ðŸš€&nbsp;Theme: Astro Cactus</span>

                        <a class="inline-block sm:hover:text-link" href="https://github.com/chrismwilliams/astro-cactus"
                           rel="noopener noreferrer " target="_blank">
                            <svg width="1em" height="1em" viewBox="0 0 24 24" aria-hidden="true" class="h-6 w-6"
                                 focusable="false" data-icon="mdi:github">
                                <symbol id="ai:mdi:github">
                                    <path fill="currentColor"
                                          d="M12 2A10 10 0 0 0 2 12c0 4.42 2.87 8.17 6.84 9.5c.5.08.66-.23.66-.5v-1.69c-2.77.6-3.36-1.34-3.36-1.34c-.46-1.16-1.11-1.47-1.11-1.47c-.91-.62.07-.6.07-.6c1 .07 1.53 1.03 1.53 1.03c.87 1.52 2.34 1.07 2.91.83c.09-.65.35-1.09.63-1.34c-2.22-.25-4.55-1.11-4.55-4.92c0-1.11.38-2 1.03-2.71c-.1-.25-.45-1.29.1-2.64c0 0 .84-.27 2.75 1.02c.79-.22 1.65-.33 2.5-.33s1.71.11 2.5.33c1.91-1.29 2.75-1.02 2.75-1.02c.55 1.35.2 2.39.1 2.64c.65.71 1.03 1.6 1.03 2.71c0 3.82-2.34 4.66-4.57 4.91c.36.31.69.92.69 1.85V21c0 .27.16.59.67.5C19.14 20.16 22 16.42 22 12A10 10 0 0 0 12 2"></path>
                                </symbol>
                                <use xlink:href="#ai:mdi:github"></use>
                            </svg>
                            <span class="sr-only">Github</span>
                        </a>
                    </p>
                </li>
            </ul>
        </div>
    </div>
    <nav aria-label="More on this site" class="flex gap-x-2 sm:gap-x-0 sm:divide-x sm:divide-gray-500">
        <a class="px-4 py-2 sm:py-0 sm:hover:text-textColor sm:hover:underline" href="index.html"> Home </a><a
            class="px-4 py-2 sm:py-0 sm:hover:text-textColor sm:hover:underline" href="/about/"> About </a>
    </nav>
</footer>
<script src="js/script.js?id=af8f4559935e7bf5bf6015373793411d"></script>
<script src="pagefind/pagefind-ui.js"></script>

<!-- Cookie Consent Banner -->
<div class="cookie-consent" id="cookieConsent">
    <div>
        <p class="text-sm">We use cookies to analyze our website traffic and provide a better browsing experience. By
            continuing to use our site, you agree to our use of cookies.</p>
    </div>
    <div class="cookie-consent-buttons">
        <button class="cookie-consent-decline" onclick="declineCookies()">Decline</button>
        <button class="cookie-consent-accept" onclick="acceptCookies()">Accept</button>
    </div>
</div>

<script>
    // Cookie consent management
    function showCookieConsent() {
        const consent = localStorage.getItem('cookieConsent');
        if (!consent) {
            document.getElementById('cookieConsent').classList.add('show');
        }
    }

    function acceptCookies() {
        localStorage.setItem('cookieConsent', 'accepted');
        document.getElementById('cookieConsent').classList.remove('show');
        loadGA(); // Load Google Analytics after consent
    }

    function declineCookies() {
        localStorage.setItem('cookieConsent', 'declined');
        document.getElementById('cookieConsent').classList.remove('show');
    }

    // Show the consent banner only for EU visitors (you can add more country codes as needed)
    fetch('https://ipapi.co/json/')
            .then(response => response.json())
            .then(data => {
                const euCountries = ['AT', 'BE', 'BG', 'HR', 'CY', 'CZ', 'DK', 'EE', 'FI', 'FR', 'DE', 'GR', 'HU', 'IE', 'IT', 'LV', 'LT', 'LU', 'MT', 'NL', 'PL', 'PT', 'RO', 'SK', 'SI', 'ES', 'SE'];
                if (euCountries.includes(data.country_code)) {
                    showCookieConsent();
                } else {
                    // For non-EU visitors, automatically load GA
                    if (!localStorage.getItem('cookieConsent')) {
                        localStorage.setItem('cookieConsent', 'accepted');
                        loadGA();
                    }
                }
            })
            .catch(() => {
                // If we can't determine location, show the consent banner to be safe
                showCookieConsent();
            });
</script>
</body>
</html>