
<!DOCTYPE html>
<html class="scroll-smooth" lang="en-US" data-theme="light">
<head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, shrink-to-fit=no" name="viewport">
    <meta content="IE=edge" http-equiv="X-UA-Compatible">
    <title>Page 16 &#x2022; Relatively General .NET</title>
    <link href="favicon.ico" rel="icon" sizes="any">
    <link href="images/apple-touch-icon.png" rel="apple-touch-icon">
    <meta content="hsl()" name="theme-color">
    <meta content="website" property="og:type">
    <meta content="Home" property="og:title">
    <link rel="stylesheet" href="css/styles.css">
    <link rel="stylesheet" href="/pagefind/pagefind-ui.css">
    <!-- Google Analytics -->
    <script>
        // Only load GA if consent is given
        function loadGA() {
            const script = document.createElement('script');
            script.src = 'https://www.googletagmanager.com/gtag/js?id=G-MDFXJY3FCY';
            script.async = true;
            document.head.appendChild(script);

            window.dataLayer = window.dataLayer || [];

            function gtag() {
                dataLayer.push(arguments);
            }

            gtag('js', new Date());
            gtag('config', 'G-MDFXJY3FCY');
        }

        // Check if consent was previously given
        if (localStorage.getItem('cookieConsent') === 'accepted') {
            loadGA();
        }
    </script>
    <!-- End Google Analytics -->
</head>
<body class="mx-auto flex min-h-screen max-w-3xl flex-col bg-bgColor px-4 pt-16 font-mono text-sm font-normal text-textColor antialiased sm:px-8">
<a class="sr-only focus:not-sr-only focus:fixed focus:start-1 focus:top-1.5" href="#main">
    skip to content
</a>
<header class="group relative mb-28 flex items-center sm:ps-[4.5rem]" id="main-header">
    <div class="flex sm:flex-col">
        <a aria-current="page" class="inline-flex items-center hover:filter-none sm:relative sm:inline-block"
           href="index.html">
            <img class="me-3 sm:absolute sm:start-[-4.5rem] sm:me-0 sm:h-16 sm:w-16 w-16" src="images/giphy.gif"
                 alt=""/>
            <span class="text-xl font-bold sm:text-2xl">Relatively General .NET</span>
        </a>
        <nav aria-label="Main menu"
             class="absolute -inset-x-4 top-14 hidden flex-col items-end gap-y-4 rounded-md bg-bgColor/[.85] py-4 text-accent shadow backdrop-blur group-[.menu-open]:z-50 group-[.menu-open]:flex sm:static sm:z-auto sm:-ms-4 sm:mt-1 sm:flex sm:flex-row sm:items-center sm:divide-x sm:divide-dashed sm:divide-accent sm:rounded-none sm:bg-transparent sm:py-0 sm:shadow-none sm:backdrop-blur-none"
             id="navigation-menu">
            <a aria-current="page" class="px-4 py-4 underline-offset-2 sm:py-0 sm:hover:underline underline"
               href="index.html"> Home </a><a
                aria-current="" class="px-4 py-4 underline-offset-2 sm:py-0 sm:hover:underline " href="about.html">
                About </a>
        </nav>
    </div>
    <site-search class="ms-auto" id="search">
        <button id="open-search"
                class="flex h-9 w-9 items-center justify-center rounded-md ring-zinc-400 transition-all hover:ring-2"
                data-open-modal="">
            <svg aria-label="search" class="h-7 w-7" fill="none" height="16" stroke="currentColor"
                 stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 24 24" width="16"
                 xmlns="http://www.w3.org/2000/svg">
                <path d="M0 0h24v24H0z" stroke="none"></path>
                <path d="M3 10a7 7 0 1 0 14 0 7 7 0 1 0-14 0M21 21l-6-6"></path>
            </svg>
        </button>
        <dialog aria-label="search"
                class="h-full max-h-full w-full max-w-full border border-zinc-400 bg-bgColor shadow backdrop:backdrop-blur sm:mx-auto sm:mb-auto sm:mt-16 sm:h-max sm:max-h-[calc(100%-8rem)] sm:min-h-[15rem] sm:w-5/6 sm:max-w-[48rem] sm:rounded-md">
            <div class="dialog-frame flex flex-col gap-4 p-6 pt-12 sm:pt-6">
                <button id="close-search"
                        class="ms-auto cursor-pointer rounded-md bg-zinc-200 p-2 font-semibold dark:bg-zinc-700"
                        data-close-modal="">Close
                </button>
                <div class="search-container">
                    <div id="cactus__search"/>
                </div>
            </div>
        </dialog>
    </site-search>
    <theme-toggle class="ms-2 sm:ms-4">
        <button id="theme-toggle" class="relative h-9 w-9 rounded-md p-2 ring-zinc-400 transition-all hover:ring-2"
                type="button">
            <span class="sr-only">Dark Theme</span>
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-7 w-7 -translate-x-1/2 -translate-y-1/2 scale-100 opacity-100 transition-all dark:scale-0 dark:opacity-0"
                 fill="none" focusable="false" id="sun-svg" stroke-width="1.5" viewBox="0 0 24 24"
                 xmlns="http://www.w3.org/2000/svg">
                <path
                    d="M12 18C15.3137 18 18 15.3137 18 12C18 8.68629 15.3137 6 12 6C8.68629 6 6 8.68629 6 12C6 15.3137 8.68629 18 12 18Z"
                    stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M22 12L23 12" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M12 2V1" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M12 23V22" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M20 20L19 19" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M20 4L19 5" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M4 20L5 19" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M4 4L5 5" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M1 12L2 12" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
            </svg>
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-7 w-7 -translate-x-1/2 -translate-y-1/2 scale-0 opacity-0 transition-all dark:scale-100 dark:opacity-100"
                 fill="none" focusable="false" id="moon-svg" stroke="currentColor" stroke-width="1.5"
                 viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M0 0h24v24H0z" fill="none" stroke="none"></path>
                <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z"></path>
                <path d="M17 4a2 2 0 0 0 2 2a2 2 0 0 0 -2 2a2 2 0 0 0 -2 -2a2 2 0 0 0 2 -2"></path>
                <path d="M19 11h2m-1 -1v2"></path>
            </svg>
        </button>
    </theme-toggle>
    <mobile-button>
        <button aria-expanded="false" aria-haspopup="menu" aria-label="Open main menu"
                class="group relative ms-4 h-7 w-7 sm:invisible sm:hidden" id="toggle-navigation-menu" type="button">
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-full w-full -translate-x-1/2 -translate-y-1/2 transition-all group-aria-expanded:scale-0 group-aria-expanded:opacity-0"
                 fill="none" focusable="false" id="line-svg" stroke="currentColor" stroke-width="1.5"
                 viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M3.75 9h16.5m-16.5 6.75h16.5" stroke-linecap="round" stroke-linejoin="round"></path>
            </svg>
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-full w-full -translate-x-1/2 -translate-y-1/2 scale-0 text-accent opacity-0 transition-all group-aria-expanded:scale-100 group-aria-expanded:opacity-100"
                 fill="none" focusable="false" id="cross-svg" stroke="currentColor" stroke-width="1.5"
                 viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M6 18L18 6M6 6l12 12" stroke-linecap="round" stroke-linejoin="round"></path>
            </svg>
        </button>
    </mobile-button>
</header>


<main id="main" data-pagefind-body>
    <section aria-label="Blog post list">
        <article id="article-151">
            <a href="https://ayende.com/blog/202468-C/scaling-hnsw-in-ravendb-optimizing-for-inadequate-hardware" target="_blank">
                <h2 class="title mb-6" id="article-151">Scaling HNSW in RavenDB: Optimizing for inadequate hardware</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: May 14, 2025
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">RavenDB 7.0 introduces vector search using the Hierarchical Navigable Small World (HNSW) algorithm, a graph-based approach for Approximate Nearest Neighbor search. HNSW enables efficient querying of large vector datasets but requires random access to the entire graph, making it memory-intensive.HNSW&#x27;s random read and insert operations demand in-memory processing. For example, inserting 100 new items into a graph of 1 million vectors scatters them randomly, with no efficient way to sort or optimize access patterns. That is in dramatic contrast to the usual algorithms we can employ (B&#x2B;Tree, for example), which can greatly benefit from such optimizations.Let&#x2019;s assume that we deal with vectors of 768 dimensions, or 3KB each. If we have 30 million vectors, just the vector&#xA0;storage is going to take 90GB of memory. Inserts into the graph require us to do effective random reads (with no good way to tell what those would be in advance). Without sufficient memory, performance degrades significantly due to disk swapping.The rule of thumb for HNSW is that you want at least 30% more memory than the vectors would take. In the case of 90GB of vectors, the minimum required memory is going to be 128GB. Cost reductionYou can also use quantization (shrinking the size of the vector with some loss of accuracy). Going to binary quantization for the same dataset requires just 3GB of space to store the vectors. But there is a loss of accuracy (may be around 20% loss). We tested RavenDB&#x2019;s HNSW implementation on a 32 GB machine with a 120 GB vector index (and no quantization), simulating a scenario with four times more data than available memory.This is an utterly invalid&#xA0;scenario, mind you. For that amount of data, you are expected to build the HNSW index on a machine with 192GB. But we wanted to see how far we could stress the system and what kind of results we would get here. The initial implementation stalled due to excessive memory use and disk swapping, rendering it impractical. Basically, it ended up doing a page fault on each and every operation, and that stalled forward progress completely. Optimization ApproachOptimizing for such an extreme scenario seemed futile, this is an invalid scenario almost by definition. But the idea is that improving this out-of-line scenario will also end up improving the performance for more reasonable setups.When we analyzed the costs of HNSW in a severely memory-constrained environment, we found that there are two primary costs.Distance computations: Comparing vectors (e.g., cosine similarity) is computationally expensive.Random vector access: Loading vectors from disk in a random pattern is slow when memory is insufficient.Distance computation is doing math on two 3KB vectors, and on a large graph (tens of millions), you&#x2019;ll typically need to run between 500 - 1,500 distance comparisons. To give some context, adding an item to a B&#x2B;Tree of the same size will have fewer than twenty comparisons (and highly localized ones, at that).That means reading about 2MB of data per insert&#xA0;on average. Even if everything is in memory, you are going to be paying a significant cost here in CPU cycles. If the data does not reside in memory, you have to fetch it (and it isn&#x2019;t as neat as having a single 2MB range to read, it is scattered all over the place, and you need to traverse the graph in order to find what you need to read). To address this issue, we completely restructured how we go about inserting nodes into the graph. We avoid serial execution and instead spawn multiple insert processes at the same time. Interestingly enough, we are single-threaded in this regard. We extract from the process the parts where it does the distance computation and loads the vectors.Each process will run the algorithm until it reaches the stage where it needs to run distance computation on some vectors. In that case, it will yield to another&#xA0;process and let it run. We keep doing this until we have no more runnable processes to execute. We can then scan the list of nodes that we need to process (run distance computation on all their edges), and we can then:Gather all vectors needed for graph traversal.Preload them from disk efficiently.Run distance computations across multiple threads simultaneously.The idea here is that we save time by preloading data efficiently. Once the data is loaded, we throw all the distance computations per node to the thread pool. As soon as any of those distance computations are done, we resume the stalled process for it. The idea is that at any given point in time, we have processes moving forward or we are preloading from the disk, while at the same time we have background threads running to compute distances.This allows us to make maximum use of the hardware. Here is what this looks like midway through the process:As you can see, we still have to go to the disk a lot (no way around that with a working set of 120GB on a 32GB machine), but we are able to make use of that efficiently. We always have forward progress instead of just waiting. Don&#x2019;ttry this on the cloud - most cloud instances have a limited amount of IOPS to use, and this approach will burn through any amount you have quickly. We are talking about roughly 15K - 20K IOPS on a sustained basis. This is meant for testing in adverse&#xA0;conditions, on hardware that is utterly unsuitable for the task at hand. A machine with the proper amount of memory will not have to deal with this issue. While still slow on a 32 GB machine due to disk reliance, this approach completed indexing 120 GB of vectors in about 38 hours (average rate of 255 vectors/sec). To compare, when we ran the same thing on a 64GB machine, we were able to complete the indexing process in just over 14 hours (average rate of 694 vectors/sec). Accuracy of the resultsWhen inserting many nodes in parallel to the graph, we risk a loss of accuracy. When we insert them one at a time, nodes that are close together will naturally be linked to one another. But when running them in parallel, we may &#x201C;miss&#x201D; those relations because the two nodes aren&#x2019;t yet discoverable.To mitigate that scenario, we preemptively link all the in-flight nodes to each other, then run the rest of the algorithm. If they are not close to one another, these edges will be pruned. It turns out that this behavior actually increased&#xA0;the overall accuracy (by about 1%) compared to the previous behavior. This is likely because items that are added at the same time are naturally linked to one another.ResultsOn smaller datasets (&#x201C;merely&#x201D; hundreds of thousands of vectors) that fit in memory, the optimizations reduced indexing time by 44%! That is mostly because we now operate in parallel and have more optimized I/O behavior.We are quite a bit faster, we are (slightly) more accurate, and we also have reasonable behavior when we exceed the system limits. &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;What about binary quantization?I mentioned that binary quantization can massively reduce the amount of space required for vector search. We also ran tests with the same dataset using binary quantization. The total size of all the vectors was around 3GB, and the total index size (including all the graph nodes, etc.) was around 18 GB. That took under 5 hours to index on the same 32GB machine. If you care to know what it is that we are doing, take a look at the Hnsw.Parallel&#xA0;file inside the repository. The implementation is quite involved, but I&#x2019;m really proud of how it turned out in the end.</p>
        </article>
        <article id="article-152">
            <a href="https://devblogs.microsoft.com/dotnet/dotnet-and-dotnet-framework-may-2025-servicing-updates/" target="_blank">
                <h2 class="title mb-6" id="article-152">.NET and .NET Framework May 2025 servicing releases updates</h2>
            </a>
            <p class="mb-2">by Tara Overfield</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: May 13, 2025
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">A recap of the latest servicing updates for .NET and .NET Framework for May 2025.</p>
        </article>
        <article id="article-153">
            <a href="https://devblogs.microsoft.com/dotnet/dotnet-10-preview-4/" target="_blank">
                <h2 class="title mb-6" id="article-153">.NET 10 Preview 4 is now available!</h2>
            </a>
            <p class="mb-2">by .NET Team</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: May 13, 2025
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">Find out about the new features in .NET 10 Preview 4 across the .NET runtime, SDK, libraries, ASP.NET Core, Blazor, C#, .NET MAUI, and more!</p>
        </article>
        <article id="article-154">
            <a href="https://andrewlock.net/using-the-new-ai-template-to-create-a-chatbot-about-a-website/" target="_blank">
                <h2 class="title mb-6" id="article-154">Using the new AI template to create a chatbot about a website</h2>
            </a>
            <p class="mb-2">by Andrew Lock</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: May 13, 2025
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">In this post I use the new Microsoft&#x27;s new .NET AI template to ingest the contents of a website and create a chatbot that can answer questions with citations&#x2026;</p>
        </article>
        <article id="article-155">
            <a href="https://ayende.com/blog/202469-C/optimizing-the-cost-of-clearing-a-set" target="_blank">
                <h2 class="title mb-6" id="article-155">Optimizing the cost of clearing a set</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: May 12, 2025
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">Let&#x2019;s say I want to count the number of reachable nodes for each node in the graph. I can do that using the following code:void DFS(Node start, HashSet&lt;Node&gt; visited) &#xD;&#xA;{&#xD;&#xA;    if (start == null || visited.Contains(start)) return;&#xD;&#xA;    visited.Add(start);&#xD;&#xA;    foreach (var neighbor in start.Neighbors) &#xD;&#xA;    {&#xD;&#xA;        DFS(neighbor, visited);&#xD;&#xA;    }&#xD;&#xA;}&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;void MarkReachableCount(Graph g)&#xD;&#xA;{&#xD;&#xA;   foreach(var node in g.Nodes)&#xD;&#xA;   {&#xD;&#xA;       HashSet&lt;Node&gt; visited = [];&#xD;&#xA;       DFS(node, visisted);&#xD;&#xA;       node.ReachableGraph = visited.Count;&#xD;&#xA;   }&#xD;&#xA;}A major performance cost for this sort of operation is the allocation cost. We allocate a separate hash set for each node in the graph, and then allocate whatever backing store is needed for it. If you have a big graph with many connections, that is expensive. A simple fix for that would be to use:void MarkReachableCount(Graph g)&#xD;&#xA;{&#xD;&#xA;   HashSet&lt;Node&gt; visited = [];&#xD;&#xA;   foreach(var node in g.Nodes)&#xD;&#xA;   {&#xD;&#xA;       visited.Clear();&#xD;&#xA;       DFS(node, visisted);&#xD;&#xA;       node.ReachableGraph = visited.Count;&#xD;&#xA;   }&#xD;&#xA;}This means that we have almost no allocations for the entire operation, yay!This function also performs significantly&#xA0;worse than the previous one, even though it barely allocates. The reason for that? The call to Clear()&#xA0;is expensive. Take a look at the implementation&#xA0;-&#xA0;this method needs to zero out two arrays, and it will end up being as large as the node with the most reachable nodes. Let&#x2019;s say we have a node that can access 10,000 nodes. That means that for each node,&#xA0;we&#x2019;ll have to clear an array of about 14,000 items, as well as another array that is as big as the number of nodes we just visited. No surprise that the allocating version was actually cheaper. We use the visited&#xA0;set for a short while, then discard it and get a new one. That means no expensive Clear()&#xA0;calls. The question is, can we do better? Before I answer that, let&#x2019;s try to go a bit deeper in this analysis. Some of the main costs in HashSet&lt;Node&gt;&#xA0;are the calls to GetHashCode()&#xA0;and Equals(). For that matter, let&#x2019;s look at the cost of the Neighbors&#xA0;array on the Node.Take a look at the following options:public record Node1(List&lt;Node&gt; Neighbors);&#xD;&#xA;public record Node2(List&lt;int&gt; NeighborIndexes);Let&#x2019;s assume each node has about 10 - 20 neighbors. What is the cost in memory for each option? Node1&#xA0;uses references (pointers), and will take 256 bytes just for the Neighbors&#xA0;backing array (32-capacity array x 8 bytes). However, the Node2 version uses half of that memory.This is an example of data-oriented design, and saving 50% of our memory costs is quite&#xA0;nice. HashSet&lt;int&gt;&#xA0;is also going to benefit quite nicely from JIT optimizations (no need to call GetHashCode(), etc. - everything is inlined).We still have the problem of allocations vs. Clear(),&#xA0;though. Can we win?Now that we have re-framed the problem using int indexes, there is a very obvious optimization opportunity: use a bit map (such as BitsArray). We know upfront how many items we have, right? So we can allocate a single array and set the corresponding bit to mark that a node (by its index) is visited.That dramatically reduces the costs of tracking whether we visited a node or not, but it does not&#xA0;address the costs of clearing the bitmap.Here is how you can handle this scenario cheaply:public class Bitmap&#xD;&#xA;{&#xD;&#xA;    private ulong[] _data;&#xD;&#xA;    private ushort[] _versions;&#xD;&#xA;    private int _version;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    public Bitmap(int size)&#xD;&#xA;    {&#xD;&#xA;        _data = new ulong[(size &#x2B; 63) / 64];&#xD;&#xA;        _versions = new ushort[_data.Length];&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    public void Clear()&#xD;&#xA;    {&#xD;&#xA;        if(_version&#x2B;&#x2B; &lt; ushort.MaxValue)&#xD;&#xA;            return;&#xD;&#xA;            &#xD;&#xA;        Array.Clear(_data);&#xD;&#xA;        Array.Clear(_versions);&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    public bool Add(int index)&#xD;&#xA;    {&#xD;&#xA;        int arrayIndex = index &gt;&gt; 6;&#xD;&#xA;        if(_versions[arrayIndex] != _version)&#xD;&#xA;        {&#xD;&#xA;            _versions[arrayIndex] = _version;&#xD;&#xA;            _data[arrayIndex] = 0;&#xD;&#xA;        }&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;        int bitIndex = index &amp; 63;&#xD;&#xA;        ulong mask = 1UL &lt;&lt; bitIndex;&#xD;&#xA;        ulong old = _data[arrayIndex];&#xD;&#xA;        _data[arrayIndex] |= mask;&#xD;&#xA;        return (old &amp; mask) == 0;&#xD;&#xA;    }&#xD;&#xA;}The idea is pretty simple, in addition to the bitmap - we also have another array that marks the version of each 64-bit range. To clear the array, we increment the version. That would mean that when adding to the bitmap, we reset the underlying array element if it doesn&#x2019;t match the current version. Once every 64K items, we&#x2019;ll need to pay the cost of actually resetting the backing stores, but that ends up being very cheap overall (and worth the space savings to handle the overflow).The code is tight, requires no allocations, and performs very&#xA0;quickly.</p>
        </article>
        <article id="article-156">
            <a href="https://www.meziantou.net/use-csharp-14-extensions-to-simplify-enum-parsing.htm" target="_blank">
                <h2 class="title mb-6" id="article-156">Use C# 14 extensions to simplify enum Parsing</h2>
            </a>
            <p class="mb-2">by G&#xE9;rald Barr&#xE9;</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: May 12, 2025
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">In .NET, many types provide a static Parse method to convert strings into their respective types. For example:C#copyint.Parse(&quot;123&quot;);&#xA;double.Parse(&quot;123.45&quot;);&#xA;DateTime.Parse(&quot;2023-01-01&quot;);&#xA;IPAddress.Parse(&quot;192.168.0.1&quot;);However, enums require the use of the Enum.Parse method:C#copyEnum.Parse&lt;MyEn</p>
        </article>
        <article id="article-157">
            <a href="https://ayende.com/blog/202531-B/not-sharding-ravendb-vector-search" target="_blank">
                <h2 class="title mb-6" id="article-157">NOT Sharding RavenDB Vector Search</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: May 09, 2025
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">I ran into an interesting post, &quot;Sharding Pgvector,&quot; in which PgDog&#xA0;(provider of scaling solutions for Postgres) discusses scaling pgvector indexes (HNSW and IVFFlat) across multiple machines to manage large-scale embeddings efficiently. This approach speeds up searches and improves recall by distributing vector data, addressing the limitations of fitting large indexes into memory on a single machine.That was interesting to me because they specifically&#xA0;mention this Wikipedia dataset, consisting of 35.1 million vectors. That&#x2026; is not really enough to justify&#xA0;sharding, in my eyes. The dataset is about 120GB of Parquet files, so I threw that into RavenDB using the following format:Each vector has 768 dimensions in this dataset. 33 minutes later, I had the full dataset in RavenDB, taking 163 GB of storage space. The next step was to define a vector search index, like so:from a in docs.Articles&#xD;&#xA;select new &#xD;&#xA;{&#xD;&#xA;    Vector = CreateVector(a.Embedding)&#xD;&#xA;}That index (using the HNSW algorithm) is all that is required to start doing proper vector searches in RavenDB.Here is what this looks like - we have 163GB for the raw data, and the index itself is 119 GB. RavenDB (and PgVector) actually need to store the vectors twice - once in the data itself and once in the index. Given the size of the dataset, I used a machine with 192 GB&#xA0;of RAM to create the index. Note that this still means the total data size is about &#x2153; bigger than the available memory, meaning we cannot compute it all in memory. This deserves a proper explanation &#xA0;- HNSW is a graph algorithm that assumes you can cheaply access any part of the graph during the indexing process. Indeed, this is effectively doing pure random reads on the entire dataset. You would generally run this on a machine with at least&#xA0;192 GB of RAM. &#xA0;I assume this is why pgvector&#xA0;required sharding for this dataset.I decided to test it out on several different machines. The key aspect here is the size of memory, I&#x2019;m ignoring CPU counts and type, they aren&#x2019;t the bottleneck for this scenario. As a reminder, we are talking about a total data size that is close to 300 GB.RAMRavenDB indexing time:192 GB2 hours, 20 minutes64 GB14 hours, 8 minutes32 GB37 hours, 40 minutesNote that all of those were run on a single machine, all using local NVMe disk. And yes, that is less than two days to index that&#xA0;much data on a machine that is grossly inadequate for it.I should note that on the smaller machines, query times are typically ~5ms, so even with a lot of data indexed, the actual search doesn&#x2019;t need to run on a machine with a lot of memory. In short, I don&#x2019;t see a reason why you would need to use sharding for that amount of data. It can comfortably fit inside a reasonably sized machine, with room to spare. I should also note that the original post talks about using the IVFFlat algorithm instead of HNSW. Pgvector supports both, but RavenDB only uses HNSW. From a technical perspective, I would love&#xA0;to be able to use IVFFlat, since it is a much more traditional algorithm for databases. You run k-means over your data to find the centroids (so you can split the data into reasonably sized chunks), and then just do an efficient linear search on that small chunk as needed. It fits much more nicely into the way databases typically work. However, it also has some significant drawbacks:You have to have the data upfront, you cannot build it incrementally.The effectiveness of the IVFFlat index degrades&#xA0;over time with inserts &amp; deletes, because the original centroids are no longer as accurate. Because of those reasons, we didn&#x2019;t implement that. HNSW is a far more complex algorithm, both in terms of the actual approach and the number of hoops we had to go through to implement that efficiently, but as you can see, it is able to provide good results even on large datasets, can be built incrementally and doesn&#x2019;t degrade over time.Head-to-head comparisonI decided to run pgvector&#xA0;and RavenDB on the same dataset to get some concrete ideas about their performance. Because I didn&#x2019;t feel like waiting for hours, I decided to use this dataset. It has 485,859 vectors and about 1.6 GB of data.RavenDB indexed that in 1 minute and 17 seconds. My first attempt with pgvector took over 7 minutes when setting&#xA0;maintenance_work_mem = &#x27;512MB&#x27;.&#xA0;I had to increase it to 2GB to get more reasonable results (and then it was 1 minute and 49 seconds).RavenDB is able to handle it a lot&#xA0;better when there isn&#x2019;t enough memory to keep it all in RAM, while pgvector seems to degrade badly. SummaryIn short, I don&#x2019;t think that you should need to go for sharding (and its associated complexity) for that amount of data. And I say that as someone whose database has&#xA0;native sharding capabilities. For best performance, you should run large vector indexes on machines with plenty of RAM, but even without that, RavenDB does an okay job of keeping things ticking.</p>
        </article>
        <article id="article-158">
            <a href="https://ayende.com/blog/202467-C/making-the-costs-visible-then-fixing-them" target="_blank">
                <h2 class="title mb-6" id="article-158">Making the costs visible, then fixing them</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: May 08, 2025
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">We recently tackled performance improvements for vector search in RavenDB. The core challenge was identifying performance bottlenecks. Details of specific changes are covered in a separate post. The post is already written, but will be published next week, here is the direct link to that.In this post, I don&#x2019;t want to talk about the actual changes we made, but the approach we took to figure out where the cost is. Take a look at the following flame graph, showing where our code is spending the most time. As you can see, almost the entire&#xA0;time is spent computing cosine similarity. That would be the best target for optimization, right?I spent a bunch of time writing increasingly complicated ways to optimize the cosine similarity function. And it worked, I was able to reduce the cost by about 1.5%!That is something that we would generally celebrate, but it was far from where we wanted to go. The problem was elsewhere, but we couldn&#x2019;t see it in the profiler output because the cost was spread around too much. Our first task was to restructure the code so we could actually see&#xA0;where the costs were. For instance, loading the vectors from disk was embedded within the algorithm. By extracting and isolating this process, we could accurately profile and measure its performance impact. This restructuring also eliminated the &quot;death by a thousand cuts&quot; issue, making hotspots evident in profiling results. With clear targets identified, we can now focus optimization efforts effectively.That major refactoring had two primary goals. The first was to actually extract the costs into highly visible locations, the second had to do with how you address them. Here is a small example that scans a social graph for friends, assuming the data is in a file.def read_user_friends(file, user_id: int) -&gt; List[int]:&#xD;&#xA;    &quot;&quot;&quot;Read friends for a single user ID starting at indexed offset.&quot;&quot;&quot;&#xD;&#xA;    pass # redacted&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;def social_graph(user_id: int, max_depth: int) -&gt; Set[int]:&#xD;&#xA;    if max_depth &lt; 1:&#xD;&#xA;        return set()&#xD;&#xA;    &#xD;&#xA;    all_friends = set() &#xD;&#xA;    visited = {user_id} &#xD;&#xA;    work_list = deque([(user_id, max_depth)])  &#xD;&#xA;    &#xD;&#xA;    with open(&quot;relations.dat&quot;, &quot;rb&quot;) as file:&#xD;&#xA;        while work_list:&#xD;&#xA;            curr_id, depth = work_list.popleft()&#xD;&#xA;            if depth &lt;= 0:&#xD;&#xA;                continue&#xD;&#xA;                &#xD;&#xA;            for friend_id in read_user_friends(file, curr_id):&#xD;&#xA;                if friend_id not in visited:&#xD;&#xA;                    all_friends.add(friend_id)&#xD;&#xA;                    visited.add(friend_id)&#xD;&#xA;                    work_list.append((friend_id, depth - 1))&#xD;&#xA;    &#xD;&#xA;    return all_friendsIf you consider this code, you can likely see that there is an expensive part of it, reading from the file. But the way the code is structured, there isn&#x2019;t really much that you can do about it. Let&#x2019;s refactor the code a bit to expose the actual costs. def social_graph(user_id: int, max_depth: int) -&gt; Set[int]:&#xD;&#xA;    if max_depth &lt; 1:&#xD;&#xA;        return set()&#xD;&#xA;    &#xD;&#xA;    all_friends = set() &#xD;&#xA;    visited = {user_id} &#xD;&#xA;    &#xD;&#xA;    with open(&quot;relations.dat&quot;, &quot;rb&quot;) as file:&#xD;&#xA;        work_list =  read_user_friends(file, [user_id])&#xD;&#xA;        while work_list and max_depth &gt;= 0:&#xD;&#xA;            to_scan = set()&#xD;&#xA;            for friend_id in work_list: # gather all the items to read&#xD;&#xA;                if friend_id in visited:&#xD;&#xA;                    continue&#xD;&#xA;                &#xD;&#xA;                all_friends.add(friend_id)&#xD;&#xA;                visited.add(friend_id)&#xD;&#xA;                to_scan.add(curr_id)&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;            # read them all in one shot&#xD;&#xA;            work_list = read_users_friends(file, to_scan)&#xD;&#xA;            # reduce for next call&#xD;&#xA;            max_depth = max_depth - 1    &#xD;&#xA;    &#xD;&#xA;    return all_friendsNow, instead of scattering the reads whenever we process an item, we gather them all and then send a list of items to read all at once. The costs are far clearer in this model, and more importantly, we have a chance to actually do&#xA0;something about it.Optimizing a lot of calls to read_user_friends(file, user_id)&#xA0;is really hard, but optimizing read_users_friends(file, users)&#xA0;is a far simpler task. Note that the actual costs didn&#x2019;t change because of this refactoring, but the ability to actually make the change is now far easier. Going back to the flame graph above, the actual cost profile differs dramatically as the size of the data rose, even if the profiler output remained the same. Refactoring the code allowed us to see where the costs were and address them effectively.Here is the end result as a flame graph. You can clearly see the preload section that takes a significant portion of the time. The key here is that the change allowed us to address this cost directly and in an optimal manner.The end result for our benchmark was:Before: 3 minutes, 6 secondsAfter: 2 minutes, 4 secondsSo almost exactly &#x2153; of the cost was removed because of the different approach we took, which is quite&#xA0;nice. This technique, refactoring the code to make the costs obvious, is a really powerful one. Mostly because it is likely the first step to take anyway&#xA0;in many performance optimizations (batching, concurrency, etc.).</p>
        </article>
        <article id="article-159">
            <a href="https://devblogs.microsoft.com/dotnet/csharp-exploring-extension-members/" target="_blank">
                <h2 class="title mb-6" id="article-159">C# 14 &#x2013; Exploring extension members</h2>
            </a>
            <p class="mb-2">by Kathleen Dollard</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: May 08, 2025
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">C# 14 introduces extension members. See how the `extension` syntax offers flexibility for extension authors and continuity for developers using extensions</p>
        </article>
        <article id="article-160">
            <a href="https://ayende.com/blog/202499-A/event-exploring-the-power-of-ai-search-in-modern-applications-today" target="_blank">
                <h2 class="title mb-6" id="article-160">Event: Exploring the Power of AI Search in Modern Applications (TODAY)</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: May 07, 2025
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">Join Our Community Discussion: Exploring the Power of AI Search in Modern ApplicationsWe&#x27;re excited to announce our second Community Open Discussion, focusing on a transformative feature in today&#x27;s applications: AI search.This technology is rapidly becoming the new standard for delivering intelligent and intuitive search experiences.Join Dejan from our DevRel team for an open and engaging discussion.Whether you&#x27;re eager to learn, contribute your insights, or simply listen in, everyone is welcome!We&#x2019;ll talk about:The growing popularity and importance of AI search.A deep dive into the technical aspects, including embeddings generation, query term caching, and quantization techniques.An open forum to discuss best practices and various approaches to implementing AI search.A live showcase demonstrating how RavenDB AI Integration allows you to implement AI Search in just 5 minutes, with the same simplicity as our regular search API.Event Details:Date:&#xA0;Wednesday, May 7th, 19:00 CETLocation:RavenDB Developers Community DiscordBring your questions and your enthusiasm &#x2013; we look forward to seeing you there!</p>
        </article>
        <div class="button flex justify-between">
            <a href="15.html"><span class="back arrow"></span></a>

            <a href="17.html"><span class="next arrow"></span></a>
        </div>
    </section>
</main>

<footer
    class="mt-auto flex w-full flex-col items-center justify-center gap-y-2 pb-4 pt-20 text-center align-top font-semibold text-gray-600 dark:text-gray-400 sm:flex-row sm:justify-between sm:text-xs">
    <div class="me-0 sm:me-4">
        <div class="flex flex-wrap items-end gap-x-2">
            <ul class="flex flex-1 items-center gap-x-2 sm:flex-initial">
                <li class="flex">
                    <p class="flex items-end gap-2 justify-center flex-wrap	">Â© Relatively General
                        .NET 2025<span
                            class="inline-block">&nbsp;ðŸš€&nbsp;Theme: Astro Cactus</span>

                        <a class="inline-block sm:hover:text-link" href="https://github.com/chrismwilliams/astro-cactus"
                           rel="noopener noreferrer " target="_blank">
                            <svg width="1em" height="1em" viewBox="0 0 24 24" aria-hidden="true" class="h-6 w-6"
                                 focusable="false" data-icon="mdi:github">
                                <symbol id="ai:mdi:github">
                                    <path fill="currentColor"
                                          d="M12 2A10 10 0 0 0 2 12c0 4.42 2.87 8.17 6.84 9.5c.5.08.66-.23.66-.5v-1.69c-2.77.6-3.36-1.34-3.36-1.34c-.46-1.16-1.11-1.47-1.11-1.47c-.91-.62.07-.6.07-.6c1 .07 1.53 1.03 1.53 1.03c.87 1.52 2.34 1.07 2.91.83c.09-.65.35-1.09.63-1.34c-2.22-.25-4.55-1.11-4.55-4.92c0-1.11.38-2 1.03-2.71c-.1-.25-.45-1.29.1-2.64c0 0 .84-.27 2.75 1.02c.79-.22 1.65-.33 2.5-.33s1.71.11 2.5.33c1.91-1.29 2.75-1.02 2.75-1.02c.55 1.35.2 2.39.1 2.64c.65.71 1.03 1.6 1.03 2.71c0 3.82-2.34 4.66-4.57 4.91c.36.31.69.92.69 1.85V21c0 .27.16.59.67.5C19.14 20.16 22 16.42 22 12A10 10 0 0 0 12 2"></path>
                                </symbol>
                                <use xlink:href="#ai:mdi:github"></use>
                            </svg>
                            <span class="sr-only">Github</span>
                        </a>
                    </p>
                </li>
            </ul>
        </div>
    </div>
    <nav aria-label="More on this site" class="flex gap-x-2 sm:gap-x-0 sm:divide-x sm:divide-gray-500">
        <a class="px-4 py-2 sm:py-0 sm:hover:text-textColor sm:hover:underline" href="index.html"> Home </a><a
            class="px-4 py-2 sm:py-0 sm:hover:text-textColor sm:hover:underline" href="about.html"> About </a>
    </nav>
</footer>
<script src="js/script.js?id=af8f4559935e7bf5bf6015373793411d"></script>
<script src="/pagefind/pagefind-ui.js"></script>

<!-- Cookie Consent Banner -->
<div class="cookie-consent" id="cookieConsent">
    <div>
        <p class="text-sm">We use cookies to analyze our website traffic and provide a better browsing experience. By
            continuing to use our site, you agree to our use of cookies.</p>
    </div>
    <div class="cookie-consent-buttons">
        <button class="cookie-consent-decline" onclick="declineCookies()">Decline</button>
        <button class="cookie-consent-accept" onclick="acceptCookies()">Accept</button>
    </div>
</div>

<script>
    // Cookie consent management
    function showCookieConsent() {
        const consent = localStorage.getItem('cookieConsent');
        if (!consent) {
            document.getElementById('cookieConsent').classList.add('show');
        }
    }

    function acceptCookies() {
        localStorage.setItem('cookieConsent', 'accepted');
        document.getElementById('cookieConsent').classList.remove('show');
        loadGA(); // Load Google Analytics after consent
    }

    function declineCookies() {
        localStorage.setItem('cookieConsent', 'declined');
        document.getElementById('cookieConsent').classList.remove('show');
    }

    // Show the consent banner only for EU visitors (you can add more country codes as needed)
    fetch('https://ipapi.co/json/')
            .then(response => response.json())
            .then(data => {
                const euCountries = ['AT', 'BE', 'BG', 'HR', 'CY', 'CZ', 'DK', 'EE', 'FI', 'FR', 'DE', 'GR', 'HU', 'IE', 'IT', 'LV', 'LT', 'LU', 'MT', 'NL', 'PL', 'PT', 'RO', 'SK', 'SI', 'ES', 'SE'];
                if (euCountries.includes(data.country_code)) {
                    showCookieConsent();
                } else {
                    // For non-EU visitors, automatically load GA
                    if (!localStorage.getItem('cookieConsent')) {
                        localStorage.setItem('cookieConsent', 'accepted');
                        loadGA();
                    }
                }
            })
            .catch(() => {
                // If we can't determine location, show the consent banner to be safe
                showCookieConsent();
            });
</script>
</body>
</html>
