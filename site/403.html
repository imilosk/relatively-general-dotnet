
<!DOCTYPE html>
<html class="scroll-smooth" lang="en-US" data-theme="light">
<head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, shrink-to-fit=no" name="viewport">
    <meta content="IE=edge" http-equiv="X-UA-Compatible">
    <title>Page 403 &#x2022; Relatively General .NET</title>
    <link href="favicon.ico" rel="icon" sizes="any">
    <link href="images/apple-touch-icon.png" rel="apple-touch-icon">
    <meta content="hsl()" name="theme-color">
    <meta content="website" property="og:type">
    <meta content="Home" property="og:title">
    <link rel="stylesheet" href="css/styles.css">
    <link rel="stylesheet" href="/pagefind/pagefind-ui.css">
    <!-- Google Analytics -->
    <script>
        // Only load GA if consent is given
        function loadGA() {
            const script = document.createElement('script');
            script.src = 'https://www.googletagmanager.com/gtag/js?id=G-MDFXJY3FCY';
            script.async = true;
            document.head.appendChild(script);

            window.dataLayer = window.dataLayer || [];

            function gtag() {
                dataLayer.push(arguments);
            }

            gtag('js', new Date());
            gtag('config', 'G-MDFXJY3FCY');
        }

        // Check if consent was previously given
        if (localStorage.getItem('cookieConsent') === 'accepted') {
            loadGA();
        }
    </script>
    <!-- End Google Analytics -->
</head>
<body class="mx-auto flex min-h-screen max-w-3xl flex-col bg-bgColor px-4 pt-16 font-mono text-sm font-normal text-textColor antialiased sm:px-8">
<a class="sr-only focus:not-sr-only focus:fixed focus:start-1 focus:top-1.5" href="#main">
    skip to content
</a>
<header class="group relative mb-28 flex items-center sm:ps-[4.5rem]" id="main-header">
    <div class="flex sm:flex-col">
        <a aria-current="page" class="inline-flex items-center hover:filter-none sm:relative sm:inline-block"
           href="index.html">
            <img class="me-3 sm:absolute sm:start-[-4.5rem] sm:me-0 sm:h-16 sm:w-16 w-16" src="images/giphy.gif"
                 alt=""/>
            <span class="text-xl font-bold sm:text-2xl">Relatively General .NET</span>
        </a>
        <nav aria-label="Main menu"
             class="absolute -inset-x-4 top-14 hidden flex-col items-end gap-y-4 rounded-md bg-bgColor/[.85] py-4 text-accent shadow backdrop-blur group-[.menu-open]:z-50 group-[.menu-open]:flex sm:static sm:z-auto sm:-ms-4 sm:mt-1 sm:flex sm:flex-row sm:items-center sm:divide-x sm:divide-dashed sm:divide-accent sm:rounded-none sm:bg-transparent sm:py-0 sm:shadow-none sm:backdrop-blur-none"
             id="navigation-menu">
            <a aria-current="page" class="px-4 py-4 underline-offset-2 sm:py-0 sm:hover:underline underline"
               href="index.html"> Home </a><a
                aria-current="" class="px-4 py-4 underline-offset-2 sm:py-0 sm:hover:underline " href="about.html">
                About </a>
        </nav>
    </div>
    <site-search class="ms-auto" id="search">
        <button id="open-search"
                class="flex h-9 w-9 items-center justify-center rounded-md ring-zinc-400 transition-all hover:ring-2"
                data-open-modal="">
            <svg aria-label="search" class="h-7 w-7" fill="none" height="16" stroke="currentColor"
                 stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 24 24" width="16"
                 xmlns="http://www.w3.org/2000/svg">
                <path d="M0 0h24v24H0z" stroke="none"></path>
                <path d="M3 10a7 7 0 1 0 14 0 7 7 0 1 0-14 0M21 21l-6-6"></path>
            </svg>
        </button>
        <dialog aria-label="search"
                class="h-full max-h-full w-full max-w-full border border-zinc-400 bg-bgColor shadow backdrop:backdrop-blur sm:mx-auto sm:mb-auto sm:mt-16 sm:h-max sm:max-h-[calc(100%-8rem)] sm:min-h-[15rem] sm:w-5/6 sm:max-w-[48rem] sm:rounded-md">
            <div class="dialog-frame flex flex-col gap-4 p-6 pt-12 sm:pt-6">
                <button id="close-search"
                        class="ms-auto cursor-pointer rounded-md bg-zinc-200 p-2 font-semibold dark:bg-zinc-700"
                        data-close-modal="">Close
                </button>
                <div class="search-container">
                    <div id="cactus__search"/>
                </div>
            </div>
        </dialog>
    </site-search>
    <theme-toggle class="ms-2 sm:ms-4">
        <button id="theme-toggle" class="relative h-9 w-9 rounded-md p-2 ring-zinc-400 transition-all hover:ring-2"
                type="button">
            <span class="sr-only">Dark Theme</span>
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-7 w-7 -translate-x-1/2 -translate-y-1/2 scale-100 opacity-100 transition-all dark:scale-0 dark:opacity-0"
                 fill="none" focusable="false" id="sun-svg" stroke-width="1.5" viewBox="0 0 24 24"
                 xmlns="http://www.w3.org/2000/svg">
                <path
                    d="M12 18C15.3137 18 18 15.3137 18 12C18 8.68629 15.3137 6 12 6C8.68629 6 6 8.68629 6 12C6 15.3137 8.68629 18 12 18Z"
                    stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M22 12L23 12" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M12 2V1" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M12 23V22" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M20 20L19 19" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M20 4L19 5" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M4 20L5 19" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M4 4L5 5" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M1 12L2 12" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
            </svg>
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-7 w-7 -translate-x-1/2 -translate-y-1/2 scale-0 opacity-0 transition-all dark:scale-100 dark:opacity-100"
                 fill="none" focusable="false" id="moon-svg" stroke="currentColor" stroke-width="1.5"
                 viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M0 0h24v24H0z" fill="none" stroke="none"></path>
                <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z"></path>
                <path d="M17 4a2 2 0 0 0 2 2a2 2 0 0 0 -2 2a2 2 0 0 0 -2 -2a2 2 0 0 0 2 -2"></path>
                <path d="M19 11h2m-1 -1v2"></path>
            </svg>
        </button>
    </theme-toggle>
    <mobile-button>
        <button aria-expanded="false" aria-haspopup="menu" aria-label="Open main menu"
                class="group relative ms-4 h-7 w-7 sm:invisible sm:hidden" id="toggle-navigation-menu" type="button">
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-full w-full -translate-x-1/2 -translate-y-1/2 transition-all group-aria-expanded:scale-0 group-aria-expanded:opacity-0"
                 fill="none" focusable="false" id="line-svg" stroke="currentColor" stroke-width="1.5"
                 viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M3.75 9h16.5m-16.5 6.75h16.5" stroke-linecap="round" stroke-linejoin="round"></path>
            </svg>
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-full w-full -translate-x-1/2 -translate-y-1/2 scale-0 text-accent opacity-0 transition-all group-aria-expanded:scale-100 group-aria-expanded:opacity-100"
                 fill="none" focusable="false" id="cross-svg" stroke="currentColor" stroke-width="1.5"
                 viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M6 18L18 6M6 6l12 12" stroke-linecap="round" stroke-linejoin="round"></path>
            </svg>
        </button>
    </mobile-button>
</header>


<main id="main" data-pagefind-body>
    <section aria-label="Blog post list">
        <article id="article-4021">
            <a href="https://ayende.com/blog/165665/on-replication-strategies-or-the-return-of-the-long-article" target="_blank">
                <h2 class="title mb-6" id="article-4021">On replication strategies, or the return of the long article</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: February 24, 2014
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">Meta note: I&#x2019;ve been doing short series of blog posts for a while. I thought that this would be a good time to change. I am not sure how big this blog post is going to be, but it is going to be big. Please let me know about which approach you find better, and your reasoning. I have been thinking about this quite a lot in the past few days. I am trying to see if there is a common solution to replication in general that we can utilize across a number of solutions. If we can do that, we can provide much better feature set for a wide variety of scenarios.  But before we can talk about how to actually implement replication, we need to talk about what type of replication we are talking about. We are assuming a single database (non sharded, running on multiple nodes). In general, there appears to be the following options:  Master / slaves Primary / secondaries Multi write partners Multi master  Those are just designations that I&#x2019;ll use for this series of blog posts. For the purpose of those posts, they are very different beast indeed. The master/ slaves approach is talking specifically for a scenario where you have a single write master and one or more slaves. A key aspect of this strategy is that you can never (at least under normal operations) make any change whatsoever to the slaves. They are pure reads, and they can not be changed to become writeable without severing their ties to the master or risking data corruption. A common example of such an approach is log shipping. I&#x2019;ll discuss it in detail later on, but you can look at the docs for any such system, changing a slave to be writable is a decidedly non trivial process. And for a good reason. The primary / secondaries mode is very similar to the master / slaves approach, however, here we have an explicit option for a secondary to become the primary. There can be only one primary, but the idea is that we allow a much easier way to switch the primary node. MongoDB uses such a system.  Multi write partners systems allow any node to accept a write, and it will take care of distributing the change to the other nodes. It also need, unlike the other options so far, to deal with conflicts. The ability of two users to write to the same value on multiple nodes at the same time. However, multi write partners usually make assumptions about their partners. For example, that they are relatively in sync, and that there is a separate protocol for bringing a new node online into the partnership that is outside the usual replication metric. Multi master systems allow, accept and encourages nodes to come and go as they please, they assume that writes can and will conflict, and the need to resolve that on an ongoing basis. There are no expectations from the other nodes about being relatively in sync, and it is common to &#x201C;re-seed&#x201D; a new node by just starting replication to it, which means that you need to replicate all the data from the beginning of time to it. It is also common to have a node pop up once in a blue moon, expect to get all changes that happened while it was gone, and then drop off again. Let us look at the actual implementation details of each, including some examples, and hopefully it&#x2019;ll be clearer what I am talking about. &#xA0;  Log Shipping Master / slaves is usually implemented via log shipping. The easiest way to think about log shipping is that the master database will send (magically, we don&#x2019;t really care much how at this point) to the slaves instructions on how to directly modify the database files. In other words, conceptually, it is sending them the following:      1: writep(fd1, 1024, new[]{ 17,85,124,13,86}, 5);   2: writep(fd1, 18432, new[]{ 12,95,34,83,76,32,59}, 7);b&#xA;Those are very low level modifications, as you can imagine. The advantage here is that it is very easy to capture and replay those changes. The disadvantage is that you cannot really do anything else. Because the changes are happening at the very bottom of the stack, there is no chance to run any sort of logic. We are just writing to the file, same as the master server did. &#xA;This is the key reason why it is so hard for a slave to allow writes. The moment it makes any independent write, it opens itself up to the risk that the master would also do a write, that would generate data corruption. That is why you have to do the major song &amp; dance if you want to switch the master &amp; the slave. You have to go through all of this trouble to ensure that you don&#x2019;t ever have a scenario where you have a write happening on both ends.&#xA;Once that happens, you can never ever get those two in sync again. It is just happening at too low a level. &#xA;Generating a new node, however, is very easy. Make sure to keep the journal around, do a full backup of the database and move it to another node. Then start shipping the logs over. Because they started at the same point, they can be safely applied.&#xA;Note that this version is very sensitive to versioning issues. You cannot have even a tiny change in the versions of working with the low level storage, because then all hell might break lose. This method is very good for generating read replicas. Indeed, this is what this is used for most of the time.&#xA;In theory, you can even get it to do failovers, because while the master is down, the slave can write. The problem is how do you handle a case where the slave think that the master is down, and the master think that everything is fine. At that point, you might have both of them accept writes, resulting in an unmergable situation.&#xA;In theory, since they share a common root, you can decide that one of them is the victor, and go with that, but that would result in losing data from the loser server, and probably data that you have no actual way of getting back. The changes we keep track of here are very small, and likely too granular to allow you to actually do something meaningful to extract the changed information.&#xA;&#xA;Oplog&#xA;This is actually quite similar to the log shipping method, but instead of sending the very low level file I/O operations, we&#x2019;re actually sending higher level commands. This leads to a quite a few benefits as far as we are concerned. The primary server can send its log as:&#xA;&#xA;&#xA;   1: set(&quot;users/1&quot;, {&quot;name&quot;: &quot;oren&quot; });   2: set(&quot;users/2&quot;, {&quot;name&quot;: &quot;ayende&quot; });   3: del(&quot;users/1&quot;);&#xA;Executing this set of instruction on the secondary will result in identical state on the secondary.&#xA0; Unlike Log Shipping option, this actually require the secondary server to perform work, so it is more expensive than just apply the already computed file updates.&#xA;However, the upside of this is that you can have a far more readable log. It is also much easier to turn a secondary into a primary. Mostly, this is silly. The actual operation is the exact same thing. But because you are working at the protocol level, rather than the file level. You can get some interesting benefits. &#xA;Let us assume that you have the same split brain issue, when both primary &amp; secondary think that they are the primary. In the Log Shipping case, we had no way to reconcile the differences. In the case of Oplog, we can actually do this.&#xA0; The key here is that we can:&#xA;&#xA;Dump one of the servers rejected operations into a recoverable state.&#xA;Attempt to apply both severs logs, hoping that they didn&#x2019;t both work on the same document.&#xA;This is the replication mode used by MongoDB. And it has chosen the first approach for handling such conflicts. Indeed, that is pretty much the only choice that it can safely make. Two servers making modifications to the same object is always going to require manual resolution, of course. And it is usually better to have to do this in advance and explicitly rather than &#x201C;sometimes it works&#x201D;.&#xA;You can see some discussion on how merging back divergent writes works in MongoDB here. In fact, continuing to use the same source, you can see the internal oplog in MongoDB here:&#xA;&#xA;&#xA;   1: // Operations   2:&#xA0;    3: &gt; use test   4: switched to db test   5: &gt; db.foo.insert({x:1})   6: &gt; db.foo.update({x:1}, {$set : {y:1}})   7: &gt; db.foo.update({x:2}, {$set : {y:1}}, true)   8: &gt; db.foo.remove({x:1})   9:&#xA0;   10: // Op log view  11:&#xA0;   12: &gt; use local  13: switched to db local  14: &gt; db.oplog.rs.find()  15: { &quot;ts&quot; : { &quot;t&quot; : 1286821527000, &quot;i&quot; : 1 }, &quot;h&quot; : NumberLong(0), &quot;op&quot; : &quot;n&quot;, &quot;ns&quot; : &quot;&quot;, &quot;o&quot; : { &quot;msg&quot; : &quot;initiating set&quot; } }  16: { &quot;ts&quot; : { &quot;t&quot; : 1286821977000, &quot;i&quot; : 1 }, &quot;h&quot; : NumberLong(&quot;1722870850266333201&quot;), &quot;op&quot; : &quot;i&quot;, &quot;ns&quot; : &quot;test.foo&quot;, &quot;o&quot; : { &quot;_id&quot; : ObjectId(&quot;4cb35859007cc1f4f9f7f85d&quot;), &quot;x&quot; : 1 } }  17: { &quot;ts&quot; : { &quot;t&quot; : 1286821984000, &quot;i&quot; : 1 }, &quot;h&quot; : NumberLong(&quot;1633487572904743924&quot;), &quot;op&quot; : &quot;u&quot;, &quot;ns&quot; : &quot;test.foo&quot;, &quot;o2&quot; : { &quot;_id&quot; : ObjectId(&quot;4cb35859007cc1f4f9f7f85d&quot;) }, &quot;o&quot; : { &quot;$set&quot; : { &quot;y&quot; : 1 } } }  18: { &quot;ts&quot; : { &quot;t&quot; : 1286821993000, &quot;i&quot; : 1 }, &quot;h&quot; : NumberLong(&quot;5491114356580488109&quot;), &quot;op&quot; : &quot;i&quot;, &quot;ns&quot; : &quot;test.foo&quot;, &quot;o&quot; : { &quot;_id&quot; : ObjectId(&quot;4cb3586928ce78a2245fbd57&quot;), &quot;x&quot; : 2, &quot;y&quot; : 1 } }  19: { &quot;ts&quot; : { &quot;t&quot; : 1286821996000, &quot;i&quot; : 1 }, &quot;h&quot; : NumberLong(&quot;243223472855067144&quot;), &quot;op&quot; : &quot;d&quot;, &quot;ns&quot; : &quot;test.foo&quot;, &quot;b&quot; : true, &quot;o&quot; : { &quot;_id&quot; : ObjectId(&quot;4cb35859007cc1f4f9f7f85d&quot;) } }&#xA;You can actually see the chain on command to oplog entry. The upsert command in line 7 was turned into an insert in line 18, for example. There appears to also be a lot of work done to avoid having to do any sort of computable work, in favor of resolving things to a simple idempotent operation.&#xA;&#xA;For example, if you have a doc that looks like {counter:1} and you do an update like {$inc:{counter:1}} on the primary, you&#x2019;ll end up with {counter:2} and the oplog will store {$set:{counter:2}}. The secondaries will replicate that instead of the $inc.&#xA;That is pretty nice feature, since it mean that you can much apply changes multiple times and end with the same result. But it all leads to the end result, in which you can&#x2019;t merge divergent writes.&#xA;You do get a much better approach for actually going over the data and doing the fixup yourself, but still.. I don&#x2019;t really like it.&#xA;&#xA;Multi write partners&#xA;In this mode, we have a set of servers, each of which is familiar with their partners. All the writes coming are accepted, and logged. Replication happen from the source server contacting all of the destination servers and asking them: What is the last you heard from me? Here are all of my changes since then. Critically, it is at this point that we can trim the log for all of the actions that were already replicated to all of the servers.&#xA;A server being down means that the log of changes to go there is going to increase in size until the partner is up again, or we remove the entry for that server from our replication destination. &#xA;So far, this is very similar to how you would structure an oplog. The major difference is how you structure the actual data you log. In the oplog scenario, you&#x2019;re going to write the changes that happens to the system. And the only way to act on this is to actually apply the op log in the same sequence as it was generated. This leads to a system where you can always have just a single primary node. And that leads to situations when split brains will result in data loss or manual merge steps.&#xA;In MWP case, we are going to keep enough context (usually full objects) so that we can give the user a better option to resolve the conflict. This also gives us the option of replaying the log in non sequential manner.&#xA;Note, however, that you cannot just bring a new server online and expect it to start playing nicely. You have to start from a known state, usually a db backup of an existing node. Like the log shipping scenario, the process is essentially, start replicating (to the currently non existent server), that will ensure that the log will be there when we actually have the new server. Backup the database and restore on a secondary server. Configure to accept replication from the source server.&#xA;The complexities here are that you need to deal with operations that you might already have. That is why this is usually paired with vector clocks, so you can automatically resolve such conflicts. When you cannot resolve such conflicts, this falls down to manual user intervention. &#xA;&#xA;Multi Master&#xA;Multi master systems are quite similar to multi write partners, but they are designed to operate independently. It is common for servers to be able communicate with one another only rarely. For example, a mobile system that is only able to get connected just a few hours a week. As such, we cannot just cap the size of the operations to replicate. In fact, the common way to bring a new server up to speed is just to replicate to it. That means that we need to be able to replicate, essentially from any point in the server history, to a new server.&#xA;That works great, as long as you don&#x2019;t have deletes. Those do tend to make things harder, because you need to keep track of those, and replicate them. RavenDB and CouchDB are both multi master systems, for example. Conflicts works the same way, pretty much, and we use a vector clock to determine if a value is in conflict or not.&#xA;&#xA0;&#xA;&#xA;Divergent writes&#xA;I mentioned this a few times, but I didn&#x2019;t fully explain. For my purposes, we assume that we are using 2 servers (and yes, I know all about quorums, etc. Not relevant for this discussion) running in master/slave mode.&#xA;At some point, the slave think that the master is down and takes over, and the master doesn&#x2019;t notice this and still think it is the master. During this time, both server accept the following writes:&#xA;&#xA;&#xA;&#xA;&#xA;Server A&#xA;Server B&#xA;&#xA;write users/1&#xA;wrier users/2&#xA;&#xA;write users/3&#xA;write users/3&#xA;&#xA;delete users/4&#xA;delete users/5&#xA;&#xA;delete users/6&#xA;write users/6&#xA;&#xA;write users/7&#xA;delete all users&#xA;&#xA;set all users to active&#xA;write users/8&#xA;After those operation happen, we restore communication between the two servers and they need to decide how to resolve those changes&#xA;&#xA;Getting down to business&#xA;Okay, that is enough talking about what those terms mean. Let us consider the implications of using them. Log shipping is by far the simplest method to use. Well, assuming that you actually have a log mechanism, but most dbs do. It is strictly one writer model, and there is absolutely no way to either resolve divergent writes or even to find out what they were. The good thing about log shipping is that it is quite easy to get this working without actually needing to care anything about the actual data involved. We work directly at the file level, we don&#x2019;t care at all about what the data is. The problem is that we can&#x2019;t even solve simple conflicts, like writes to the different objects. This is because we are actually working at the file level, and all the changes are there. Attempting to merge changes from multiple logs would likely result in file corruption. The up side is that it is probably the most efficient way to go about doing this.&#xA;Oplog is a step above log shipping, but not a big one. It doesn&#x2019;t resolve the divergent writes issues. This is now an application level protocol. The log needs to contain information specific to the actual type of data that we store. And you need to write explicit code to handle this. That is nice, but it also require strict sequence of all operations. Now, you can try to merge things between different logs. However, you need to worry about conflicts, and more to the point, there is usually nothing in the data itself that will help you even detect conflicts.&#xA;Multi write partners are meant to take this up a notch. They do keep track of the version history (usually via vector clocks). Here, the situation is more complex, because we need to explicitly decide how to deal with conflicts (either resolve automatically or defer to user decision), but also how to handle distribution of updates. Usually they are paired with some form of logic that tells you how to direct your writes. So all writes for a particular piece of data would go to a preferred node, to avoid generating multiple versions. The data needs to contains some information about that, so we keep vector clock information around. Once we sent the changes to all our partners, we can drop them, saving in space.&#xA;Multi master is meant to ensure that you can have partners that might only see one another occasionally, and it makes no assumptions about the topology. It can handle a node that comes on, get some data replicated, and drop off for a while (or forever). Each node is fully independent, and while they would collaborate with others, they don&#x2019;t need them. The downside here is that we need to keep track of some things forever. In particular, we need to keep track of deletes, to ensure that we can get them to the remote machines.&#xA;&#xA;What about set operations?&#xA;Interesting enough, that is probably the hardest issue to resolve. Consider the case when you have the following operations happen:&#xA;&#xA;&#xA;&#xA;&#xA;Server A&#xA;Server B&#xA;&#xA;write users/7&#xA;delete all users&#xA;&#xA;set all users to active&#xA;write users/8 (active: false)&#xA;What should be the result of this? There isn&#x2019;t a really good answer. Should users/8 be set to active: true? What about users/7, should it be deleted or kept?&#xA;It gets hard because you don&#x2019;t have good choices. The hard part here is actually figuring out that you have a conflict. And there isn&#x2019;t a really good way to handle set operations nicely with conflicts. The common solution is to translate this to the actual operations made (delete users/1,user/2, users/3 &#x2013; writer users/8, users/5) and leave it at that. The set based operation is translated to the actual individual operations that actually happened. And on that we can detect conflicts much more easily.&#xA;Log shipping is easiest to work with, operationally speaking. You know what you get, and you deal with that. Oplog is also simple, you have a single master, and that works. Multi master and multi write partners requires you to take explicit notice of conflicts, selection of the appropriate node to reduce conflicts, etc.&#xA;In practice, at least in the scenarios relating to RavenDB, the ability to take a server offline for weeks or months doesn&#x2019;t seem to be used that often. The common deployment model is of servers running as steady partners. There are some optimizations that you can do for multi write partners that are hard/impossible to do with multi master.&#xA;My current personal preference at this point would like to go with either log shipping or multi write master. I think that either one of them would be fairly simple to implement and support operationally. I think that I&#x2019;ll discuss actual design for the time series topic using either option in my next posts.</p>
        </article>
        <article id="article-4022">
            <a href="https://ayende.com/blog/165633/logging-production-systems" target="_blank">
                <h2 class="title mb-6" id="article-4022">Logging &amp; Production systems</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: February 21, 2014
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">I got a question from Dominic about logging:  Jeff Atwood wrote a great blog post about over-using logging, where stack traces should be all a developer needs to find the root cause of a problem. Therefore ... When building an enterprise level system, what rules do you have to deem a log message &#x27;useful&#x27; to a developer or support staff? This is the relevant part in Jeff&#x2019;s post:  So is logging a giant waste of time? I&#x27;m sure some people will read about this far and draw that conclusion, no matter what else I write. I am not anti-logging. I am anti-abusive-logging. Like any other tool in your toolkit, when used properly and appropriately, it can help you create better programs. The problem with logging isn&#x27;t the logging, per se -- it&#x27;s the seductive OCD &quot;just one more bit of data in the log&quot; trap that programmers fall into when implementing logging. Logging gets a bad name because it&#x27;s so often abused. It&#x27;s a shame to end up with all this extra code generating volumes and volumes of logs that aren&#x27;t helping anyone. We&#x27;ve since removed all logging from Stack Overflow, relying exclusively on exception logging. Honestly, I don&#x27;t miss it at all. I can&#x27;t even think of a single time since then that I&#x27;d wished I&#x27;d had a giant verbose logfile to help me diagnose a problem. I don&#x2019;t really think that I can disagree with this more vehemently. This might be a valid approach if/when you are writing what is essentially a single threaded, single use, code. It just so happens that most web applications are actually composed of something like that. The request code very rarely does threading, and it is usually just dealing with its own stuff. For system where most of the code is actually doing ongoing work, there really isn&#x2019;t any alternative to logging. You cannot debug multi threaded code efficiently. The only way to really handle that is to do printf debugging. In which you write what happens, and then construct the actual execution from the traces. And that is leaving aside one very important issue. It isn&#x2019;t the exceptions that will get you, it is when your system is subtly wrong. Maybe it missed an update, or skipped a validation, or something just doesn&#x2019;t look right. And you need to figure out what is going on. And then you have distributed system, when you might have things happening concurrently in multiple systems, and good luck trying to get a good grip about how to resolve problems without using logging. Speaking of which, here is a reply to a customer from one of our developers:   There is absolutely no way this would have been found without logging. The actual client&#xA0; visible issue happened quite a bit later than when the actual bug was, and no exception was thrown. Of course, this is all just solving problems on the developer machine. When you go to production, the rules are different, usually the only thing that you have are the logs, and you need to be able to figure out what was wrong and how to fix it, when the system is running. I find that I don&#x2019;t really sweat Debug vs. Info and Warn vs. Error debates. The developers will write whatever they consider to be relevant on each case. And you might get Errors that show up in the logs that are error for that feature, but are merely warnings for the entire product.&#xA0; I don&#x2019;t care, all of that can be massages later, using log configuration &amp; filtering. But the very first thing that has to happen is to have the logs. Most of the time you don&#x2019;t care, but it is pretty much the same as in saying: &#x201C;We removed all the defibrillators from the building, because they were expensive and took up space. We rely exclusively on CPR in the event of a heart failure. Honestly, I don&#x2019;t miss it at all. I can&#x2019;t think of a single time since then that I&#x2019;d wished I&#x2019;d a machine to send electricity into someone&#x2019;s heart to solve a problem.&#x201D; When you&#x2019;ll realize that you need it, it is likely going to be far too late.</p>
        </article>
        <article id="article-4023">
            <a href="https://ardalis.com/working-with-kendo-ui-templates/" target="_blank">
                <h2 class="title mb-6" id="article-4023">Working with Kendo UI Templates</h2>
            </a>
            <p class="mb-2">by Ardalis</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: February 20, 2014
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">I&#x2019;m watching the Introduction to Kendo UI course by Keith Burnell on Pluralsight and decided to play around a little bit with templates&#x2026;Keep Reading &#x2192;</p>
        </article>
        <article id="article-4024">
            <a href="https://ayende.com/blog/165604/time-series-feature-design-querying-over-large-data-sets" target="_blank">
                <h2 class="title mb-6" id="article-4024">Time series feature design</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: February 20, 2014
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">What happens when you want to do an aggregation query over very large data set? Let us say that you have 1 million data points within the range you want to query, and you want to get a rollup of all the data in the range of a week. As it turns out, there is a relatively simple solution for optimizing this and maintaining a relatively constant query speed, regardless of the actual data set size. Time series data has the great benefit of being easily aggregated. Most often, the data looks like this:  The catch is that you have a lot of it.  The set of aggregation that you can do over the data is also relatively simple. You have mean, max, min, std deviation, etc. The time ranges are also pretty fixed, and the nice thing about time series data is that the bigger the range you want to go over, the bigger your rollup window is. In other words, if you want to look at things over a week, you would probably use a day or hour rollup. If you want to see things over a month, you will use a week or a day, over a year, you&#x2019;ll use a week or a month, etc. Let us assume that the cost of aggregation is 10,000 operations per second (just some number I pulled because it is round and nice to work with, real number is likely several orders of magnitude higher). So if we have to run this over a set that is 1 million data points in size, with the data being entered on per minute basis. With 1 million data points, we are going to wait almost two minutes for the reply. But there is really no need to actually check all those data points manually. What we can do is actually prepare, ahead of time, the rollups on an hourly basis. That gives us a summary on a per hour basis of just over 16 thousand data points, and will result in a query that runs in under 2 seconds. If we also do a daily rollup, we move from having a million data points to less than a thousand.  Actually maintaining those computed rollups would probably be a bit complex, but it won&#x2019;t be any more complex than how we are computing map/reduce indexes in RavenDB (this is a private, and simplified, case of map/reduce). And the result would be instant query times, even on very large data sets.</p>
        </article>
        <article id="article-4025">
            <a href="https://ayende.com/blog/165603/time-series-feature-design-scale-out-high-availability" target="_blank">
                <h2 class="title mb-6" id="article-4025">Time series feature design</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: February 19, 2014
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">I expect (at a bare minimum), to be able to do about 25,000 sustained writes per second on a single machine. I actually expect to be able to do significantly more. But let us go with that amount for now as something that if it drops below that value, we are in trouble. Over a day, that is 2.16 billion records. I don&#x2019;t think this likely, but that is a lot of data to work with. That leads to interesting questions on scale out story. Do we need one? Well, probably not for performance reasons (said the person who haven&#x2019;t written the code, much less benchmarked it) at least not if my hope for the actual system performance comes about. However, just writing the data isn&#x2019;t very interesting, we also need to read and work with it. One of the really nice thing about time series data is that it is very easily sharded, because pretty much all the operations you have are naturally accumulative. You have too much data for a single server, go ahead and split it. On query time, you can easily merge it all back up again and be done with it. However, a much more interesting scenario for us is the high availability / replication story. This is where things gets&#x2026; interesting. With RavenDB, we do async server side replication. With time series data, we can do that (and we have the big advantage of not having to worry about conflicts), but the question is how. The underlying mechanism for all the replication in RavenDB is the notion of etags, of a way to track the order of changes to the database. In time series data, that means tracking the changes that happens in some form of a sane fashion. It means having to track, per write, at least another 16 bytes. (8 bytes for a monotonically increasing etag number, another 8 for the time that was changed). And we haven&#x2019;t yet spoken of things like replication of deletes, replication of series tag changes, etc. I can tell you that dealing with replication of deletes in RavenDB, for example, was no picnic.  A much simpler alternative would be to not handle this at the data level. One nice thing about Voron is that we can actually do log shipping. That would trivially solve pretty much the entire problem set of replication, because it would happen at the storage layer, and take care of all of it. However, it does come with its own set of issues. In particular, it means that the secondary server has to be read only, it cannot accept any writes. And by that I mean that it would physically reject them. That leads to a great scale out story with read replicas, but it means that you have to carefully consider how you are dealing the case of the primary server going down for a duration, or any master/master story. I am not sure that I have any good ideas about this at this point in time. I would love to hear suggestions.</p>
        </article>
        <article id="article-4026">
            <a href="https://ayende.com/blog/165602/time-series-feature-design-user-interface" target="_blank">
                <h2 class="title mb-6" id="article-4026">Time series feature design</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: February 18, 2014
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">We need one.&#xA0; But that is pretty much all I have to say about it. Most of the ways you&#x2019;ve to look at time series data end up something like this:  But before you can get here, you need to handle:  Looking at series Inspecting raw series data Tagging / searching series Looking at the roll up values across different dates for different series Delete data (full series or range of data) Probably other stuff, but those are the things that I can think of. Probably a good place to do a lot of graphs, too, just to let the users play with the data. But that is what I have in mind here so far. Oh, and quite obviously, we want to be able to output everything to CSV so users can look at that in Excel, of course.</p>
        </article>
        <article id="article-4027">
            <a href="https://ayende.com/blog/165601/time-series-feature-design-client-api" target="_blank">
                <h2 class="title mb-6" id="article-4027">Time series feature design</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: February 17, 2014
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">We have gone over the system behavior, the wire protocol and how we actually store the data on disk. Now, let us talk about the actual client API. The entry point is going to the TimeSeries class, which will have the following behavior: Stateless operations:  Queries:  timeSeries.Query(&#x201C;sensor1.heat&#x201D;, &#x201C;sensor1.flow&#x201D;)&#xA0;&#xA0; .Range(start,end)&#xA0;&#xA0; .Rollup(Rollup.Weekly)&#xA0;&#xA0; .Aggergation(AggergateBy.Max, AggergateBy.Min, AggergateBy.Mean); timeSeries.SeriesBy(&#x201C;temp:C&#x201D;); Operations:  timeSeries.Delete(&#x201C;sensor1.heat&#x201D;, start, end); timeSeries.Tag(&#x201C;sensor1.heat&#x201D;, &#x201C;temp:C&#x201D;); Those types of operations have no state, require nothing beyond just knowing where the server is located and can be immediately executed without requiring any external state. The returned results aren&#x2019;t tracked or managed&#xA0; by us in any way, so there is no need for a session.&#xA0;  Stateful operation - The only stateful operation we have (at least so far) is adding data to the database. We do that using the connection abstraction. This is very close to the actual on the wire representation, which is always good. We have something like:      1: using(var con = timeSeries.OpenConnection(waitForServerFlush: true))   2: {   3:     using(var series = con.AddToSeries(&quot;sensor1.heat&quot;))   4:     {   5:         for(var i = 0; i &lt; 100; i&#x2B;&#x2B;)    6:         {   7:             series.Add(time.AddMinutes(i), value &#x2B; i);   8:         }   9:     }  10: }&#xA;This is a bit of an awkward API, but it serves a purpose, it is very close to the way the on-wire format is, and it is optimized for performance, not for being nice.&#xA;We can also have:&#xA;&#xA;con.Add(&#x201C;sensor1.heat&#x201D;, time, value);&#xA;But if you are mixing things up (add sensor1.heat, sensor1.flow and then sensor1.heat again, etc), it probably won&#x2019;t be as efficient. (It is important to be able to expose those optimizations all the way from the disk to the wire to the client API. Most times, they don&#x2019;t matter, which is why we have the higher level API, but when they do, they really do.&#xA;And&#x2026; this is pretty much it.&#xA;The API will probably be an async one, to keep up with the times, but those are pretty much the high level things that we have here.</p>
        </article>
        <article id="article-4028">
            <a href="https://ayende.com/blog/165825/from-ravendb-with-love" target="_blank">
                <h2 class="title mb-6" id="article-4028">From RavenDB, with love</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: February 14, 2014
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">For Valentine day, we are going to have a special sale for the upcoming RavenDB conference. The sale is for 2 or more tickets only.  RavenDB Conference &amp; Workshop Special Offer &#x2013; Save 700$  RavenDB Conference &#x2013; Special Offer &#x2013; Save 28$</p>
        </article>
        <article id="article-4029">
            <a href="https://ayende.com/blog/165573/time-series-feature-design-system-behavior" target="_blank">
                <h2 class="title mb-6" id="article-4029">Time series feature design</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: February 14, 2014
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">It is easy to forget that a database isn&#x2019;t just about storing and retrieving data. A lot of work goes into the actual behavior of the system beyond the storage aspect.  In the case of time series database, just storing the data isn&#x2019;t enough, we very rarely actually want to access all the data. If we have a sensor that send us a value once a minute, that comes to 43,200 data points per month. There is very little that we actually want to do for that amount of data. Usually we want to do things over some rollup of the data. For example, we might want to see the mean per day, or the standard deviation on a weakly basis, etc. We might also want to do some down sampling. By that I mean that we take a series whose value is stored on a per minute / second basis and we want to store just the per day totals and delete the old data to save space. The reason that I am using time series data for this series of blog posts is that there really isn&#x2019;t all that much that you can do for a time series data, to be honest. You store it, aggregate over it, and&#x2026; that is about it. Users might be able to do derivations on top of that, but that is out of scope for a database product.  Can you think about any other behaviors that the system needs to provide?</p>
        </article>
        <article id="article-4030">
            <a href="https://ayende.com/blog/165572/time-series-feature-design-the-wire-format" target="_blank">
                <h2 class="title mb-6" id="article-4030">Time series feature design</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: February 13, 2014
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">Now that we actually have the storage interface nailed down, let us talk about how we are going to actually expose this over the wire. The storage interface we defined is really low level and not something that I would really care to try working with if I was writing user level code. Because we are in the business of creating databases, we want to allow this to run as a server, not just as a library. So we need to think about how we are going to expose this over the wire. There are several options here. We can expose this as a ZeroMQ service, sending messages around. This looks really interesting, and would certainly be a great research to do, but it is a foreign concept for most people, and it would have to be exposed as such through any API we use, so I am afraid we won&#x2019;t be doing that. We can write a custom TCP protocol, but that has its own issues. Chief among which, just like ZeroMQ, we would have to deal, from scratch with things like:  Authentication Tracing Debugging Hosting Monitoring Easily &#x201C;hackable&#x201D; format Replayability And probably a whole lot more on top of that. I like building the wire interface over some form of HTTP interface (I intentionally don&#x2019;t use the overly laden REST word). We get a lot of things for free, in particular, we get easy ability to do things like pipe stuff through Fiddler so we can easily debug them. That also influence decisions such as how to design the wire format itself (hint, human readable is good). I want to be able to easily generate requests and see them in the browser. I want to be able to read the fiddler output and figure out what is going on We actually have several different requirements that we need to do.   Enter data Query data Do operations Entering data is probably the most interesting aspect. The reason is that we need to handle inserting many entries in as efficient a manner as possible. That means reduce the number of server round trips. At the same time, we want to keep other familiar aspects, such as the ability to easily predict when the data is &#x201C;safe&#x201D; and the storage transaction has been committed to disk. Querying data and handling one off operations is actually much easier. We can handle it via a simple REST interface:  /timeseries/query?start=2013-01-01&amp;end=2014-01-01&amp;id=sensor1.heat&amp;id=sensor1.flow&amp;aggregation=avg&amp;period=weekly /timeseries/delete?id=sensor1.heat&amp;id=sensor1.flow&amp;start=2013-01-01&amp;end=2014-01-01 /timeseries/tag?id=sensor1.heat&amp;tag=temp:C Nothing much to it, so far. We expose it as JSON endpoints, and that is&#x2026; it. For entering data, we need something a bit more elaborate. I chose to use websockets for this, mostly because I want two way communication and that pretty much force my hand. The reason that I want to use two way communication mechanism is that I want to enable the following story:  The client send the data to the server as fast as it is able to. The server will aggregate the data and will decide, base on its own consideration, when to actually commit the data to disk. The client need some way of knowing when the data it just sent is actually flushed so it can rely on it. the connection should stay open (potentially for a long period of time) so we can keep sending new stuff in without having to create a new connection. As far as the server is concerned, by the way, it will decide to flush the pending changes to disk whenever:   over 50 ms passed waiting for the next value to arrive, or it has been at least 500 ms from the last flush, or there are over 16,384 items pending, or the client indicated that it wants to close the connection, or the moon is full on a Tuesday However, how does the server communicate that to the client? The actual format we&#x2019;ll use is something like this (binary, little endian format):      1: enter-data-stream =     2:    data-packet&#x2B;   3:&#xA0;    4: data-packet =    5:    series-name-len [4 bytes]   6:    series-name [series-name-len, utf8]   7:    entries   8:    no-entries   9:&#xA0;   10: entries =  11:    count [2 bytes]  12:    entry[count]  13:&#xA0;   14: no-entries =  15:    count [2 bytes] = 0  16:&#xA0;   17: entry =  18:    date [8 bytes]     19:    value [8 bytes]&#xA;The basic idea is that this format should be pretty good in conveying just enough information to send the data we need, and at the same time be easy to parse and work with. I am not sure if this is a really good idea, because this might be premature optimization and we can just send the data as json strings. That would certainly be more human readable. I&#x2019;ll probably go with that and leave the &#x201C;optimized binary approach&#x201D; for when/if we actually see a real need for it. The general idea is that computer time is much cheaper than human time, so it is worth the time to make things human readable.&#xA;The server keep tracks of the number of entries that were sent to it for each connection, and whenever it is flushing the buffered data to disk, it will send notification to that effect to the client. The client can then decide &#x201C;okay, I can notify my caller that everything that I have sent has been properly saved to disk&#x201D;. Or it can just ignore it (usual case if they have a long running connection).&#xA;And that is about it as far as the wire protocol is concerned. Next, we will take a look at some of the operations that we need.</p>
        </article>
        <div class="button flex justify-between">
            <a href="402.html"><span class="back arrow"></span></a>

            <a href="404.html"><span class="next arrow"></span></a>
        </div>
    </section>
</main>

<footer
    class="mt-auto flex w-full flex-col items-center justify-center gap-y-2 pb-4 pt-20 text-center align-top font-semibold text-gray-600 dark:text-gray-400 sm:flex-row sm:justify-between sm:text-xs">
    <div class="me-0 sm:me-4">
        <div class="flex flex-wrap items-end gap-x-2">
            <ul class="flex flex-1 items-center gap-x-2 sm:flex-initial">
                <li class="flex">
                    <p class="flex items-end gap-2 justify-center flex-wrap	">© Relatively General
                        .NET 2025<span
                            class="inline-block">&nbsp;🚀&nbsp;Theme: Astro Cactus</span>

                        <a class="inline-block sm:hover:text-link" href="https://github.com/chrismwilliams/astro-cactus"
                           rel="noopener noreferrer " target="_blank">
                            <svg width="1em" height="1em" viewBox="0 0 24 24" aria-hidden="true" class="h-6 w-6"
                                 focusable="false" data-icon="mdi:github">
                                <symbol id="ai:mdi:github">
                                    <path fill="currentColor"
                                          d="M12 2A10 10 0 0 0 2 12c0 4.42 2.87 8.17 6.84 9.5c.5.08.66-.23.66-.5v-1.69c-2.77.6-3.36-1.34-3.36-1.34c-.46-1.16-1.11-1.47-1.11-1.47c-.91-.62.07-.6.07-.6c1 .07 1.53 1.03 1.53 1.03c.87 1.52 2.34 1.07 2.91.83c.09-.65.35-1.09.63-1.34c-2.22-.25-4.55-1.11-4.55-4.92c0-1.11.38-2 1.03-2.71c-.1-.25-.45-1.29.1-2.64c0 0 .84-.27 2.75 1.02c.79-.22 1.65-.33 2.5-.33s1.71.11 2.5.33c1.91-1.29 2.75-1.02 2.75-1.02c.55 1.35.2 2.39.1 2.64c.65.71 1.03 1.6 1.03 2.71c0 3.82-2.34 4.66-4.57 4.91c.36.31.69.92.69 1.85V21c0 .27.16.59.67.5C19.14 20.16 22 16.42 22 12A10 10 0 0 0 12 2"></path>
                                </symbol>
                                <use xlink:href="#ai:mdi:github"></use>
                            </svg>
                            <span class="sr-only">Github</span>
                        </a>
                    </p>
                </li>
            </ul>
        </div>
    </div>
    <nav aria-label="More on this site" class="flex gap-x-2 sm:gap-x-0 sm:divide-x sm:divide-gray-500">
        <a class="px-4 py-2 sm:py-0 sm:hover:text-textColor sm:hover:underline" href="index.html"> Home </a><a
            class="px-4 py-2 sm:py-0 sm:hover:text-textColor sm:hover:underline" href="about.html"> About </a>
    </nav>
</footer>
<script src="js/script.js?id=af8f4559935e7bf5bf6015373793411d"></script>
<script src="/pagefind/pagefind-ui.js"></script>

<!-- Cookie Consent Banner -->
<div class="cookie-consent" id="cookieConsent">
    <div>
        <p class="text-sm">We use cookies to analyze our website traffic and provide a better browsing experience. By
            continuing to use our site, you agree to our use of cookies.</p>
    </div>
    <div class="cookie-consent-buttons">
        <button class="cookie-consent-decline" onclick="declineCookies()">Decline</button>
        <button class="cookie-consent-accept" onclick="acceptCookies()">Accept</button>
    </div>
</div>

<script>
    // Cookie consent management
    function showCookieConsent() {
        const consent = localStorage.getItem('cookieConsent');
        if (!consent) {
            document.getElementById('cookieConsent').classList.add('show');
        }
    }

    function acceptCookies() {
        localStorage.setItem('cookieConsent', 'accepted');
        document.getElementById('cookieConsent').classList.remove('show');
        loadGA(); // Load Google Analytics after consent
    }

    function declineCookies() {
        localStorage.setItem('cookieConsent', 'declined');
        document.getElementById('cookieConsent').classList.remove('show');
    }

    // Show the consent banner only for EU visitors (you can add more country codes as needed)
    fetch('https://ipapi.co/json/')
            .then(response => response.json())
            .then(data => {
                const euCountries = ['AT', 'BE', 'BG', 'HR', 'CY', 'CZ', 'DK', 'EE', 'FI', 'FR', 'DE', 'GR', 'HU', 'IE', 'IT', 'LV', 'LT', 'LU', 'MT', 'NL', 'PL', 'PT', 'RO', 'SK', 'SI', 'ES', 'SE'];
                if (euCountries.includes(data.country_code)) {
                    showCookieConsent();
                } else {
                    // For non-EU visitors, automatically load GA
                    if (!localStorage.getItem('cookieConsent')) {
                        localStorage.setItem('cookieConsent', 'accepted');
                        loadGA();
                    }
                }
            })
            .catch(() => {
                // If we can't determine location, show the consent banner to be safe
                showCookieConsent();
            });
</script>
</body>
</html>
