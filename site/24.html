
<!DOCTYPE html>
<html class="scroll-smooth" lang="en-US" data-theme="light">
<head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, shrink-to-fit=no" name="viewport">
    <meta content="IE=edge" http-equiv="X-UA-Compatible">
    <title>Page 24 &#x2022; Relatively General .NET</title>
    <link href="favicon.ico" rel="icon" sizes="any">
    <link href="images/apple-touch-icon.png" rel="apple-touch-icon">
    <meta content="hsl()" name="theme-color">
    <meta content="website" property="og:type">
    <meta content="Home" property="og:title">
    <link rel="stylesheet" href="css/styles.css">
    <link rel="stylesheet" href="/pagefind/pagefind-ui.css">
    <!-- Google Analytics -->
    <script>
        // Only load GA if consent is given
        function loadGA() {
            const script = document.createElement('script');
            script.src = 'https://www.googletagmanager.com/gtag/js?id=G-MDFXJY3FCY';
            script.async = true;
            document.head.appendChild(script);

            window.dataLayer = window.dataLayer || [];

            function gtag() {
                dataLayer.push(arguments);
            }

            gtag('js', new Date());
            gtag('config', 'G-MDFXJY3FCY');
        }

        // Check if consent was previously given
        if (localStorage.getItem('cookieConsent') === 'accepted') {
            loadGA();
        }
    </script>
    <!-- End Google Analytics -->
</head>
<body class="mx-auto flex min-h-screen max-w-3xl flex-col bg-bgColor px-4 pt-16 font-mono text-sm font-normal text-textColor antialiased sm:px-8">
<a class="sr-only focus:not-sr-only focus:fixed focus:start-1 focus:top-1.5" href="#main">
    skip to content
</a>
<header class="group relative mb-28 flex items-center sm:ps-[4.5rem]" id="main-header">
    <div class="flex sm:flex-col">
        <a aria-current="page" class="inline-flex items-center hover:filter-none sm:relative sm:inline-block"
           href="index.html">
            <img class="me-3 sm:absolute sm:start-[-4.5rem] sm:me-0 sm:h-16 sm:w-16 w-16" src="images/giphy.gif"
                 alt=""/>
            <span class="text-xl font-bold sm:text-2xl">Relatively General .NET</span>
        </a>
        <nav aria-label="Main menu"
             class="absolute -inset-x-4 top-14 hidden flex-col items-end gap-y-4 rounded-md bg-bgColor/[.85] py-4 text-accent shadow backdrop-blur group-[.menu-open]:z-50 group-[.menu-open]:flex sm:static sm:z-auto sm:-ms-4 sm:mt-1 sm:flex sm:flex-row sm:items-center sm:divide-x sm:divide-dashed sm:divide-accent sm:rounded-none sm:bg-transparent sm:py-0 sm:shadow-none sm:backdrop-blur-none"
             id="navigation-menu">
            <a aria-current="page" class="px-4 py-4 underline-offset-2 sm:py-0 sm:hover:underline underline"
               href="index.html"> Home </a><a
                aria-current="" class="px-4 py-4 underline-offset-2 sm:py-0 sm:hover:underline " href="about.html">
                About </a>
        </nav>
    </div>
    <site-search class="ms-auto" id="search">
        <button id="open-search"
                class="flex h-9 w-9 items-center justify-center rounded-md ring-zinc-400 transition-all hover:ring-2"
                data-open-modal="">
            <svg aria-label="search" class="h-7 w-7" fill="none" height="16" stroke="currentColor"
                 stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 24 24" width="16"
                 xmlns="http://www.w3.org/2000/svg">
                <path d="M0 0h24v24H0z" stroke="none"></path>
                <path d="M3 10a7 7 0 1 0 14 0 7 7 0 1 0-14 0M21 21l-6-6"></path>
            </svg>
        </button>
        <dialog aria-label="search"
                class="h-full max-h-full w-full max-w-full border border-zinc-400 bg-bgColor shadow backdrop:backdrop-blur sm:mx-auto sm:mb-auto sm:mt-16 sm:h-max sm:max-h-[calc(100%-8rem)] sm:min-h-[15rem] sm:w-5/6 sm:max-w-[48rem] sm:rounded-md">
            <div class="dialog-frame flex flex-col gap-4 p-6 pt-12 sm:pt-6">
                <button id="close-search"
                        class="ms-auto cursor-pointer rounded-md bg-zinc-200 p-2 font-semibold dark:bg-zinc-700"
                        data-close-modal="">Close
                </button>
                <div class="search-container">
                    <div id="cactus__search"/>
                </div>
            </div>
        </dialog>
    </site-search>
    <theme-toggle class="ms-2 sm:ms-4">
        <button id="theme-toggle" class="relative h-9 w-9 rounded-md p-2 ring-zinc-400 transition-all hover:ring-2"
                type="button">
            <span class="sr-only">Dark Theme</span>
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-7 w-7 -translate-x-1/2 -translate-y-1/2 scale-100 opacity-100 transition-all dark:scale-0 dark:opacity-0"
                 fill="none" focusable="false" id="sun-svg" stroke-width="1.5" viewBox="0 0 24 24"
                 xmlns="http://www.w3.org/2000/svg">
                <path
                    d="M12 18C15.3137 18 18 15.3137 18 12C18 8.68629 15.3137 6 12 6C8.68629 6 6 8.68629 6 12C6 15.3137 8.68629 18 12 18Z"
                    stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M22 12L23 12" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M12 2V1" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M12 23V22" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M20 20L19 19" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M20 4L19 5" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M4 20L5 19" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M4 4L5 5" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M1 12L2 12" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
            </svg>
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-7 w-7 -translate-x-1/2 -translate-y-1/2 scale-0 opacity-0 transition-all dark:scale-100 dark:opacity-100"
                 fill="none" focusable="false" id="moon-svg" stroke="currentColor" stroke-width="1.5"
                 viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M0 0h24v24H0z" fill="none" stroke="none"></path>
                <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z"></path>
                <path d="M17 4a2 2 0 0 0 2 2a2 2 0 0 0 -2 2a2 2 0 0 0 -2 -2a2 2 0 0 0 2 -2"></path>
                <path d="M19 11h2m-1 -1v2"></path>
            </svg>
        </button>
    </theme-toggle>
    <mobile-button>
        <button aria-expanded="false" aria-haspopup="menu" aria-label="Open main menu"
                class="group relative ms-4 h-7 w-7 sm:invisible sm:hidden" id="toggle-navigation-menu" type="button">
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-full w-full -translate-x-1/2 -translate-y-1/2 transition-all group-aria-expanded:scale-0 group-aria-expanded:opacity-0"
                 fill="none" focusable="false" id="line-svg" stroke="currentColor" stroke-width="1.5"
                 viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M3.75 9h16.5m-16.5 6.75h16.5" stroke-linecap="round" stroke-linejoin="round"></path>
            </svg>
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-full w-full -translate-x-1/2 -translate-y-1/2 scale-0 text-accent opacity-0 transition-all group-aria-expanded:scale-100 group-aria-expanded:opacity-100"
                 fill="none" focusable="false" id="cross-svg" stroke="currentColor" stroke-width="1.5"
                 viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M6 18L18 6M6 6l12 12" stroke-linecap="round" stroke-linejoin="round"></path>
            </svg>
        </button>
    </mobile-button>
</header>


<main id="main" data-pagefind-body>
    <section aria-label="Blog post list">
        <article id="article-231">
            <a href="https://devblogs.microsoft.com/dotnet/dotnet-9-performance-improvements-in-dotnet-maui/" target="_blank">
                <h2 class="title mb-6" id="article-231">.NET MAUI Performance Features in .NET 9</h2>
            </a>
            <p class="mb-2">by Jonathan,Simon</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: February 20, 2025
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">Optimize .NET MAUI application size and startup times with trimming and NativeAOT. Learn about `dotnet-trace` and `dotnet-gcdump` for measuring performance.</p>
        </article>
        <article id="article-232">
            <a href="https://ayende.com/blog/202019-C/ravendb-7-1-clocking-at-200-fsync-second" target="_blank">
                <h2 class="title mb-6" id="article-232">RavenDB 7.1</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: February 19, 2025
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">I have been delaying the discussion about the performance numbers for a reason. Once we did all the work that I described in the previous posts, we put it to an actual test and ran it against the benchmark suite. In particular, we were interested in the following scenario:High insert rate, with about 100 indexes active at the same time. Target: Higher requests / second, lowered latencyPreviously, after hitting a tipping point, we would settle at under 90 requests/second and latency spikes that hit over 700(!) ms. That was the trigger for much of this work, after all. The initial results were quite promising, we were showing massive improvement across the board, with over 300 requests/second and latency peaks of 350 ms.On the one hand, that is a really amazing boost, over 300% improvement. On the other hand, just&#xA0;300 requests/second - I hoped for much higher numbers. When we started looking into exactly what was going on, it became clear that I seriously messed up.Under load, RavenDB would issue fsync()&#xA0;at a rate of over 200/sec. That is&#x2026; a lot, and it means that we are seriously limited in the amount of work we can do. That is weird&#xA0;since we worked so hard on reducing the amount of work the disk has to do. Looking deeper, it turned out to be an interesting combination of issues.Whenever Voron changes the active journal file, we&#x2019;ll register the new journal number in the header file, which requires an fsync()&#xA0;call. Because we are using shared journals, the writes from both the database and all&#xA0;the indexes go to the same file, filling it up very quickly. That meant we were creating new journal files at a rate of more than one per second. That is quite&#xA0;a rate for creating new journal files, mostly resulting from the sheer number of writes that we funnel into a single file. The catch here is that on each&#xA0;journal file creation, we need to register each one of the environments that share the journal. In this case, we have over a hundred environments participating, and we need to update the header file for each environment. With the rate of churn that we have with the new shared journal mode, that alone increases the number of fsync()&#xA0;generated.It gets more annoying when you realize that in order to actually share the journal, we need to create a hard link between the environments. On Linux, for example, we need to write the following code:bool create_hard_link_durably(const char *src, const char *dest) {&#xD;&#xA;    if (link(src, dest) == -1) &#xD;&#xA;        return false;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    int dirfd = open(dest, O_DIRECTORY);&#xD;&#xA;    if (dirfd == -1) &#xD;&#xA;        return false;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    int rc = fsync(dirfd);&#xD;&#xA;    close(dirfd);&#xD;&#xA;    return rc != -1;&#xD;&#xA;}You need to make 4 system calls to do this properly, and most crucially, one of them is fsync()&#xA0;on the destination directory. This is required because the man page states:Calling fsync()&#xA0;does not necessarily ensure that the entry in the directory containing the file has also reached disk. &#xA0;For that an explicit fsync()&#xA0;on a file descriptor for the directory is also needed.Shared journals mode requires that we link the journal file when we record a transaction from that environment to the shared journal. In our benchmark scenario, that means that each second, we&#x2019;ll write from each environment to each journal. We need an fsync()&#xA0;for the directory of each environment per journal, and in total, we get to over 200 fsync()&#xA0;per journal file, which we replace at a rate of more than one per second.Doing nothing as fast as possible&#x2026;Even with&#xA0;this cost, we are still 3 times faster than before, which is great, but I think we can do better. In order to be able to do that, we need to figure out a way to reduce the number of fsync()&#xA0;calls being generated. The first task to handle is updating the file header whenever we create a new journal. We are already calling fsync()&#xA0;on the directory when we create the journal file, so we ensure that the file is properly persisted in the directory. There is no need to also record it in the header file. Instead, we can just use the directory listing to handle this scenario. That change alone saved us about 100 fsync()&#xA0;/ second.The second problem is with the hard links, we need to make sure that these are persisted. But calling fsync()&#xA0;for each one is cost-prohibitive. Luckily, we already have a transactional journal, and we can re-use that. As part of committing a set of transactions to the shared journal, we&#x2019;ll also record an entry in the journal with the associated linked paths. That means we can skip calling fsync()&#xA0;after creating the hard link, since if we run into a hard crash, we can recover the linked journals during the journal recovery. That allows us to skip the other&#xA0;100 fsync()&#xA0;/ second.Another action we can take to reduce costs is to increase the size of the journal files. Since we are writing entries from both the database and indexes to the same file, we are going through them a lot faster now, so increasing the default maximum size will allow us to amortize the new file costs across more transactions.The devil is in the detailsThe idea of delaying the fsync()&#xA0;of the parent directory of the linked journals until recovery is a really great one because we usually don&#x2019;t&#xA0;recover. That means we delay the cost indefinitely, after all. Recovery is rare, and adding a few milliseconds to the recovery time is usually not meaningful. However&#x2026; There is a problem when you look at this closely. The whole idea behind shared journals is that transactions from multiple storage environments are written to a single file, which is hard-linked to multiple directories. Once written, each storage environment deals with the journal files independently. That means that if the root environment is done with a journal file and deletes it before the journal file hard link was properly persisted to disk and&#xA0;there was a hard crash&#x2026; on recovery, we won&#x2019;t have a journal to re-create the hard links, and the branch environments will be in an invalid state.That is a set of circumstances that is going to be unlikely, but that is something that we have to prevent, nevertheless. The solution for that is to keep track of all the journal directories, and whenever we are about to delete a journal file, we&#x2019;ll sync all the associated journal directories. The key here is that when we do that, we keep track of the current&#xA0;journal written to that directory. Instead of having to run fsync() for&#xA0;each directory per journal file, we can amortize this cost. Because we delayed the actual syncing, we have time to create more journal files, so calling fsync()&#xA0;on the directory ensures that multiple files are properly persisted.We still have to sync the directories, but at least it is not to the tune of hundreds of times per second.Performance resultsAfter making all of those changes, we run the benchmark again. We are looking at over 500 req/sec and latency peaks that hover around 100 ms under load. As a reminder, that is almost twice as much as the improved&#xA0;version (and a hugeimprovement in latency).If we compare this to the initial result, we increased the number of requests per second by over 500%.But I have some more ideas about that, I&#x2019;ll discuss them in the next post&#x2026;</p>
        </article>
        <article id="article-233">
            <a href="https://devblogs.microsoft.com/dotnet/announcing-chroma-db-csharp-sdk/" target="_blank">
                <h2 class="title mb-6" id="article-233">Building .NET AI apps with Chroma</h2>
            </a>
            <p class="mb-2">by Luis,Jiri</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: February 19, 2025
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">Get started building AI applications using Chroma DB using the C# client SDK.</p>
        </article>
        <article id="article-234">
            <a href="https://andrewlock.net/setting-environment-variables-in-iis-and-avoiding-app-pool-restarts/" target="_blank">
                <h2 class="title mb-6" id="article-234">Setting application environment variables in IIS without restarts</h2>
            </a>
            <p class="mb-2">by Andrew Lock</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: February 18, 2025
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">In this post I outline how IIS works, describe how to add environment variables to app pools, and show how to prevent automatic app pool recycling on changes&#x2026;</p>
        </article>
        <article id="article-235">
            <a href="https://ayende.com/blog/201991-A/ravendb-7-1-shared-journals" target="_blank">
                <h2 class="title mb-6" id="article-235">RavenDB 7.1</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: February 17, 2025
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">I wrote before about a surprising benchmark&#xA0;that we ran to discover the limitations of modern I/O systems. Modern disks such as NVMe have impressive&#xA0;capacity and amazing performance for everyday usage. When it comes to the sort of activities that a database engine is interested in, the situation is quite different.At the end of the day, a transactional database cares a lot about actually persisting the data to disk safely. The usual metrics we have for disk benchmarks are all about buffered writes, that is why we run our own benchmark. The results were really interesting (see the post), basically, it feels like there is a bottleneck writing to the disk. The bottleneck is with the number&#xA0;of writes, not how big they are.If you are issuing a lot of small writes, your writes will contend on that bottleneck and you&#x2019;ll see throughput that is slow. The easiest way to think about it is to consider a bus carrying 50 people at once versus 50 cars with one person each. The same road would be able to transport a lot more people with the bus rather than with individual cars, even though the bus is heavier and (theoretically, at least) slower.Databases &amp; StoragesIn this post, I&#x2019;m using the term Storage to refer to a particular folder on disk, which is its own self-contained storage with its own ACID transactions. A RavenDB database is composed of many such Storages that are cooperating together behind the scenes.The I/O behavior we observed is very interesting for RavenDB. The way RavenDB is built is that a single database is actually made up of a central Storage for the data, and separate Storages for each of the indexes you have. That allows us to do split work between different cores, parallelize work, and most importantly, benefit from batching changes to the indexes. The downside of that is that a single transaction doesn&#x2019;t cause a single write to the disk but multiple writes. Our test case is the absolute worst-case scenario for the disk, we are making a lot of individual writes to a lot of documents, and there are about 100 indexes active on the database in question. In other words, at any given point in time, there are many&#xA0;(concurrent) outstanding writes to the disk. We don&#x2019;t actually care&#xA0;about most of those writers, mind you. The only writer that matters (and is on the critical path) is the database one. All the others are meant&#xA0;to complete in an asynchronous manner and, under load, will actually perform better if they stall (since they can benefit from batching).The problem is that we are&#xA0;suffering from this situation. In this situation, the writes that the user is actually waiting for are basically stuck in traffic behind all the lower-priority writes. That is quite&#xA0;annoying, I have to say. The role of the Journal in VoronThe Write Ahead Journal in Voron is responsible for ensuring that your transactions are durable. &#xA0;I wrote about it extensively in the past&#xA0;(in fact, I would recommend the whole series detailing the internals of Voron). In short, whenever a transaction is committed, Voron writes that to the journal file using unbuffered I/O. Remember that the database and each index are running their own separate storages, each of which can commit completely independently of the others. Under load, all of them may issue unbuffered writes at the same time, leading to congestion and our current problem.During normal operations, Voron writes to the journal, waits to flush the data to disk, and then deletes the journal. They are never&#xA0;actually read except during startup. So all the I/O here is just to verify that, on recovery, we can trust that we won&#x2019;t lose any data.The fact that we have many independent writers that aren&#x2019;t coordinating with one another is an absolute killer for our performance in this scenario. We need to find a way to solve this, but how?One option is to have both indexes and the actual document in the same storage. That would mean that we have a single journal and a single writer, which is great. However, Voron has a single writer model, and for very good reasons. We want to be able to process indexes in parallel and in batches, so that was a non-starter.The second option was to not&#xA0;write to the journal in a durable manner for indexes. That sounds&#x2026; insane for a transactional database, right? But there is logic to this madness. RavenDB doesn&#x2019;t actually need&#xA0;its indexes to be transactional, as long as they are consistent, we are fine with &#x201C;losing&#x201D; transactions (for indexes only, mind!). The reasoning behind that is that we can re-index from the documents themselves (who would&#xA0;be writing in a durable manner). We actively considered that option for a while, but it turned out that if we don&#x2019;t have a durable journal, that makes it a lot&#xA0;more difficult to recover. We can&#x2019;t rely on the data on disk to be consistent, and we don&#x2019;t have a known good source to recover from. Re-indexing a lot&#xA0;of data can also be pretty expensive. In short, that was an attractive option from a distance, but the more we looked into it, the more complex it turned out to be. The final option was to merge the journals. Instead of each index writing to its own journal, we could write to a single shared journal at the database level. The problem was that if we did individual writes, we would be right back in the same spot, now on a single file rather than many. But our tests show that this doesn&#x2019;t actually matter. Luckily, we are well-practiced in the notion of transaction merging, so this is exactly what we did. Each storage inside a database is completely independent and can carry on without needing to synchronize with any other. We defined the following model for the database:Root Storage: DocumentsBranch: Index - Users/SearchBranch: Index - Users/LastSuccessfulLoginBranch: Index - Users/ActivityThis root &amp; branch model is a simple hierarchy, with the documents storage serving as the root and the indexes as branches. Whenever an index completes a transaction, it will prepare the journal entry to be written, but instead of writing the entry to its own journal, it will pass the entry to the root. The root (the actual database, I remind you) will be able to aggregate the journal entries from its own transaction as well as all the indexes and write them to the disk in a single system call. Going back to the bus analogy, instead of each index going to the disk using its own car, they all go on the bus together.We now write all the entries from multiple storages into the same journal, which means that we have to distinguish between the different entries. &#xA0;I wrote a bit about the challenges involved&#xA0;there, but we got it done.The end result is that we now have journal writes merging for all the indexes of a particular database, which for large databases can reduce the total number of disk writes significantly. Remember our findings from earlier, bigger writes are just fine, and the disk can process them at GB/s rate. It is the number&#xA0;of individual writes that matters most here.Writing is not even half the job, recovery (read) in a shared worldThe really tough challenge here wasn&#x2019;t how to deal with the write side for this feature. Journals are never read during normal operations. Usually we only ever write to them, and they keep a persistent record of transactions until we flush all changes to the data file, at which point we can delete them.It is only when the Storage starts that we need to read from the journals, to recover all transactions that were committed to them. As you can imagine, even though this is a rare occurrence, it is one of critical importance for a database. This change means that we direct all the transactions from both the indexes and the database into a single journal file. Usually, each Storage environment has its own Journals/&#xA0;directory that stores its journal files. On startup, it will read through all those files and recover the data file. How does it work in the shared journal model? For a root storage (the database), nothing much changes. We need to take into account that the journal files may contain transactions from a different storage, such as an index, but that is easy enough to filter. What about branch storage (an index) recovery? Well, it can probably just read the Journals/&#xA0;directory of the root (the database), no?Well, maybe. Here are some problems with this approach. How do we encode the relationship between root &amp; branch? Do we store a relative path, or an absolute path? We could of course just always use the root&#x2019;s Journals/&#xA0;directory, but that is problematic. It means that we could only&#xA0;open the branch storage if we already have the root storage open. Accepting this limitation means adding a new wrinkle into the system that currently doesn&#x2019;t exist.It is highly desirable (for many reasons) to want to be able to work with just a single environment. For example, for troubleshooting a particular index, we may want to load it in isolation from its database. Losing that ability, which ties a branch storage to its root, is not something we want.The current state, by the way, in which each storage is a self-contained folder, is pretty good for us. Because we can play certain tricks. For example, we can stop a database, rename an index folder, and start it up again. The index would be effectively re-created. Then we can stop the database and rename the folders again, going back to the previous state. That is not&#xA0;possible if we tie all their lifetimes together with the same journal file.Additional complexity is not welcome in this projectBuilding a database is complicated enough, adding additional levels of complexity is a Bad Idea. Adding additional complexity to the recovery process&#xA0;(which by its very nature is both critical and rarely executed) is a Really Bad Idea.I started laying out the details about what this feature entails:A database cannot delete its journal files until all the indexes have synced their state. What happens if an index is disabled by the user?What happens if an index is in an error state?How do you manage the state across snapshot &amp; restore?There is a well-known optimization for databases in which we split the data file and the journal files into separate volumes. How is that going to work in this model?Putting the database and indexes on separate volumes altogether is also a well-known optimization technique. Is that still possible?How do we migrate from legacy databases to the new shared journal model? I started to provide answers for all of these questions&#x2026; I&#x2019;ll spare you the flow chart that was involved, it looked something between abstract art and the remains of a high school party. The problem is that at a very deep level, a Voron Storage is meant to be its own independent entity, and we should be able to deal with it as such. For example, RavenDB has a feature called Side-by-Side indexes, which allows us to have two&#xA0;versions of an index at the same time. When both the old and new versions are fully caught up, we can shut down both indexes, delete the old one, and rename the new index with the old one&#x2019;s path. A single shared journal would have to deal with this scenario explicitly, as well as many other different ones that all made such assumptions about the way the system works.Not my monkeys, not my circus, not my problemI got a great idea about how to dramatically simplify the task when I realized that a core tenet of Voron and RavenDB in general is that we should not&#xA0;go against the grain and add complexity to our lives. In the same way that Voron uses memory-mapped files and carefully designed its data access patterns to take advantage of the kernel&#x2019;s heuristics. The idea is simple, instead of having a single shared journal that is managed by the database (the root storage) and that we need to access from the indexes (the branch storages), we&#x2019;ll have a single shared journal with many references.The idea is that instead of having a single journal file, we&#x2019;ll take advantage of an interesting feature: hard links. A hard link is just a way to associate the same file data&#xA0;with multiple file names, which can reside in separate directories. A hard link is limited to files running on the same volume, and the easiest way to think about them is as pointers to the file data. Usually, we make no distinction between the file name and the file itself, but at the file system level, we can attach a single file to multiple names. The file system will manage the reference counts for the file, and when the last&#xA0;reference to the file is removed, the file system will delete the file. The idea is that we&#x2019;ll keep the same Journals/&#xA0;directory structure as before, where each Storage has its own directory. But instead of having separate journals for each index and the database, we&#x2019;ll have hard links between them. You can see how it will look like here, the numbers next to the file names are the inode numbers, you can see that there are multiple such files with the same inode number (indicating that there are multiple links to the same underlying file).. &#x2514;&#x2500;&#x2500; [  40956] my.shop.db&#xD;&#xA;    &#x251C;&#x2500;&#x2500; [  40655] Indexes&#xD;&#xA;    &#x2502;   &#x251C;&#x2500;&#x2500; [  40968] Users_ByName&#xD;&#xA;    &#x2502;   &#x2502;   &#x2514;&#x2500;&#x2500; [  40970] Journals&#xD;&#xA;    &#x2502;   &#x2502;       &#x251C;&#x2500;&#x2500; [  80120] 0002.journal&#xD;&#xA;    &#x2502;   &#x2502;       &#x2514;&#x2500;&#x2500; [  82222] 0003.journal&#xD;&#xA;    &#x2502;   &#x2514;&#x2500;&#x2500; [  40921] Users_Search&#xD;&#xA;    &#x2502;       &#x2514;&#x2500;&#x2500; [  40612] Journals&#xD;&#xA;    &#x2502;           &#x251C;&#x2500;&#x2500; [  81111] 0001.journal&#xD;&#xA;    &#x2502;           &#x2514;&#x2500;&#x2500; [  82222] 0002.journal&#xD;&#xA;    &#x2514;&#x2500;&#x2500; [  40812] Journals&#xD;&#xA;        &#x251C;&#x2500;&#x2500; [  81111] 0014.journal&#xD;&#xA;        &#x2514;&#x2500;&#x2500; [  82222] 0015.journalWith this idea, here is how a RavenDB database manages writing to the journal. When the database needs to commit a transaction, it will write to its journal, located in the Journals/ directory. If an index (a branch storage) needs to commit a transaction, it does not&#xA0;write to its own journal but passes the transaction to the database (the root storage), where it will be merged with any other writes (from the database or other indexes), reducing the number of write operations.The key difference here is that when we write to the journal, we check if that journal file is already associated with this storage environment. Take a look at the Journals/0015.journal&#xA0;file, if the Users_ByName index needs to write, it will check if the journal file is already associated with it or not. &#xA0;In this case, you can see that Journals/0015.journal&#xA0;points to the same file (inode) as Indexes/Users_ByName/Journals/0003.journal. What this means is that the shared journals mode is only applicable for committing transactions, there have been no changes required for the reads / recovery side. That is a major plus for this sort of a critical feature since it means that we can rely on code that we have proven to work over 15 years.The single writer mode makes it workA key fact to remember is that there is always only a single&#xA0;writer to the journal file. That means that there is no need to worry about contention or multiple concurrent writes competing for access. There is one writer and many readers (during recovery), and each of them can treat the file as effectively immutable during recovery.The idea behind relying on hard links is that we let the operating system manage the file references. If an index flushes its file and is done with a particular journal file, it can delete that without requiring any coordination with the rest of the indexes or the database. That lack of coordination is a huge&#xA0;simplification in the process.In the same manner, features such as copying &amp; moving folders around still work. Moving a folder will not break the hard links, but copying the folder will. In that case, we don&#x2019;t actually care, we&#x2019;ll still read from the journal files as normal. When we need to commit a new transaction after a copy, we&#x2019;ll create a new&#xA0;linked file. That means that features such as snapshots just work (although restoring from a snapshot will create multiple copies of the &#x201C;same&#x201D; journal file). We don&#x2019;t really care about that, since in short order, the journals will move beyond that and share the same set of files once again.In the same way, that is how we&#x2019;ll migrate from the old system to the new one. It is just a set of files on disk, and we can just create new hard links as needed.Advanced scenarios behaviorI mentioned earlier that a well-known technique for database optimizations is to split the database file and the journals into separate volumes (which provides higher overall I/O throughput). If the database and the indexes reside on different volumes, we cannot use hard links, and the entire premise of this feature fails. In practice, at least for our users&#x2019; scenarios, that tends to be the exception rather than the rule. And shared journals are a relevant optimization for the most common deployment model.Additional optimizations - vectored I/OThe idea behind shared journals is that we can funnel the writes from multiple environments through a single pipe, presenting the disk with fewer (and larger) writes. The fact that we need to write multiple buffers at the same time also means that we can take advantage of even more optimizations.In Windows, we can use WriteFileGather&#xA0;to submit a single system call to merge multiple writes from many indexes and the database. On Linux, we use pwritev&#xA0;for the same purpose. The end result is additional optimizations beyond just the merged writes. I have been working on this set of features for a very long time, and all of them are designed to be completely invisible to the user. They either give us a lot more flexibility internallyor they are meant to just provide better performance without requiring any action from the user.I&#x2019;m really&#xA0;looking forward to showing the performance results. We&#x2019;ll get to that in the next post&#x2026;</p>
        </article>
        <article id="article-236">
            <a href="https://www.meziantou.net/detect-missing-migrations-in-entity-framework-core.htm" target="_blank">
                <h2 class="title mb-6" id="article-236">Detect missing migrations in Entity Framework Core</h2>
            </a>
            <p class="mb-2">by G&#xE9;rald Barr&#xE9;</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: February 17, 2025
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">Entity Framework Core allows to update the database schema using migrations. The migrations are created manually by running a CLI command. It&#x27;s easy to forget to create a new migration when you change the model. To ensure the migrations are up-to-date, you can write a test that compares the current</p>
        </article>
        <article id="article-237">
            <a href="https://ayende.com/blog/201990-A/ravendb-7-1-reclaiming-disk-space" target="_blank">
                <h2 class="title mb-6" id="article-237">RavenDB 7.1</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: February 14, 2025
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">After describing in detail the major refactoring we did for how RavenDB (via Voron, its storage engine) has gone through, there is one question remaining. What&#x2019;s the point? The code is a lot&#xA0;simpler, of course, but the whole point of this much effort is to allow us to do interesting things.There is performance, of course, but we haven&#x2019;t gotten around to testing that yet because something that is a lot&#xA0;more interesting came up: Disk space management. Voron allocates disk space from the operating system in batches of up to 1GB at a time. This is done to reduce file fragmentation and allow the file system to optimize the disk layout. It used to be something critical, but SSDs and NVMe made that a lot less important (but still a&#xA0;factor).What happens if we have a very large database, but we delete a big collection of documents? This is a case where the user&#x2019;s expectations and Voron&#x2019;s behavior diverge. A user who just deleted a few million documents would expect to see a reduction in the size of the database. But Voron will mark the area on the disk as &#x201C;to-be-used-later&#x201D; and won&#x2019;t free the disk space back to the operating system. There were two reasons for this behavior:We designed Voron in an era where it was far more common for systems to have hard disks, where fragmentation was a very serious problem. It is really complicated to actually release disk space back to the operating system.The first reason is no longer that relevant, since most database servers can reasonably expect to run on SSD or NVMe these days, significantly reducing the cost of fragmentation. The second reason deserves a more in-depth answer.In order to release disk space back to the operating system, you have to do one of three things:Store the data across multiple files and delete a file where it is no longer in use.Run compaction, basically re-build the database from scratch in a compact form.Use advanced features such as sparse files (hole punching) to return space to the file system without changing the file size.The first option, using multiple files, is possible but pretty complex. Mostly because of the details of how you split to multiple files, whenever a single entry in an otherwise empty file will prevent its deletion, etc. There are also practical issues, such as the number of open file handles that are allowed, internal costs at the operating system level, etc.Compaction, on the other hand, requires that you have enough space available during the compaction to run. In other words, if your disk is 85% full, and you delete 30% of the data, you don&#x2019;t have free space to run a compaction. Another consideration for compaction is that it can be really expensive. Running compaction on a 100GB database, for example, can easily take hours and in the cloud will very quickly exhaust your I/O credits.RavenDB &amp; Voron have supported compaction for over a decade, but it was always something that you did on very rare occasions. A user had to manually trigger it, and the downsides are pretty heavy, as you can see.In most cases, I have to say, returning disk space back to the operating system is not something that is that interesting. That free space is handled by RavenDB and will be reused before we&#x2019;ll allocate any additional new space from the OS. However, this is one of those features that keep coming up, because we go against users&#x2019; expectations. The final option I discussed is using hole punching or sparse files (the two are pretty much equivalent - different terms between operating systems). The idea is that we can go to the operating system and tell it that a certain range in the file is not used, and that it can make use of that disk space again. Any future read from that range will return zeroes. If you write to this region, the file system will allocate additional space for those writes.That behavior is problematic for RavenDB, because we used to use memory-mapped I/O to write to the disk. If there isn&#x2019;t sufficient space to write, memory-mapped I/O is going to generate a segmentation fault / access violation. In general, getting an access violation because of a disk full is not&#xA0;okay by us, so we couldn&#x2019;t use sparse files. The only option we were able to offer to reduce disk space was full compaction.You might have noticed that I used past tense in the last paragraph. That is because I am now no longer limited to using just memory-mapped I/O. Using normal I/O for this purpose works&#xA0;even if we run out of disk space, we will get the usual disk full error (which we are already handling anyway). Yes, that means that starting with RavenDB 7.1, we&#x2019;ll automatically release free disk space directly back to the operating system, matching your likely expectations about the behavior. This is done in increments of 1MB, since we still want to reduce fragmentation and the number of file metadata that the file system needs to manage.The one MB triggerRavenDB will punch a hole in the file whenever there is a consecutive 1MB of free space. This is important to understand because of fragmentation. If you wrote 100 million documents, each 2 KB in size, and then deleted every second document, what do you think will happen? There won&#x2019;t be&#xA0;any consecutive 1MB range for us to free.Luckily, that sort of scenario tends to be pretty rare, and it is far more common to have clustering of writes and deletes, which allow us to take advantage of locality and free the disk space back to the OS automatically.RavenDB will first use all the free space inside the file, reclaiming sparse regions as needed, before it will request additional disk space from the OS. When we do&#xA0;request additional space, we&#x2019;ll still get it in large chunks (and without using sparse files). That is because it is far more likely to be immediately used, and we want to avoid giving the file system too much work.Note that the overall file&#xA0;size is going to stay the same, but the actually used&#xA0;disk space is going to be reduced. We updated the RavenDB Studio to report both numbers, but when browsing the files manually, you need to keep that in mind.I expect that this will be most noticeable for users who are running on cloud instances, where it is common to size the disks to be just sufficiently big enough for actual usage. It Just WorksThere is no action that you need to take to enable this behavior, and on first start of RavenDB 7.1, it will immediately release any free space already in the data files. The work was actually completed and merged in August 2024, but it is going to be released sometime in Q2/Q3 of 2025. You might have noticed that there have been a lot&#xA0;of low-level changes targeted at RavenDB 7.1. We need to run them through the wringer to make sure that everything works as it should.I&#x2019;m looking forward to seeing this in action, there are some really nice indications about the sort of results we can expect. I&#x2019;ll talk about that in more detail in another post, this one is getting long enough.</p>
        </article>
        <article id="article-238">
            <a href="https://devblogs.microsoft.com/dotnet/enhancing-razor-productivity-with-new-features/" target="_blank">
                <h2 class="title mb-6" id="article-238">New Features for Enhanced Razor Productivity!</h2>
            </a>
            <p class="mb-2">by Leslie Richardson</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: February 13, 2025
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">The Extract to Component refactoring and the Roslyn tokenizer are two new features designed to help improve your productivity in Razor files.</p>
        </article>
        <article id="article-239">
            <a href="https://ayende.com/blog/201959-A/ravendb-7-1-write-modes" target="_blank">
                <h2 class="title mb-6" id="article-239">RavenDB 7.1</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: February 12, 2025
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">In the previous post, I talked about a massive amount of effort (2&#x2B; months of work) and about 25,000 lines of code changes. The only purpose of this task was to remove two locks from the system. During high load, we spent huge&#xA0;amounts of time contending for these locks, so removing them was well worth the effort.During this work, I essentially found myself in the guts of Voron (RavenDB&#x2019;s storage engine) and mostly dealing with old&#xA0;code. I&#x2019;m talking about code that was written between 10 and 15 years ago. I wrote a blog post about it at the time. Working with old code is an interesting experience, especially since most of this code was written by me. I can remember some of my thoughts from the time I wrote it.Old code is working code, and old code is also something that was built upon. Other parts of the codebase are making assumptions about the way the system behaves. And the more time a piece of code doesn&#x27;t change, the more likely its behavior is going to ossify. Changing old code is hard because of the many ways that such dependencies can express themselves. I dug through all of this decade-plus old code and I realized something pretty horrible. It turns out that I made a mistake&#xA0;in understanding how Windows implements buffering for memory-mapped files. I realized my mistake around mid-2024, see the related post for theactual details.The TLDR summary, however, is that when using unbuffered&#xA0;file I/O with memory-mapped files on Windows, you cannot expect the mapped memory to reflect the data written using the file I/O API. Windows calls it coherence, and it was quite confusing when I first realized what the problem was. It turns out that this applies only to unbuffered I/O and there is no such problem with buffered I/O. The scenario I needed to work with can&#xA0;use buffered I/O, however, which has been a profound shock to me. Large&#xA0;portions of the architecture of Voron are actually shaped by this limitation. Because I thought that you couldn&#x2019;t use both file I/O and memory-mapped files at the same time in Windows and get a consistent view of the data (the documentation literally says that, I have to add), RavenDB used memory-mapped I/O to write to the data file. That is&#xA0;a&#xA0;choice, certainly, but not one that I particularly liked. It was just that I was able to make things work and move on to other things.This is another tale of accidental complexity, actually. I had a problem and found a solution to it, which at the time I considered quite clever. Because I had a solution, I never tried to dig any deeper into it and figure out whether this is the only solution.This choice of using only memory-mapped I/O to write to the data file had consequences. In particular, it meant that:We had to map the data using read-write mode.There was no simple way to get an error if a write failed - since we just copied the data to memory, there was no actual write&#xA0;to fail. An error to write to disk would show up as a memory access violation (segmentation fault!) or just not show up at all.Writing to a page that isn&#x2019;t in memory may require us to read it first (even if we are overwriting all of it).I accepted those limitations because I thought that this was the only way to do things. When I realized that I was wrong, that opened up so many possibilities. As far as the refactoring work, the way Voron did things changed significantly. We are now mapping the data file as read-only and writing to it using file I/O.That means we have a known point of failure&#xA0;if we fail to write. That probably deserves some explanation. Failure to write to the disk can come in a bunch of ways. In particular, successfully writing to a file is not&#xA0;enough to safely store data, you also need to sync&#xA0;the file before you can be assured that the data is safe. The key here is that write &#x2B; sync ensures that you&#x2019;ll know that this either succeeded or failed. Here is the old way&#xA0;we were writing&#xA0;to the data file. Conceptually, this looks like this:auto mem = EnsureFileSize(pagesToWrite[pagesToWriteLength - 1].EndPosition);&#xD;&#xA;for(auto i = 0; i &lt; pagesToWriteLength; i&#x2B;&#x2B;)&#xD;&#xA;{&#xD;&#xA;    auto path = pagesToWrite[i];&#xD;&#xA;    memcpy(mem &#x2B; page.Number * 8192, page.Buffer, page.Length);    &#xD;&#xA;}&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;// some later time&#xD;&#xA;if(FAILED(msync(mem))&#xD;&#xA;   return SYNC_FAILURE;And here is the first iteration of using the file I/O API for writes.fallocate_if_needed(pagesToWrite[pagesToWriteLength - 1].EndPosition);&#xD;&#xA;for(auto i = 0; i &lt; pagesToWriteLength; i&#x2B;&#x2B;)&#xD;&#xA;{&#xD;&#xA;    auto path = pagesToWrite[i];&#xD;&#xA;    if(FAILED(pwrite(page.Number * 8192, page.Buffer, page.Length)))&#xD;&#xA;         return WRITE_FAILURE;&#xD;&#xA;}&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;// some time later&#xD;&#xA;if (FAILED(fdatasync(file))&#xD;&#xA;   return SYNC_FAILURE;Conceptually, this is just the same, but notice that we respond immediately to write failures here. When we started testing this feature, we realized something really interesting. The new version was much&#xA0;slower than the previous one, and it also generated a lot&#xA0;more disk writes.I moved mountains for this?Sometimes you get a deep sense of frustration when you look at benchmark results. The amount of work invested in this change is&#x2026; pretty high. And from an architectural point of view, I&#x2019;m loving&#xA0;it. The code is simpler, more robust, and allows us to cleanly do a lot&#xA0;more than we used to be able to.The code also should&#xA0;be much faster, but it wasn&#x2019;t. And given that performance is a critical aspect of RavenDB, that may cause us to scrap the whole thing. Looking more deeply into the issue, it turned out that my statement about old code and the state of the system was spot on. Take a look at the two code snippets above and consider how they look from the point of view of the operating system. In the case of the memcpy()&#xA0;version, there is a strong likelihood that the kernel isn&#x2019;t even involved (the pages are already paged in), and the only work done here is marking them as dirty (done by the CPU directly).That means that the OS will figure out that it has stuff to write to the disk either when we call msync() or when its own timer is called. On the other hand, when we call pwrite(), we involve the OS at every stage of the process, making it far more likely that it will start the actual write to the disk earlier. That means that we are wasting batching opportunities.In other words, because we used memory-mapped writes, we (accidentally, I might add) created a situation where we tried very hard to batch those writes in memory as much as possible. Another aspect here is that we are issuing a separate&#xA0;system call for each page. That means we are paying another high price. The good thing about this is that we now have a good way to deal&#xA0;with those issues. The pwrite()&#xA0;code above was simply the first version used to test things out. Since we now have the freedom&#xA0;to run, we can use whatever file I/O we want.In particular, RavenDB 7.1 now supports the notion of write modes, with the following possible options:mmap - exactly like previous versions, uses a writable memory map and memcpy()&#xA0;to write the values to the data file.file_io - uses&#xA0;pwrite()&#xA0;to write the data, onc page at a time, as shown above.vectored_file_io - uses pwritev()&#xA0;to write the data, merging adjacent writes to reduce the number of system calls we use (Posix only, since Windows has strict limits on this capability).io_ring - uses HIORING (Windows) / IO_Uring&#xA0;(Linux) to submit the whole set of work to the kernel as a single batch of operations.RavenDB will select the appropriate mode for the system on its own, usually selecting io_ring&#xA0;for modern Linux and Windows machines, and vectored_file_io&#xA0;for Mac. You can control that using the RAVEN_VORON_WRITER_MODE environment variable, but that is provided only because we want to have an escape hatch, not something that you are meant to configure.With those changes, we are on a much&#xA0;better footing for overall performance, but we aren&#x2019;t done yet! I would love to give you the performance numbers, but we didn&#x2019;t actually run the full test suite with just these changes. And that is because we aren&#x2019;t done yet, I&#x2019;ll cover that in the next post.</p>
        </article>
        <article id="article-240">
            <a href="https://ardalis.com/when-qa-keeps-finding-bugs/" target="_blank">
                <h2 class="title mb-6" id="article-240">When QA Keeps Finding Bugs</h2>
            </a>
            <p class="mb-2">by Ardalis</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: February 12, 2025
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">A developer manager recently reached out with a concern: their new QA team member was finding too many bugs, leading to frustration among&#x2026;Keep Reading &#x2192;</p>
        </article>
        <div class="button flex justify-between">
            <a href="23.html"><span class="back arrow"></span></a>

            <a href="25.html"><span class="next arrow"></span></a>
        </div>
    </section>
</main>

<footer
    class="mt-auto flex w-full flex-col items-center justify-center gap-y-2 pb-4 pt-20 text-center align-top font-semibold text-gray-600 dark:text-gray-400 sm:flex-row sm:justify-between sm:text-xs">
    <div class="me-0 sm:me-4">
        <div class="flex flex-wrap items-end gap-x-2">
            <ul class="flex flex-1 items-center gap-x-2 sm:flex-initial">
                <li class="flex">
                    <p class="flex items-end gap-2 justify-center flex-wrap	"> Relatively General
                        .NET 2025<span
                            class="inline-block">&nbsp;&nbsp;Theme: Astro Cactus</span>

                        <a class="inline-block sm:hover:text-link" href="https://github.com/chrismwilliams/astro-cactus"
                           rel="noopener noreferrer " target="_blank">
                            <svg width="1em" height="1em" viewBox="0 0 24 24" aria-hidden="true" class="h-6 w-6"
                                 focusable="false" data-icon="mdi:github">
                                <symbol id="ai:mdi:github">
                                    <path fill="currentColor"
                                          d="M12 2A10 10 0 0 0 2 12c0 4.42 2.87 8.17 6.84 9.5c.5.08.66-.23.66-.5v-1.69c-2.77.6-3.36-1.34-3.36-1.34c-.46-1.16-1.11-1.47-1.11-1.47c-.91-.62.07-.6.07-.6c1 .07 1.53 1.03 1.53 1.03c.87 1.52 2.34 1.07 2.91.83c.09-.65.35-1.09.63-1.34c-2.22-.25-4.55-1.11-4.55-4.92c0-1.11.38-2 1.03-2.71c-.1-.25-.45-1.29.1-2.64c0 0 .84-.27 2.75 1.02c.79-.22 1.65-.33 2.5-.33s1.71.11 2.5.33c1.91-1.29 2.75-1.02 2.75-1.02c.55 1.35.2 2.39.1 2.64c.65.71 1.03 1.6 1.03 2.71c0 3.82-2.34 4.66-4.57 4.91c.36.31.69.92.69 1.85V21c0 .27.16.59.67.5C19.14 20.16 22 16.42 22 12A10 10 0 0 0 12 2"></path>
                                </symbol>
                                <use xlink:href="#ai:mdi:github"></use>
                            </svg>
                            <span class="sr-only">Github</span>
                        </a>
                    </p>
                </li>
            </ul>
        </div>
    </div>
    <nav aria-label="More on this site" class="flex gap-x-2 sm:gap-x-0 sm:divide-x sm:divide-gray-500">
        <a class="px-4 py-2 sm:py-0 sm:hover:text-textColor sm:hover:underline" href="index.html"> Home </a><a
            class="px-4 py-2 sm:py-0 sm:hover:text-textColor sm:hover:underline" href="about.html"> About </a>
    </nav>
</footer>
<script src="js/script.js?id=af8f4559935e7bf5bf6015373793411d"></script>
<script src="/pagefind/pagefind-ui.js"></script>

<!-- Cookie Consent Banner -->
<div class="cookie-consent" id="cookieConsent">
    <div>
        <p class="text-sm">We use cookies to analyze our website traffic and provide a better browsing experience. By
            continuing to use our site, you agree to our use of cookies.</p>
    </div>
    <div class="cookie-consent-buttons">
        <button class="cookie-consent-decline" onclick="declineCookies()">Decline</button>
        <button class="cookie-consent-accept" onclick="acceptCookies()">Accept</button>
    </div>
</div>

<script>
    // Cookie consent management
    function showCookieConsent() {
        const consent = localStorage.getItem('cookieConsent');
        if (!consent) {
            document.getElementById('cookieConsent').classList.add('show');
        }
    }

    function acceptCookies() {
        localStorage.setItem('cookieConsent', 'accepted');
        document.getElementById('cookieConsent').classList.remove('show');
        loadGA(); // Load Google Analytics after consent
    }

    function declineCookies() {
        localStorage.setItem('cookieConsent', 'declined');
        document.getElementById('cookieConsent').classList.remove('show');
    }

    // Show the consent banner only for EU visitors (you can add more country codes as needed)
    fetch('https://ipapi.co/json/')
            .then(response => response.json())
            .then(data => {
                const euCountries = ['AT', 'BE', 'BG', 'HR', 'CY', 'CZ', 'DK', 'EE', 'FI', 'FR', 'DE', 'GR', 'HU', 'IE', 'IT', 'LV', 'LT', 'LU', 'MT', 'NL', 'PL', 'PT', 'RO', 'SK', 'SI', 'ES', 'SE'];
                if (euCountries.includes(data.country_code)) {
                    showCookieConsent();
                } else {
                    // For non-EU visitors, automatically load GA
                    if (!localStorage.getItem('cookieConsent')) {
                        localStorage.setItem('cookieConsent', 'accepted');
                        loadGA();
                    }
                }
            })
            .catch(() => {
                // If we can't determine location, show the consent banner to be safe
                showCookieConsent();
            });
</script>
</body>
</html>
