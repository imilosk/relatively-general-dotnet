
<!DOCTYPE html>
<html class="scroll-smooth" lang="en-US" data-theme="light">
<head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, shrink-to-fit=no" name="viewport">
    <meta content="IE=edge" http-equiv="X-UA-Compatible">
    <title>Page 1 â€¢ Relatively General .NET</title>
    <link href="favicon.ico" rel="icon" sizes="any">
    <link href="images/apple-touch-icon.png" rel="apple-touch-icon">
    <meta content="hsl()" name="theme-color">
    <meta content="website" property="og:type">
    <meta content="Home" property="og:title">
    <link rel="stylesheet" href="css/styles.css">
    <link rel="stylesheet" href="pagefind/pagefind-ui.css">
</head>
<body class="mx-auto flex min-h-screen max-w-3xl flex-col bg-bgColor px-4 pt-16 font-mono text-sm font-normal text-textColor antialiased sm:px-8">

<a class="sr-only focus:not-sr-only focus:fixed focus:start-1 focus:top-1.5" href="#main">
    skip to content
</a>
<header class="group relative mb-28 flex items-center sm:ps-[4.5rem]" id="main-header">
    <div class="flex sm:flex-col">
        <a aria-current="page" class="inline-flex items-center hover:filter-none sm:relative sm:inline-block"
           href="index.html">
            <img class="me-3 sm:absolute sm:start-[-4.5rem] sm:me-0 sm:h-16 sm:w-16 w-16" src="images/giphy.gif"
                 alt=""/>
            <span class="text-xl font-bold sm:text-2xl">Relatively General .NET</span>
        </a>
        <nav aria-label="Main menu"
             class="absolute -inset-x-4 top-14 hidden flex-col items-end gap-y-4 rounded-md bg-bgColor/[.85] py-4 text-accent shadow backdrop-blur group-[.menu-open]:z-50 group-[.menu-open]:flex sm:static sm:z-auto sm:-ms-4 sm:mt-1 sm:flex sm:flex-row sm:items-center sm:divide-x sm:divide-dashed sm:divide-accent sm:rounded-none sm:bg-transparent sm:py-0 sm:shadow-none sm:backdrop-blur-none"
             id="navigation-menu">
            <a aria-current="page" class="px-4 py-4 underline-offset-2 sm:py-0 sm:hover:underline"
               href="index.html"> Home </a><a
                class="px-4 py-4 underline-offset-2 sm:py-0 sm:hover:underline" href="/about/">
                About </a>
        </nav>
    </div>
    <site-search class="ms-auto" id="search">
        <button id="open-search"
                class="flex h-9 w-9 items-center justify-center rounded-md ring-zinc-400 transition-all hover:ring-2"
                data-open-modal="">
            <svg aria-label="search" class="h-7 w-7" fill="none" height="16" stroke="currentColor"
                 stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 24 24" width="16"
                 xmlns="http://www.w3.org/2000/svg">
                <path d="M0 0h24v24H0z" stroke="none"></path>
                <path d="M3 10a7 7 0 1 0 14 0 7 7 0 1 0-14 0M21 21l-6-6"></path>
            </svg>
        </button>
        <dialog aria-label="search"
                class="h-full max-h-full w-full max-w-full border border-zinc-400 bg-bgColor shadow backdrop:backdrop-blur sm:mx-auto sm:mb-auto sm:mt-16 sm:h-max sm:max-h-[calc(100%-8rem)] sm:min-h-[15rem] sm:w-5/6 sm:max-w-[48rem] sm:rounded-md">
            <div class="dialog-frame flex flex-col gap-4 p-6 pt-12 sm:pt-6">
                <button id="close-search"
                        class="ms-auto cursor-pointer rounded-md bg-zinc-200 p-2 font-semibold dark:bg-zinc-700"
                        data-close-modal="">Close
                </button>
                <div class="search-container">
                    <div id="cactus__search"/>
                </div>
            </div>
        </dialog>
    </site-search>
    <theme-toggle class="ms-2 sm:ms-4">
        <button id="theme-toggle" class="relative h-9 w-9 rounded-md p-2 ring-zinc-400 transition-all hover:ring-2"
                type="button">
            <span class="sr-only">Dark Theme</span>
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-7 w-7 -translate-x-1/2 -translate-y-1/2 scale-100 opacity-100 transition-all dark:scale-0 dark:opacity-0"
                 fill="none" focusable="false" id="sun-svg" stroke-width="1.5" viewBox="0 0 24 24"
                 xmlns="http://www.w3.org/2000/svg">
                <path
                    d="M12 18C15.3137 18 18 15.3137 18 12C18 8.68629 15.3137 6 12 6C8.68629 6 6 8.68629 6 12C6 15.3137 8.68629 18 12 18Z"
                    stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M22 12L23 12" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M12 2V1" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M12 23V22" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M20 20L19 19" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M20 4L19 5" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M4 20L5 19" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M4 4L5 5" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M1 12L2 12" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
            </svg>
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-7 w-7 -translate-x-1/2 -translate-y-1/2 scale-0 opacity-0 transition-all dark:scale-100 dark:opacity-100"
                 fill="none" focusable="false" id="moon-svg" stroke="currentColor" stroke-width="1.5"
                 viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M0 0h24v24H0z" fill="none" stroke="none"></path>
                <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z"></path>
                <path d="M17 4a2 2 0 0 0 2 2a2 2 0 0 0 -2 2a2 2 0 0 0 -2 -2a2 2 0 0 0 2 -2"></path>
                <path d="M19 11h2m-1 -1v2"></path>
            </svg>
        </button>
    </theme-toggle>
    <mobile-button>
        <button aria-expanded="false" aria-haspopup="menu" aria-label="Open main menu"
                class="group relative ms-4 h-7 w-7 sm:invisible sm:hidden" id="toggle-navigation-menu" type="button">
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-full w-full -translate-x-1/2 -translate-y-1/2 transition-all group-aria-expanded:scale-0 group-aria-expanded:opacity-0"
                 fill="none" focusable="false" id="line-svg" stroke="currentColor" stroke-width="1.5"
                 viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M3.75 9h16.5m-16.5 6.75h16.5" stroke-linecap="round" stroke-linejoin="round"></path>
            </svg>
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-full w-full -translate-x-1/2 -translate-y-1/2 scale-0 text-accent opacity-0 transition-all group-aria-expanded:scale-100 group-aria-expanded:opacity-100"
                 fill="none" focusable="false" id="cross-svg" stroke="currentColor" stroke-width="1.5"
                 viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M6 18L18 6M6 6l12 12" stroke-linecap="round" stroke-linejoin="round"></path>
            </svg>
        </button>
    </mobile-button>
</header>
<main id="main" data-pagefind-body>
    <section aria-label="Blog post list">
        <article id="article-1">
            <a href="https://ayende.com/blog/201891-B/the-memory-leak-in-concurrentqueue" target="_blank">
                <h2 class="title mb-6" id="article-1">The memory leak in ConcurrentQueue</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: January 13, 2025
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">We ran into a memory issue recently in RavenDB, which had a pretty interesting root cause. Take a look at the following code and see if you can spot what is going on:ConcurrentQueue&lt;Buffer&gt; _buffers = new();&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;void FlushUntil(long maxTransactionId)&#xD;&#xA;{&#xD;&#xA;    List&lt;Buffer&gt; toFlush = new();&#xD;&#xA;    while(_buffers.TryPeek(out buffer) &amp;&amp; &#xD;&#xA;        buffer.TransactionId &lt;= maxTransactionId)&#xD;&#xA;    {&#xD;&#xA;        if(_buffers.TryDequeue(out buffer))&#xD;&#xA;        {&#xD;&#xA;            toFlush.Add(buffer);&#xD;&#xA;        }&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    FlushToDisk(toFlush);&#xD;&#xA;}The code handles flushing data to disk based on the maximum transaction ID. Can you see the memory leak?If we have a lot of load on the system, this will run just fine. The problem is when the load is over. If we stop&#xA0;writing new items to the system, it will keep a lot&#xA0;of stuff in memory, even though there is no reason for it to do so. The reason for that is the call to TryPeek(). You can read the source directly, but the basic idea is that when you peek, you have to guard against concurrent TryTake(). If you are not careful, you may encounter something called a torn read.Let&#x2019;s try to explain it in detail. Suppose we store a large struct in the queue and call TryPeek() and TryTake()&#xA0;concurrently. The TryPeek() starts copying the struct to the caller at the same time that TryTake() does the same and zeros the value. So it is possible that TryPeek() would get an invalid value. To handle that, if you are using TryPeek(), the queue will not&#xA0;zero out the values. This means that until that queue segment is completely full and a new one is generated, we&#x2019;ll retain references to those buffers, leading to an interesting&#xA0;memory leak.</p>
        </article>
        <article id="article-2">
            <a href="https://www.meziantou.net/hsts-for-httpclient-in-dotnet.htm" target="_blank">
                <h2 class="title mb-6" id="article-2">Enhance Your .NET HttpClient with HSTS Support</h2>
            </a>
            <p class="mb-2">by G&#xE9;rald Barr&#xE9;</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: January 13, 2025
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">HTTP Strict Transport Security (HSTS) is a security feature that indicates a client to only connect to a website over HTTPS. Websites can set the Strict-Transport-Security header to inform the client to always use HTTPS. ASP.NET Core can easily set the header, but there is no built-in feature to en</p>
        </article>
        <article id="article-3">
            <a href="https://ayende.com/blog/201862-C/performance-discovery-iops-vs-iops" target="_blank">
                <h2 class="title mb-6" id="article-3">Performance discovery</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: January 10, 2025
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">RavenDB is a transactional database, we care&#xA0;deeply about ACID. The D in ACID stands for durability, which means that to acknowledge a transaction, we must&#xA0;write it to a persistent medium. Writing to disk is expensive, writing to the disk and ensuring durability is even more expensive.After seeing some weird performance numbers on a test machine, I decided to run an experiment to understand exactly how durable writes affect disk performance.A few words about the term durable writes. Disks are slow, so we use buffering &amp; caches to avoid going to the disk. But a write to a buffer isn&#x2019;t durable. A failure could cause it to never hit a persistent medium. So we need to tell the disk in some way that we are willing to wait until it can ensure that this write is actually durable. This is typically done using either fsync or&#xA0;O_DIRECT | O_DSYNC&#xA0;flags. So this is what we are testing in this post.I wanted to test things out without any of my own code, so I ran the following benchmark. I pre-allocated a file and then ran the following commands.Normal writes (buffered) with different sizes (256 KB, 512 KB, etc). dd if=/dev/zero of=/data/test bs=256K count=1024&#xD;&#xA;dd if=/dev/zero of=/data/test bs=512K count=1024Durable writes (force the disk to acknowledge them) with different sizes:dd if=/dev/zero of=/data/test bs=256k count=1024 oflag=direct,sync&#xD;&#xA;dd if=/dev/zero of=/data/test bs=256k count=1024 oflag=direct,syncThe code above opens the file using:openat(AT_FDCWD, &quot;/data/test&quot;, O_WRONLY|O_CREAT|O_TRUNC|O_SYNC|O_DIRECT, 0666) = 3I got myself an i4i.xlarge&#xA0;instance on AWS and started running some tests. That machine has a local NVMe drive of about 858 GB, 32 GB of RAM, and 4 cores. Let&#x2019;s see what kind of performance I can get out of it.Write sizeTotal writesBuffered writes256 KB256 MB1.3 GB/s512 KB512 MB1.2 GB/s1 MB1 GB1.2 GB/s2 MB2 GB731 Mb/s8 MB8 GB571 MB/s16 MB16 GB561 MB/s2 MB8 GB559 MB/s1 MB1 GB554 MB/s4 KB16 GB557 MB/s16 KB16 GB553 MB/sWhat you can see here is that writes are really&#xA0;fast when buffered. But when I hit a certain size (above 1 GB or so), we probably start having to write to the disk itself (which is NVMe, remember). Our top speed is about 550 MB/s at this point, regardless of the size of the buffers I&#x2019;m passing to the write()&#xA0;syscall. I&#x2019;m writing here using cached I/O, which is something that as a database vendor, I don&#x2019;t really care about. What happens when we run with direct &amp; sync I/O, the way I would with a real database? Here are the numbers for the i4i.xlarge&#xA0;instance for durable writes.Write sizeTotal writesDurable writes256 KB256 MB1.3 GB/s256 KB1 GB1.1 GB/s16 MB16 GB584 GB/s64 KB16 GB394 MB/s32 KB16 GB237 MB/s16 KB16 GB126 MB/sIn other words, when using direct I/O, the smaller the write, the more time it takes. Remember that we are talking about forcing the disk to write the data, and we need to wait for it to complete before moving to the next one. For 16 KB writes, buffered writes achieve a throughput of 553 MB/s vs. 126 MB/s for durable writes. This makes sense, since those writes are cached, so the OS is probably sending big batches to the disk. The numbers we have here clearly show that bigger batches are better.My next test was to see what would happen when I try to write things in parallel. In this test, we run 4 processes that write to the disk using direct I/O and measure their output. I assume that I&#x2019;m maxing out the throughput on the drive, so the total rate across all commands should be equivalent to the rate I would get from a single command. To run this in parallel I&#x2019;m using a really simple mechanism - just spawn processes that would do the same work. Here is the command template I&#x2019;m using:parallel -j 4 --tagstring &#x27;Task {}&#x27; dd if=/dev/zero of=/data/test bs=16M count=128 seek={} oflag=direct,sync ::: 0 1024 2048 3072This would write to 4 different portions of the same file, but I also tested that on separate files. The idea is to generate a sufficient volume of writes to stress the disk drive.Write sizeTotal writesDurable &amp; Parallel writes16 MB8 GB650 MB/s16 KB64 GB252 MB/sI also decided to write some low-level C code to test out how this works with threads and a single program. You can find the code here. &#xA0;I basically spawn NUM_THREADS threads, and each will open a file using O_SYNC | O_DIRECT&#xA0;and write to the file WRITE_COUNT times with a buffer of size BUFFER_SIZE.This code just opens a lot of files and tries to write to them using direct I/O with 8 KB buffers. In total, I&#x2019;m writing 16 GB (128 MB x 128 threads) to the disk. I&#x2019;m getting a rate of about 320 MB/sec when using this approach.As before, increasing the buffer size seems to help here. I also tested a version where we write using buffered I/O and call fsync every now and then, but I got similar results. The interim conclusion that I can draw from this experiment is that NVMes are pretty cool, but once you hit their limits you can really feel it. There is another aspect to consider though, I&#x2019;m running this on a disk that is literally called ephemeral storage. I need to repeat those tests on real hardware to verify whether the cloud disk simply ignores the command to persist properly and always uses the cache.That is supported by the fact that using both direct I/O on small data sizes didn&#x2019;t have a big impact (and I expected it should). Given that the point of direct I/O in this case is to force the disk to properly persist (so it would be durable in the case of a crash), while at the same time an ephemeral disk is wiped if the host machine is restarted, that gives me good reason to believe that these numbers are because the hardware &#x201C;lies&#x201D; to me.In fact, if I were in charge of those disks, lying about the durability of writes would be the first thing I would do. Those disks are local to the host machine, so we have two failure modes that we need to consider:The VM crashed - in which case the disk is perfectly fine and &#x201C;durable&#x201D;.The host crashed - in which case the disk is considered lost entirely.Therefore, there is no point&#xA0;in trying to achieve durability, so we can&#x2019;t&#xA0;trust those numbers. The next step is to run it on a real&#xA0;machine. The economics of benchmarks on cloud instances are weird. For a one-off scenario, the cloud is a godsend. But if you want to run benchmarks on a regular basis, it is far&#xA0;more economical to just buy a physical machine. Within a month or two, you&#x2019;ll already see a return on the money spent. We got a machine in the office called Kaiju (a Japanese term for enormous monsters, think: Godzilla) that has:32 cores188 GB RAM2 TB NVMe for the system disk4 TB NVMe for the data diskI ran the same commands on that machine as well and got really interesting results.Write sizeTotal writesBuffered writes4 KB16 GB1.4 GB/s256 KB256 MB1.4 GB/s2 MB2 GB1.6 GB/s2 MB16 GB1.7 GB/s4 MB32 GB1.8 GB/s4 MB64 GB1.8 GB/sWe are faster than the cloud instance, and we don&#x2019;t have a drop-off point when we hit a certain size. We are also&#xA0;seeing higher performance when we throw bigger buffers at the system. But when we test with small buffers, the performance is also great. That is amazing, but what about durable writes with direct I/O?I tested the same scenario with both buffered and durable writes:ModeBufferedDurable1 MB buffers, 8 GB write1.6 GB/s1.0 GB/s2 MB buffers, 16 GB write1.7 GB/s1.7 GB/sWow, that is an interesting result. Because it means that when we use direct I/O with 1 MB buffers, we lose&#xA0;about 600 MB/sec compared to buffered I/O. Note that this is actually a pretty good result. 1 GB/sec is amazing.And if you use big buffers, then the cost of direct I/O is basically gone. What about when we go the other way around and use smaller buffers?ModeBufferedDurable128 KB buffers, 8 GB write1.7 GB/s169 MB/s32 KB buffers, 2 GB1.6 GB/s49.9 MB/sParallel:&#xA0;8, 1 MB, 8 GB5.8 GB/s3.6 GB/sParallel: 8, 128 KB, 8 GB6.0 GB/s550 MB/sFor buffered I/O - I&#x2019;m getting simply dreamy numbers, pretty much regardless of what I do &#x1F642;. For durable writes, the situation is clear. The bigger the buffer we write, the better we perform, and we pay&#xA0;for small buffers. Look at the numbers for 128 KB in the durable column for both single-threaded and parallel scenarios.169 MB/s in the single-threaded result, but with 8 parallel processes, we didn&#x2019;t reach 1.3 GB/s (which is 169x8). Instead, we achieved less than half of our expected performance. It looks like there is a fixed cost for making a direct I/O write to the disk, regardless of the amount of data that we write. &#xA0;When using 32 KB writes, we are not even breaking into the 200 MB/sec. And with 8 KB writes, we are barely breaking into the 50 MB/sec range. Those are some really&#xA0;interesting results because they show a very strong preference for bigger writes over smaller writes. I also tried using the same C code as before. As a reminder, we use direct I/O to write to 128 files in batches of 8 KB, writing a total of 128 MB per file. All of that is done concurrently to really stress the system. When running iotop in&#xA0;this environment, we get:Total DISK READ:         0.00 B/s | Total DISK WRITE:       522.56 M/s&#xD;&#xA;Current DISK READ:       0.00 B/s | Current DISK WRITE:     567.13 M/s&#xD;&#xA;    TID  PRIO  USER     DISK READ DISK WRITE&gt;    COMMAND&#xD;&#xA; 142851 be/4 kaiju-1     0.00 B/s    4.09 M/s ./a.out&#xD;&#xA; 142901 be/4 kaiju-1     0.00 B/s    4.09 M/s ./a.out&#xD;&#xA; 142902 be/4 kaiju-1     0.00 B/s    4.09 M/s ./a.out&#xD;&#xA; 142903 be/4 kaiju-1     0.00 B/s    4.09 M/s ./a.out&#xD;&#xA; 142904 be/4 kaiju-1     0.00 B/s    4.09 M/s ./a.out&#xD;&#xA;... redacted ...So each thread is getting about 4.09 MB/sec for writes, but we total 522 MB/sec across all writes. I wondered what would happen if I limited it to fewer threads, so I tried with 16 concurrent threads, resulting in:Total DISK READ:         0.00 B/s | Total DISK WRITE:        89.80 M/s&#xD;&#xA;Current DISK READ:       0.00 B/s | Current DISK WRITE:     110.91 M/s&#xD;&#xA;    TID  PRIO  USER     DISK READ DISK WRITE&gt;    COMMAND&#xD;&#xA; 142996 be/4 kaiju-1     0.00 B/s    5.65 M/s ./a.out&#xD;&#xA; 143004 be/4 kaiju-1     0.00 B/s    5.62 M/s ./a.out&#xD;&#xA; 142989 be/4 kaiju-1     0.00 B/s    5.62 M/s ./a.out&#xD;&#xA;... redacted ..Here we can see that each thread is getting (slightly) more throughput, but the overall system throughput is greatly reduced. To give some context, with 128 threads running, the process wrote 16GB in 31 seconds, but with 16 threads, it took 181 seconds to write the same amount. In other words, there is a throughput issue here. I also tested this with various levels of concurrency:Concurrency(8 KB x 16K times - 128 MB)Throughput per threadTime / MB written115.5 MB / sec8.23 seconds / 128 MB25.95 MB / sec18.14 seconds / 256 MB45.95 MB / sec20.75 seconds / 512 MB86.55 MB / sec20.59 seconds / 1024 MB165.70 MB / sec22.67 seconds / 2048 MBTo give some context, here are two attempts to write 2GB to the disk:ConcurrencyWriteThroughputTotal writtenTotal time16128 MB in 8 KB writes5.7 MB / sec2,048 MB22.67 sec8256 MB in 16 KB writes12.6 MB / sec2,048 MB22.53 sec16256 MB in 16 KB writes10.6 MB / sec4,096 MB23.92 secIn other words, we can see the impact of concurrent writes. There is absolutely some contention at the disk level when making direct I/O writes. The impact is related to the number&#xA0;of writes rather than the amount of data being written.Bigger writes are far more efficient. And concurrent writes allow you to get more data overall but come with a horrendous latency impact for each individual thread.The difference between the cloud and physical instances is really interesting, and I have to assume that this is because the cloud instance isn&#x2019;t actually forcing the data to the physical disk (it doesn&#x2019;t make sense that it would). I decided to test that on an m6i.2xlarge&#xA0;instance with a 512 GB io2 disk with 16,000 IOPS. The idea is that an io2&#xA0;disk has to be durable, so it will probably have similar behavior to physical hardware. DiskBuffer SizeWritesDurableParallelTotalRateio2&#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;256.00&#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;1,024.00&#xA0;No &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;1.00&#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;256.00&#xA0; &#xA0;1,638.40io2&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;2,048.00&#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;1,024.00&#xA0;No &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;1.00&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;2,048.00&#xA0; &#xA0;1,331.20io2&#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;4.00&#xA0; &#xA0;4,194,304.00&#xA0;No &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;1.00&#xA0; &#xA0;16,384.00&#xA0; &#xA0;1,228.80io2&#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;256.00&#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;1,024.00&#xA0;Yes &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;1.00&#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;256.00&#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;144.00io2&#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;256.00&#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;4,096.00&#xA0;Yes &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;1.00&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;1,024.00&#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;146.00io2&#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;64.00&#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;8,192.00&#xA0;Yes &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;1.00&#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;512.00&#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;50.20io2&#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;32.00&#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;8,192.00&#xA0;Yes &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;1.00&#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;256.00&#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;26.90io2&#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;8.00&#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;8,192.00&#xA0;Yes &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;1.00&#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;64.00&#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;7.10io2&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;1,024.00&#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;8,192.00&#xA0;Yes &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;1.00&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;8,192.00&#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;502.00io2&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;1,024.00&#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;2,048.00&#xA0;No &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;8.00&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;2,048.00&#xA0; &#xA0;1,909.00io2&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;1,024.00&#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;2,048.00&#xA0;Yes &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;8.00&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;2,048.00&#xA0; &#xA0;1,832.00io2&#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;32.00&#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;8,192.00&#xA0;No &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;8.00&#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;256.00&#xA0; &#xA0;3,526.00io2&#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;32.00&#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;8,192.00&#xA0;Yes &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;8.00&#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;256.00150.9io2&#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;8.00&#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;8,192.00&#xA0;Yes &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;8.00&#xA0; &#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;64.00&#xA0; &#xA0; &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;37.10In other words, we are seeing pretty much the same behavior as on the physical machine, unlike the ephemeral drive. In conclusion, it looks like the limiting factor for direct I/O writes is the number of writes, not their size. There appears to be some benefit for concurrency in this case, but there is also some contention. The best option we got was with big writes.Interestingly, big writes are a win, period. For example, 16 MB writes, direct I/O:Single-threaded - 4.4 GB/sec2 threads - 2.5 GB/sec X 2 - total 5.0 GB/sec4 threads - 1.4 X 4 &#xA0;- total 5.6 GB/sec8 threads - ~590 MB/sec x 8 - total 4.6 GB/secWriting 16 KB, on the other hand:8 threads - 11.8 MB/sec x 8 - total 93 MB/sec4 threads - 12.6 MB/sec x 4- total 50.4 MB/sec2 threads - 12.3 MB/sec x 2 - total 24.6 MB/sec1 thread - 23.4 MB/secThis leads me to believe that there is a bottleneck somewhere in the stack, where we need to handle the durable write, but it isn&#x2019;t related to the actual amount we write. In short, fewer and bigger writes are more effective, even with concurrency. As a database developer, that leads to some interesting questions about design. It means that I want to find some way to batch more writes to the disk, especially for durable writes, because it matters so much.Expect to hear more about this in the future.</p>
        </article>
        <article id="article-4">
            <a href="https://ayende.com/blog/201861-C/aggregating-trees-with-ravendb" target="_blank">
                <h2 class="title mb-6" id="article-4">Aggregating trees with RavenDB</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: January 07, 2025
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">We got an interesting question in the RavenDB discussion group:How to do aggregation on a tree structure?The task is to build a Work Breakdown Structure, where you have:ProjectsMajor deliverablesSub-deliverablesWork packagesThe idea is to be able to track EstimatedHours and CompletedHours across the entire tree. For example, let&#x2019;s say that I have the following:Project: Bee Keeper Chronicle AppMajor deliverable: App DesignSub-deliverable: Wireframes all screensWork Package: Login page wireframeUsers can add the EstimatedHours and CompletedHours at any level, and we want to be able to aggregate the data upward. So the Project: &#x201C;Bee Keeper Chronicle App&#x201D; should have a total estimated time and the number of hours that were worked on.The question is how to model &amp; track that in RavenDB. Here is what I think the document structure should look like:{&#xD;&#xA;    &quot;Name&quot;: &quot;Login page wire frame&quot;,&#xD;&#xA;    &quot;Parent&quot;: {&#xD;&#xA;        &quot;Type&quot;: &quot;Subs&quot;,&#xD;&#xA;        &quot;Id&quot;: &quot;subs/0000000000000000009-A&quot;&#xD;&#xA;    },&#xD;&#xA;    &quot;EsimatedHours&quot;: 8,&#xD;&#xA;    &quot;CompletedHours&quot;: 3,&#xD;&#xA;    &quot;@metadata&quot;: {&#xD;&#xA;        &quot;@collection&quot;: &quot;WorkPackages&quot;&#xD;&#xA;    }&#xD;&#xA;}&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;{&#xD;&#xA;    &quot;Name&quot;: &quot;Wire frames all screens&quot;,&#xD;&#xA;    &quot;Parent&quot;: {&#xD;&#xA;        &quot;Type&quot;: &quot;Majors&quot;,&#xD;&#xA;        &quot;Id&quot;: &quot;major/0000000000000000008-A&quot;&#xD;&#xA;    },&#xD;&#xA;    &quot;EsimatedHours&quot;: 20,&#xD;&#xA;    &quot;CompletedHours&quot;: 7,&#xD;&#xA;    &quot;@metadata&quot;: {&#xD;&#xA;        &quot;@collection&quot;: &quot;Subs&quot;&#xD;&#xA;    }&#xD;&#xA;}&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;{&#xD;&#xA;    &quot;Name&quot;: &quot;App Design&quot;,&#xD;&#xA;    &quot;Parent&quot;: {&#xD;&#xA;        &quot;Type&quot;: &quot;Projects&quot;,&#xD;&#xA;        &quot;Id&quot;: &quot;projects/0000000000000000011-A&quot;&#xD;&#xA;    },&#xD;&#xA;    &quot;EsimatedHours&quot;: 50,&#xD;&#xA;    &quot;CompletedHours&quot;: 12,&#xD;&#xA;    &quot;@metadata&quot;: {&#xD;&#xA;        &quot;@collection&quot;: &quot;Majors&quot;&#xD;&#xA;    }&#xD;&#xA;}&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;{&#xD;&#xA;    &quot;Name&quot;: &quot;Bee Keeper Chronicle App&quot;,&#xD;&#xA;    &quot;EsimatedHours&quot;: 34,&#xD;&#xA;    &quot;CompletedHours&quot;: 21,&#xD;&#xA;    &quot;@metadata&quot;: {&#xD;&#xA;        &quot;@collection&quot;: &quot;Projects&quot;&#xD;&#xA;    }&#xD;&#xA;}I used a Parent relationship, since that is the most flexible (it allows you to move each item to a completely different part of the tree easily). Now, we need to aggregate the values for all of those items using a Map-Reduce index. Because of the similar structure, I created the following JS function:function processWorkBreakdownHours(doc) {&#xD;&#xA;    let hours = {&#xD;&#xA;        EsimatedHours: doc.EsimatedHours,&#xD;&#xA;        CompletedHours: doc.CompletedHours&#xD;&#xA;    };&#xD;&#xA;    let results = [Object.assign({&#xD;&#xA;        Scope: id(doc)&#xD;&#xA;    }, hours)];&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    let current = doc;&#xD;&#xA;    while (current.Parent) {&#xD;&#xA;        current = load(current.Parent.Id, current.Parent.Type);&#xD;&#xA;        results.push(Object.assign({&#xD;&#xA;            Scope: id(current)&#xD;&#xA;        }, hours));&#xD;&#xA;    }&#xD;&#xA;    return results;&#xD;&#xA;}This will scan over the hierarchy and add the number of estimated and completed hours to all the levels. Now we just need to add this file as Additional Sources to an index and call it for all the relevant collections, like this:map(&quot;WorkPackages&quot;,processWorkBreakdownHours);&#xD;&#xA;map(&quot;Subs&quot;,processWorkBreakdownHours);&#xD;&#xA;map(&quot;Majors&quot;,processWorkBreakdownHours);&#xD;&#xA;map(&quot;Projects&quot;,processWorkBreakdownHours);And the last step is to aggregate across all of them in the reduce function: groupBy(x =&gt; x.Scope).aggregate(g =&gt; {&#xD;&#xA;    return {&#xD;&#xA;        Scope: g.key,&#xD;&#xA;        EsimatedHours: g.values.reduce((c, val) =&gt; val.EsimatedHours &#x2B; c, 0),&#xD;&#xA;        CompletedHours: g.values.reduce((c, val) =&gt; val.CompletedHours &#x2B; c, 0)&#xD;&#xA;    };&#xD;&#xA;})You can see the full index definition here.The end result is automatic aggregation at all levels. Change one item, and it will propagate upward.Considerations: I&#x2019;m using load()&#xA0;here, which creates a reference from the parent to the child. The idea is that if we move a Work Package from one Sub-deliverable to another (in the same or a different Major &amp; Project), this index will automatically re-index what is required and get you the right result.However, that also means that whenever the Major document changes, we&#x2019;ll have to re-index everything below it (because it might have changed the Project). There are two ways to handle that. Instead of using load(),&#xA0;we&#x2019;ll use noTracking.load(),&#xA0;which tells RavenDB that when the referenced document changes, we should not&#xA0;re-index. Provide the relevant scopes at the document level, like this:{&#xD;&#xA;    &quot;Name&quot;: &quot;Login page wire frame&quot;,&#xD;&#xA;    &quot;Scope&quot;: [&#xD;&#xA;       &quot;subs/0000000000000000009-A&quot;,&#xD;&#xA;       &quot;major/0000000000000000008-A&quot;,&#xD;&#xA;       &quot;projects/0000000000000000011-A&quot;&#xD;&#xA;    ],&#xD;&#xA;    &quot;EsimatedHours&quot;: 8,&#xD;&#xA;    &quot;CompletedHours&quot;: 3,&#xD;&#xA;    &quot;@metadata&quot;: {&#xD;&#xA;        &quot;@collection&quot;: &quot;WorkPackages&quot;&#xD;&#xA;    }&#xD;&#xA;}Note that in this case, changing the root will be more complex because you have to scan / touch everything if you move between parts of the tree. In most cases, that is such a rare event that it shouldn&#x2019;t be a consideration, but it depends largely on your context. And there you have it, a simple Map-Reduce index that can aggregate across an entire hierarchy with ease.</p>
        </article>
        <article id="article-5">
            <a href="https://devblogs.microsoft.com/dotnet/dotnet-aspire-container-lifetime/" target="_blank">
                <h2 class="title mb-6" id="article-5">.NET Aspire Quick Tip &#x2013; Managing Container &amp; Data Lifetime</h2>
            </a>
            <p class="mb-2">by James Montemagno</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: January 07, 2025
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">.NET Aspire 9 adds new control over the lifetime of containers on your local developer environment. Let&#x27;s look at how to manage them with the new APIs!</p>
        </article>
        <article id="article-6">
            <a href="https://andrewlock.net/creating-a-source-generator-part-11-implementing-an-interceptor-with-a-source-generator/" target="_blank">
                <h2 class="title mb-6" id="article-6">Implementing an interceptor with a source generator</h2>
            </a>
            <p class="mb-2">by Andrew Lock</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: January 07, 2025
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">Creating a source generator - Part 11</p>
        </article>
        <article id="article-7">
            <a href="https://www.meziantou.net/exploring-collectionsmarshal-for-dictionary.htm" target="_blank">
                <h2 class="title mb-6" id="article-7">Exploring CollectionsMarshal for Dictionary</h2>
            </a>
            <p class="mb-2">by G&#xE9;rald Barr&#xE9;</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: January 06, 2025
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">Unlike ConcurrentDictionary, Dictionary does not have a GetOrAdd method. This method is useful when you want to add a key-value pair to the dictionary if the key does not exist, or return the value if the key already exists. The naive implementation of this method looks like this:C#copypublic stati</p>
        </article>
        <article id="article-8">
            <a href="https://ayende.com/blog/201860-C/performance-discovery-managed-vs-unmanaged-memory" target="_blank">
                <h2 class="title mb-6" id="article-8">Performance discovery</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: January 03, 2025
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">When building RavenDB, we occasionally have to deal with some ridiculous numbers in both size and scale. In one of our tests, we ran into an interesting problem. Here are the performance numbers of running a particular query 3 times.First Run: 19,924 msSecond Run: 3,181 msThird Run: 1,179 msThose are not good numbers, so we dug into this to try to figure out what is going on. Here is the query that we are running:from index &#x27;IntFloatNumbers-Lucene&#x27; where Int &gt; 0And the key here is that this index covers 400 million documents, all of which are actually greater than 0. So this is actually a pretty complex task for the database to handle, mostly because of the internals of how Lucene works. Remember that we provide both&#xA0;the first page of the results as well as its total number. So we have to go through the entire result set to find out how many items we have. That is a lot of work. But it turns out that most of the time here isn&#x2019;t actually processing the query, but dealing with the GC. Here are some entries from the GC log while the queries were running:2024-12-12T12:39:40.4845987Z, Type: GC, thread id: 30096, duration: 2107.9972ms, index: 25, generation: 2, reason: Induced&#xD;&#xA;2024-12-12T12:39:53.1359744Z, Type: GC, thread id: 30096, duration: 1650.9207ms, index: 26, generation: 2, reason: Induced&#xD;&#xA;2024-12-12T12:40:07.5835527Z, Type: GC, thread id: 30096, duration: 1629.1771ms, index: 27, generation: 2, reason: Induced&#xD;&#xA;2024-12-12T12:40:20.2205602Z, Type: GC, thread id: 30096, duration: 776.24ms, index: 28, generation: 2, reason: InducedThat sound you heard was me going: Ouch!Remember that this query actually goes through 400M results. Here are the details about its Memory Usage &amp; Object Count:Number of objects for GC&#xA0;(under LuceneIndexPersistence): 190M (~12.63GB)Managed Memory: 13.01GBUnmanaged Memory: 4.53MBWhat is going on? It turns out that Lucene handles queries such as Int&gt;0&#xA0;by creating an array with all the unique values, something similar to:string[] sortedTerms = new string[190_000_000];&#xD;&#xA;long[] termPostingListOffset = new long[190_000_000];This isn&#x2019;t exactly&#xA0;how it works, mind. But the details don&#x2019;t really matter for this story. The key here is that we have an array with a sorted list of terms, and in this case, we have a lot&#xA0;of terms.Those values are cached, so they aren&#x2019;t actually allocated and thrown away each time we query. However, remember that the .NET GC uses a Mark &amp; Sweep algorithm. Here is the core part of the Mark portion of the algorithm:long _marker;&#xD;&#xA;void Mark()&#xD;&#xA;{&#xD;&#xA;    var currentMarker = &#x2B;&#x2B;_marker;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    foreach (var root in GetRoots())&#xD;&#xA;    {&#xD;&#xA;        Mark(root);&#xD;&#xA;    }&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;    void Mark(object o)&#xD;&#xA;    {&#xD;&#xA;        // already visited&#xD;&#xA;        if (GetMarket(o) == currentMarker)&#xD;&#xA;            return;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;        foreach (var child in GetReferences(node))&#xD;&#xA;        {&#xD;&#xA;            Mark(child);&#xD;&#xA;        }&#xD;&#xA;    }&#xD;&#xA;}Basically, start from the roots (static variables, items on the stack, etc.), scan the reachable object graph, and mark all the objects in use. The code above is generic, of course (and basically pseudo-code), but let&#x2019;s consider what the performance will be like when dealing with an array of 190M strings. It has to scan&#xA0;the entire thing, which means it is proportional to the number of objects. And we do have quite a lot of those. The problem was the number of managed objects, so we pulled all of those out. We moved the term storage to unmanaged memory, outside the purview of the GC. As a result, we now have the following Memory Usage &amp; Object Count:Number of objects for GC&#xA0;(under LuceneIndexPersistence): 168K (~6.64GB)Managed Memory: 6.72GBUnmanaged Memory: 1.32GBLooking at the GC logs, we now have:2024-12-16T18:33:29.8143148Z, Type: GC, thread id: 8508, duration: 93.6835ms, index: 319, generation: 2, reason: Induced&#xD;&#xA;2024-12-16T18:33:30.7013255Z, Type: GC, thread id: 8508, duration: 142.1781ms, index: 320, generation: 2, reason: Induced&#xD;&#xA;2024-12-16T18:33:31.5691610Z, Type: GC, thread id: 8508, duration: 91.0983ms, index: 321, generation: 2, reason: Induced&#xD;&#xA;2024-12-16T18:33:37.8245671Z, Type: GC, thread id: 8508, duration: 112.7643ms, index: 322, generation: 2, reason: InducedSo the GC time is now in the range of 100ms, instead of several seconds. This change helps both reduce overall GC pause times and greatly reduce the amount of CPU spent on managing garbage. Those are still big queries, but now we can focus on executing the query, rather than managing maintenance tasks. Incidentally, those sorts of issues are one of the key reasons why we built Corax, which can process queries directly on top of persistent structures, without needing to materialize anything from the disk.</p>
        </article>
        <article id="article-9">
            <a href="https://ayende.com/blog/201859-C/sometimes-its-the-hardware" target="_blank">
                <h2 class="title mb-6" id="article-9">Sometimes it&#x27;s the hardware</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: December 31, 2024
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">An issue&#xA0;was recently raised with a really scary title: Intermittent Index corruption: VoronUnrecoverableErrorException. Those are the kinds of issues that you know&#xA0;are going to be complex. Fixing such issues in the past was usually a Task Force effort and quite a challenge. We asked for more information and started figuring out who would handle the issue (given the time of the year) when the user came back with:After pressing the disk check issue with our hosting provider, we found out that one of the disks was reporting an error but according to our hosting, it was only because the manufacturer&#x27;s guarantee expired, and not the actual disk failure. We swapped the disk anyway, and so far we are not seeing the issue.I&#x2019;m so happy that I can close that issue &#x1F642;</p>
        </article>
        <article id="article-10">
            <a href="https://devblogs.microsoft.com/dotnet/top-dotnet-videos-live-streams-of-2024/" target="_blank">
                <h2 class="title mb-6" id="article-10">Top .NET Videos &amp; Live Streams of 2024</h2>
            </a>
            <p class="mb-2">by James Montemagno</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: December 30, 2024
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">Let&#x27;s take a look back at the amazing .NET videos, events, and live streams from 2024!</p>
        </article>
        <div class="button flex justify-between">
            <span class="back invisible arrow"></span>

            <a href="2.html"><span class="next arrow"></span></a>
        </div>
    </section>
</main>
<footer
    class="mt-auto flex w-full flex-col items-center justify-center gap-y-2 pb-4 pt-20 text-center align-top font-semibold text-gray-600 dark:text-gray-400 sm:flex-row sm:justify-between sm:text-xs">
    <div class="me-0 sm:me-4">
        <div class="flex flex-wrap items-end gap-x-2">
            <ul class="flex flex-1 items-center gap-x-2 sm:flex-initial">
                <li class="flex">
                    <p class="flex items-end gap-2 justify-center flex-wrap	">Â© Relatively General
                        .NET 2025<span
                            class="inline-block">&nbsp;ðŸš€&nbsp;Theme: Astro Cactus</span>

                        <a class="inline-block sm:hover:text-link" href="https://github.com/chrismwilliams/astro-cactus"
                           rel="noopener noreferrer " target="_blank">
                            <svg width="1em" height="1em" viewBox="0 0 24 24" aria-hidden="true" class="h-6 w-6"
                                 focusable="false" data-icon="mdi:github">
                                <symbol id="ai:mdi:github">
                                    <path fill="currentColor"
                                          d="M12 2A10 10 0 0 0 2 12c0 4.42 2.87 8.17 6.84 9.5c.5.08.66-.23.66-.5v-1.69c-2.77.6-3.36-1.34-3.36-1.34c-.46-1.16-1.11-1.47-1.11-1.47c-.91-.62.07-.6.07-.6c1 .07 1.53 1.03 1.53 1.03c.87 1.52 2.34 1.07 2.91.83c.09-.65.35-1.09.63-1.34c-2.22-.25-4.55-1.11-4.55-4.92c0-1.11.38-2 1.03-2.71c-.1-.25-.45-1.29.1-2.64c0 0 .84-.27 2.75 1.02c.79-.22 1.65-.33 2.5-.33s1.71.11 2.5.33c1.91-1.29 2.75-1.02 2.75-1.02c.55 1.35.2 2.39.1 2.64c.65.71 1.03 1.6 1.03 2.71c0 3.82-2.34 4.66-4.57 4.91c.36.31.69.92.69 1.85V21c0 .27.16.59.67.5C19.14 20.16 22 16.42 22 12A10 10 0 0 0 12 2"></path>
                                </symbol>
                                <use xlink:href="#ai:mdi:github"></use>
                            </svg>
                            <span class="sr-only">Github</span>
                        </a>
                    </p>
                </li>
            </ul>
        </div>
    </div>
    <nav aria-label="More on this site" class="flex gap-x-2 sm:gap-x-0 sm:divide-x sm:divide-gray-500">
        <a class="px-4 py-2 sm:py-0 sm:hover:text-textColor sm:hover:underline" href="index.html"> Home </a><a
            class="px-4 py-2 sm:py-0 sm:hover:text-textColor sm:hover:underline" href="/about/"> About </a>
    </nav>
</footer>
<script src="js/script.js?id=af8f4559935e7bf5bf6015373793411d"></script>
<script src="pagefind/pagefind-ui.js"></script>
</body>
</html>