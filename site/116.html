
<!DOCTYPE html>
<html class="scroll-smooth" lang="en-US" data-theme="light">
<head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, shrink-to-fit=no" name="viewport">
    <meta content="IE=edge" http-equiv="X-UA-Compatible">
    <title>Page 116 â€¢ Relatively General .NET</title>
    <link href="favicon.ico" rel="icon" sizes="any">
    <link href="images/apple-touch-icon.png" rel="apple-touch-icon">
    <meta content="hsl()" name="theme-color">
    <meta content="website" property="og:type">
    <meta content="Home" property="og:title">
    <link rel="stylesheet" href="css/styles.css">
    <link rel="stylesheet" href="pagefind/pagefind-ui.css">
</head>
<body class="mx-auto flex min-h-screen max-w-3xl flex-col bg-bgColor px-4 pt-16 font-mono text-sm font-normal text-textColor antialiased sm:px-8">

<a class="sr-only focus:not-sr-only focus:fixed focus:start-1 focus:top-1.5" href="#main">
    skip to content
</a>
<header class="group relative mb-28 flex items-center sm:ps-[4.5rem]" id="main-header">
    <div class="flex sm:flex-col">
        <a aria-current="page" class="inline-flex items-center hover:filter-none sm:relative sm:inline-block"
           href="index.html">
            <img class="me-3 sm:absolute sm:start-[-4.5rem] sm:me-0 sm:h-16 sm:w-16 w-16" src="images/giphy.gif"
                 alt=""/>
            <span class="text-xl font-bold sm:text-2xl">Relatively General .NET</span>
        </a>
        <nav aria-label="Main menu"
             class="absolute -inset-x-4 top-14 hidden flex-col items-end gap-y-4 rounded-md bg-bgColor/[.85] py-4 text-accent shadow backdrop-blur group-[.menu-open]:z-50 group-[.menu-open]:flex sm:static sm:z-auto sm:-ms-4 sm:mt-1 sm:flex sm:flex-row sm:items-center sm:divide-x sm:divide-dashed sm:divide-accent sm:rounded-none sm:bg-transparent sm:py-0 sm:shadow-none sm:backdrop-blur-none"
             id="navigation-menu">
            <a aria-current="page" class="px-4 py-4 underline-offset-2 sm:py-0 sm:hover:underline"
               href="index.html"> Home </a><a
                class="px-4 py-4 underline-offset-2 sm:py-0 sm:hover:underline" href="/about/">
                About </a>
        </nav>
    </div>
    <site-search class="ms-auto" id="search">
        <button id="open-search"
                class="flex h-9 w-9 items-center justify-center rounded-md ring-zinc-400 transition-all hover:ring-2"
                data-open-modal="">
            <svg aria-label="search" class="h-7 w-7" fill="none" height="16" stroke="currentColor"
                 stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" viewBox="0 0 24 24" width="16"
                 xmlns="http://www.w3.org/2000/svg">
                <path d="M0 0h24v24H0z" stroke="none"></path>
                <path d="M3 10a7 7 0 1 0 14 0 7 7 0 1 0-14 0M21 21l-6-6"></path>
            </svg>
        </button>
        <dialog aria-label="search"
                class="h-full max-h-full w-full max-w-full border border-zinc-400 bg-bgColor shadow backdrop:backdrop-blur sm:mx-auto sm:mb-auto sm:mt-16 sm:h-max sm:max-h-[calc(100%-8rem)] sm:min-h-[15rem] sm:w-5/6 sm:max-w-[48rem] sm:rounded-md">
            <div class="dialog-frame flex flex-col gap-4 p-6 pt-12 sm:pt-6">
                <button id="close-search"
                        class="ms-auto cursor-pointer rounded-md bg-zinc-200 p-2 font-semibold dark:bg-zinc-700"
                        data-close-modal="">Close
                </button>
                <div class="search-container">
                    <div id="cactus__search"/>
                </div>
            </div>
        </dialog>
    </site-search>
    <theme-toggle class="ms-2 sm:ms-4">
        <button id="theme-toggle" class="relative h-9 w-9 rounded-md p-2 ring-zinc-400 transition-all hover:ring-2"
                type="button">
            <span class="sr-only">Dark Theme</span>
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-7 w-7 -translate-x-1/2 -translate-y-1/2 scale-100 opacity-100 transition-all dark:scale-0 dark:opacity-0"
                 fill="none" focusable="false" id="sun-svg" stroke-width="1.5" viewBox="0 0 24 24"
                 xmlns="http://www.w3.org/2000/svg">
                <path
                    d="M12 18C15.3137 18 18 15.3137 18 12C18 8.68629 15.3137 6 12 6C8.68629 6 6 8.68629 6 12C6 15.3137 8.68629 18 12 18Z"
                    stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M22 12L23 12" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M12 2V1" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M12 23V22" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M20 20L19 19" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M20 4L19 5" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M4 20L5 19" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M4 4L5 5" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M1 12L2 12" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
            </svg>
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-7 w-7 -translate-x-1/2 -translate-y-1/2 scale-0 opacity-0 transition-all dark:scale-100 dark:opacity-100"
                 fill="none" focusable="false" id="moon-svg" stroke="currentColor" stroke-width="1.5"
                 viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M0 0h24v24H0z" fill="none" stroke="none"></path>
                <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z"></path>
                <path d="M17 4a2 2 0 0 0 2 2a2 2 0 0 0 -2 2a2 2 0 0 0 -2 -2a2 2 0 0 0 2 -2"></path>
                <path d="M19 11h2m-1 -1v2"></path>
            </svg>
        </button>
    </theme-toggle>
    <mobile-button>
        <button aria-expanded="false" aria-haspopup="menu" aria-label="Open main menu"
                class="group relative ms-4 h-7 w-7 sm:invisible sm:hidden" id="toggle-navigation-menu" type="button">
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-full w-full -translate-x-1/2 -translate-y-1/2 transition-all group-aria-expanded:scale-0 group-aria-expanded:opacity-0"
                 fill="none" focusable="false" id="line-svg" stroke="currentColor" stroke-width="1.5"
                 viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M3.75 9h16.5m-16.5 6.75h16.5" stroke-linecap="round" stroke-linejoin="round"></path>
            </svg>
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-full w-full -translate-x-1/2 -translate-y-1/2 scale-0 text-accent opacity-0 transition-all group-aria-expanded:scale-100 group-aria-expanded:opacity-100"
                 fill="none" focusable="false" id="cross-svg" stroke="currentColor" stroke-width="1.5"
                 viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M6 18L18 6M6 6l12 12" stroke-linecap="round" stroke-linejoin="round"></path>
            </svg>
        </button>
    </mobile-button>
</header>
<main id="main" data-pagefind-body>
    <section aria-label="Blog post list">
        <article id="article-1151">
            <a href="https://ayende.com/blog/193062-A/building-a-social-media-platform-without-going-bankrupt-part-x-optimizing-for-whales" target="_blank">
                <h2 class="title mb-6" id="article-1151">Building a social media platform without going bankrupt</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: February 05, 2021
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">Unless I get good feedback / questions on the other posts in the series, this is likely to be the last post on the topic. I was trying to show what kind of system and constraints you have to deal with if you wanted to build a social media platform without breaking the bank.I talked about the expected numbers that we have for the system, and then set out to explain each part of it independently. Along the way, I was pretty careful not to mention any one particular technological solution. We are going to need:CachingObject storage (S3 compatible API)Content Delivery NetworkKey/value storeQueuing and worker infrastructureNote that the whole thing is generic and there are very little constraints on the architecture. That is by design, because if your architecture can hit the lowest common denominator, you have a lot more freedom. Instead of tying yourself to a particular provider, you have a lot more freedom. For that matter, you can likely set things up so you can have multiple disparate providers without too much of a hassle. My goal with this system was to be able to accept 2,500 posts per second and to handle reads of 250,000 per second. This sounds like a lot, but a most of the load is meant to be handled by CDN and the infrastructure, not the core servers. Caching in a social network is somewhat problematic, since you&#x2019;ll have a lot of the work is obviously personalized. That said, there is still quite a lot that can be cached, especially the more popular posts and threads. If we&#x2019;ll assume that only about 10% of the reading load hits our servers, that is 25,000 reads per second. If we have just 25 servers for handling this (assuming five each in five separate data centers) we can accept the load at 1,000 requests per second. On the one hand, that is a lot, but on the other hand&#x2026;. most of the cost is supposed to be about authorization, minor logic, etc. We can also at this point add more application servers and scale linearly. Just to give some indication of costs, a dedicated server with 8 cores &amp; 32 GB disk will cost 100$ a month, and there is no charge for traffic. Assuming that I&#x2019;m running 25 of these, that will cost me 2,500 USD a month. I can safely double or triple that amount without much trouble, I think.Having to deal with 1,000 requests per server is something that requires paying attention to what you are doing, but it isn&#x2019;t really that hard, to be frank. RavenDB can handle more than a million queries a second, for example.One thing that I didn&#x2019;t touch on, however, which is quite important, is the notion of whales. In this case, a whale is a user that has a lot of followers. Let&#x2019;s take Mr. Beat as an example, he has 15 million followers and is a prolific poster. In our current implementation, we&#x2019;ll need to add to the timeline of all his followers every time that he posts something. Mrs. Bold, on the other hand, has 12 million followers. At one time Mr. Beat and Mrs. Bold got into a post fight. This looks like this:Mr. Beat: I think that Mrs. Bold has a Broccoli&#x2019;s bandana. Mrs. Bold: @mrBeat How dare you, you sniveling knaveMr. Beat: @boldMr2 I dare, you green teeth monsterMrs. Bold: @mrBeat You are a yellow belly deerMr. Beat: @boldMr2 Your momma is a dearThis incredibly witty post exchange happened during a three minute span. Let&#x2019;s consider what this will do, given the architecture that we outlined so far:Post #1 &#x2013; written to 15 million timelines.Post #2 - 5 &#x2013; written to the timelines of everyone that follows both of them (mention), let&#x2019;s call that 10 million.That is 55 million timeline writes to process within the span of a few minutes. If other whales also join in (and they might) the number of writes we&#x2019;ll have to process will sky rocket. Instead, we are going to take advantage of the fact that only a small number of accounts are actually followed by many people. We&#x2019;ll place the limit at 10,000 followers. At which point, we&#x2019;ll no longer process writes for such accounts. Instead, we&#x2019;ll place the burden at the client&#x2019;s side. The code for showing the timeline will then become something like this:&#xA;    &#xA;      &#xA;        &#xA;  &#xA;    &#xA;    &#xA;&#xA;        &#xA;&#xA;&#xA;  &#xA;  &#xA;  &#xA;    &#xA;&#xA;    &#xA;      This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.&#xA;      Learn more about bidirectional Unicode characters&#xA;    &#xA;&#xA;&#xA;              Show hidden characters&#xA;&#xA;&#xA;&#xA;&#xA;  &#xA;    &#xA;    &#xA;&#xA;&#xA;&#xA;  &#xA;        &#xA;          &#xA;          async function load_timeline(user_id: string) {&#xA;        &#xA;        &#xA;          &#xA;              var user = await users.get(user_id);&#xA;        &#xA;        &#xA;          &#xA;              var promises = [timelines.get_first(user.timelines[&quot;self&quot;])];&#xA;        &#xA;        &#xA;          &#xA;              for (const whale of user.whales) {&#xA;        &#xA;        &#xA;          &#xA;                  promises.push(timelines.get_first(whale));&#xA;        &#xA;        &#xA;          &#xA;              }&#xA;        &#xA;        &#xA;          &#xA;              var all = await Promise.all(promises);&#xA;        &#xA;        &#xA;          &#xA;              var timeline = [].concat(all); // flatten the arrays&#xA;        &#xA;        &#xA;          &#xA;              timeline.sort(); // remember, post ids are semi sortable&#xA;        &#xA;        &#xA;          &#xA;              return timeline;&#xA;        &#xA;        &#xA;          &#xA;          }&#xA;        &#xA;  &#xA;&#xA;&#xA;&#xA;    &#xA;&#xA;  &#xA;&#xA;&#xA;      &#xA;      &#xA;        view raw&#xA;        &#xA;          read.js&#xA;        &#xA;        hosted with &#x2764; by GitHub&#xA;      &#xA;    &#xA;&#xA;In other words, we record the high profile users in the system and instead of doing the work for them on write, we&#x2019;ll do that on read. The benefit of doing it in this manner is that the high profile users tiimeline reads will have very high cache utilization.Given that the number of high profile people you&#x2019;ll follow are naturally limited, that can save quite a lot of work.The code above can be improved, of course, there are usually a lot of difference in the timeline posts, so we may have a high profile user that is off for a day or two, they shouldn&#x2019;t show up in the current timeline and can be removed entirely. You need to do a bit more work around the time frames as well, which means that timeline should also allow us to query itself by most recent post id, but that is also not too hard to implement. And with that, we are at the end. I think that I covered quite a few edge cases and interesting details, and hopefully that was interesting for you to read. As usual, I really appreciate any and all feedback.</p>
        </article>
        <article id="article-1152">
            <a href="https://ayende.com/blog/193061-A/building-a-social-media-platform-without-going-bankrupt-part-ix-dealing-with-the-past" target="_blank">
                <h2 class="title mb-6" id="article-1152">Building a social media platform without going bankrupt</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: February 04, 2021
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">A social media platform has to deal with the concept of now and its history. For the most part, most users are interacting with the current state of the system. Looking at their timeline, watching current posts, etc.&#xA0; At the same time, there is a wealth of information that you can get from looking at the past. It isn&#x2019;t out of the question that you&#x2019;ll have users diving into the history of posts of another user going as far back as possible. That can be a parent whose kids just left the house, looking at baby pictures or it can be a new friend, trying to learn some interesting tidbits before a party (when we still had those). It can also be automated processes, such as: &#x201C;5 years ago you posted&#x2026;&#x201D;The architecture that I presented in these posts is relatively agnostic for such a scenario. Given the timeline feature, going back in time means that you can fairly easily discriminate based on age. Older sections in the timeline can be moved to lower class storage tier (basically, move to HDD instead of NVMe, for example). They are still accessible, still available, but far cheaper to store. I don&#x2019;t believe that you can usually go with an archive tier level for the timelines, not unless you are willing to effectively be unable to access them if a user requests it, but a policy of moving old and rarely used timeline sections and posts to HDD is absolutely doable. Note that things like intelligent tiering is not a good solution for our needs. That would move items based on age and access, but while we want to move items by age, older items are still access, just far more rarely, so we don&#x2019;t want to move them back into hot storage if they are rarely accessed.That said, certain posts are likely to generate active for a long time. So we can&#x2019;t just send data to cold storage just based on age. Need to also take into account the recent access patterns. On the other hand, consider a post a few years ago that talks about Broccoli, when people still did that. Mr. Beat discovers that Mrs. Bold has such a post and blast it all over social media. Very quickly that old post become very active. That means that we should have a way to move data back to hot storage if there is enough access.Ideally, we can rely on the underlying storage to do that for us, but we have to know how it actually works behind the scenes and understand what is actually going on there. The nice thing about this is that unlike most of the details we discussed so far, that is something that we can punt down the road, we already have the architecture in place that will allow us to introduce this cost savings measure down the line, we don&#x2019;t have to have it figured out from day one. Given the fact that we have multi level caches, that means that we can probably just age out old information to cold storage and not usually have to think about it too much.When we have enough data that this is a serious concern, on the other hand&#x2026; we will have the time and resources to also handle it.</p>
        </article>
        <article id="article-1153">
            <a href="https://ayende.com/blog/193060-A/building-a-social-media-platform-without-going-bankrupt-part-viii-tagging-and-searching" target="_blank">
                <h2 class="title mb-6" id="article-1153">Building a social media platform without going bankrupt</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: February 03, 2021
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">Quite a few of the features that we consider native to social media actually came about as a result of users&#x2019; behavior, not pre-planned actions. For example, the #tagging and @mentions were both created by users and then adopted as an official action by the social media giants.I already touched on how mentioned are handled, as part of writing the document itself, we insert the post id to the timeline of the mentioned user. For tagging, the process is very similar. Each tag has a timeline, and we can insert posts into the tag&#x2019;s timeline. From there on, the process is basically identical for what we already describe.I want to stop for a second and emphasis the coolness factor of a significant feature being handle (in the backend) via simply:&#xA;    &#xA;      &#xA;        &#xA;  &#xA;    &#xA;    &#xA;&#xA;        &#xA;&#xA;&#xA;  &#xA;  &#xA;  &#xA;    &#xA;&#xA;    &#xA;      This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.&#xA;      Learn more about bidirectional Unicode characters&#xA;    &#xA;&#xA;&#xA;              Show hidden characters&#xA;&#xA;&#xA;&#xA;&#xA;  &#xA;    &#xA;    &#xA;&#xA;&#xA;&#xA;  &#xA;        &#xA;          &#xA;          async function handle_tags_async(post){&#xA;        &#xA;        &#xA;          &#xA;              for(const tag in post.tags){&#xA;        &#xA;        &#xA;          &#xA;                  var timeline = await timelines.for_tag(tag);&#xA;        &#xA;        &#xA;          &#xA;                  await timelines.append(timeline, post.id);&#xA;        &#xA;        &#xA;          &#xA;              }&#xA;        &#xA;        &#xA;          &#xA;          }&#xA;        &#xA;  &#xA;&#xA;&#xA;&#xA;    &#xA;&#xA;  &#xA;&#xA;&#xA;      &#xA;      &#xA;        view raw&#xA;        &#xA;          tags.js&#xA;        &#xA;        hosted with &#x2764; by GitHub&#xA;      &#xA;    &#xA;&#xA;There is probably UI work to do here, but that is roughly all you&#x2019;ll need to manage tags. Presumably you&#x2019;ll want some better policies, but that is the core behavior you&#x2019;ll need to support this sort of feature.What about searching? Full search indexes are nothing new and you can get an off the shelve solution to manage your searches easily enough. That is likely to be one of the annoying pieces of the system. Luckily, we can usually handle things by offering two tiers of searching. We have the first tier, which cover posts in the recent past (a month or two) which must have very high speed queries and then we have full data search, for which we have a lot longer SLA. By far most queries are going to be hitting the recent data set, which makes the task itself easier. The actual choice of indexing solution and its usage is fairly irrelevant at this point. You&#x2019;ll need something that is distributed, but there is enough variety there that you can get away with selecting pretty much anything. We aren&#x2019;t going to need to provide sophisticated full text search features, we just want users to be able to find results by text queries. You&#x2019;ll note that throughout this series of posts, I&#x2019;m not trying to find novel ways to get the best solution. I&#x2019;m using practical options for the actual use case presented and in many cases, I can get away with a lot by changing the requirements just slightly. For that matter, a lot of the limitations that I accept are real limitations that you&#x2019;ll find with other social media networks as well.Finally, I just wanted to show how we can enable basic search capability using minimal amount of code, given the infrastructure we have so far:&#xA;    &#xA;      &#xA;        &#xA;  &#xA;    &#xA;    &#xA;&#xA;        &#xA;&#xA;&#xA;  &#xA;  &#xA;  &#xA;    &#xA;&#xA;    &#xA;      This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.&#xA;      Learn more about bidirectional Unicode characters&#xA;    &#xA;&#xA;&#xA;              Show hidden characters&#xA;&#xA;&#xA;&#xA;&#xA;  &#xA;    &#xA;    &#xA;&#xA;&#xA;&#xA;  &#xA;        &#xA;          &#xA;          var stopwords = [&quot;a&quot;, &quot;the&quot;, &quot;has&quot;, &quot;have&quot;];&#xA;        &#xA;        &#xA;          &#xA;          async function handle_search(post){&#xA;        &#xA;        &#xA;          &#xA;              var terms = post.text.split(&#x27; &#x27;);&#xA;        &#xA;        &#xA;          &#xA;              for (const stopword of stopwords) {&#xA;        &#xA;        &#xA;          &#xA;                  terms.remove(stopword);&#xA;        &#xA;        &#xA;          &#xA;              }&#xA;        &#xA;        &#xA;          &#xA;              for (const term of terms) {&#xA;        &#xA;        &#xA;          &#xA;                  let term_timeline = await timelines.search_for(term);&#xA;        &#xA;        &#xA;          &#xA;                  await timelines.append(term_timeline, post.id);&#xA;        &#xA;        &#xA;          &#xA;              }&#xA;        &#xA;        &#xA;          &#xA;          }&#xA;        &#xA;  &#xA;&#xA;&#xA;&#xA;    &#xA;&#xA;  &#xA;&#xA;&#xA;      &#xA;      &#xA;        view raw&#xA;        &#xA;          search.js&#xA;        &#xA;        hosted with &#x2764; by GitHub&#xA;      &#xA;    &#xA;&#xA;As you can see, we built a simple full text search here. To query it, you get the timeline for a particular term and get the list of post that it has. For tags and searches, as you can imagine, this can be a huge list, which is partly why timelines are built on the concept of sections that can be so easily distributed. The solution above isn&#x2019;t actually a good one for full text search. I can&#x2019;t easily turn that into a search by a phrase, and there are many other features that I&#x2019;ll likely want to have, but that is a good example of how the infrastructure that we built for one part of the system can be utilized for completely different purpose.</p>
        </article>
        <article id="article-1154">
            <a href="https://ayende.com/blog/193059-A/building-a-social-media-platform-without-going-bankrupt-part-vii-counting-views-replies-and-likes" target="_blank">
                <h2 class="title mb-6" id="article-1154">Building a social media platform without going bankrupt</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: February 02, 2021
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">I touched briefly on the issue of posts statistics in a previous post, but it deserve its own post. There are all sort of metrics that we want to track on a post. Here are just a few of them:Unlike most of the items that we discussed so far, these details are going to be very relevant for both reads and writes. In particular, it is very common for these numbers to be update concurrently, especially when talking about the popular posts. At the simplest level, these can be represented as a map&lt;key, int64&gt;. That gives us the maximum flexibility for our needs and can be also utilized in the future for additional use cases. Given that this is effectively a distributed counter problem, there are all sort of ways that we can handle this. At the client level, we send the increment operation to the server and manually update the value. That gets us 90% there in terms of the UX factors, but there is a lot to handle this behind the scenes.A good algorithm to use for this is the PN Counters model from the CRDT playbook. RavenDB implements these for you, for example. In essence, that means that we have the following data model:&#xA;    &#xA;      &#xA;        &#xA;  &#xA;    &#xA;    &#xA;&#xA;        &#xA;&#xA;&#xA;  &#xA;  &#xA;  &#xA;    &#xA;&#xA;    &#xA;      This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.&#xA;      Learn more about bidirectional Unicode characters&#xA;    &#xA;&#xA;&#xA;              Show hidden characters&#xA;&#xA;&#xA;&#xA;&#xA;  &#xA;    &#xA;    &#xA;&#xA;&#xA;&#xA;  &#xA;        &#xA;          &#xA;          {&#xA;        &#xA;        &#xA;          &#xA;            &quot;likes&quot;: {&#xA;        &#xA;        &#xA;          &#xA;              &quot;node-128&quot;: { &quot;v&quot;: 348, &quot;etag&quot;: 34 },&#xA;        &#xA;        &#xA;          &#xA;              &quot;node-382&quot;: { &quot;v&quot;: 8, &quot;etag&quot;: 231 },&#xA;        &#xA;        &#xA;          &#xA;              &quot;node-023&quot;: { &quot;v&quot;: -12, &quot;etag&quot;: 488 },&#xA;        &#xA;        &#xA;          &#xA;              &quot;node-493&quot;: { &quot;v&quot;: 493, &quot;etag&quot;: 1823 },&#xA;        &#xA;        &#xA;          &#xA;            },&#xA;        &#xA;        &#xA;          &#xA;            &quot;replies&quot;: {&#xA;        &#xA;        &#xA;          &#xA;              &quot;node-372&quot;: { &quot;v&quot;: 23, &quot;etag&quot;: 121 },&#xA;        &#xA;        &#xA;          &#xA;              &quot;node-023&quot;: { &quot;v&quot;: 2, &quot;etag&quot;: 788 },&#xA;        &#xA;        &#xA;          &#xA;            }&#xA;        &#xA;        &#xA;          &#xA;          }   &#xA;        &#xA;  &#xA;&#xA;&#xA;&#xA;    &#xA;&#xA;  &#xA;&#xA;&#xA;      &#xA;      &#xA;        view raw&#xA;        &#xA;          counters.json&#xA;        &#xA;        hosted with &#x2764; by GitHub&#xA;      &#xA;    &#xA;&#xA;The likes and replies object has a property per each node that increment a value. That contains the value that we have for that node as well as the etag for this change. It is easy to merge such a model between different versions, because we can always take value of the higher etag to get the latest value. In this way, we can allow concurrent and distributed updates across the entire system and it will resolve itself in the end to the right value. Another option may be to push the commands all way to the owning data center, where we&#x2019;ll apply the operations, but that may add a high load on hot posts in the system. Better to distribute this globally and not really concern ourselves with the matter. Looking at Twitter, there are about 200 billion tweets a year. That means that we have to be ready for quite a few of those values. Having that in a dedicated system is a good idea, since it has far different read &amp; write skew than other parts of our system. As part of reading of posts, however, we&#x2019;ll likely want to build some mechanism for pushing those counters to the post itself so we can remove that from the rest of the system. An easy way to handle that is to do some on an hourly basis. So instead of the format above, we&#x2019;ll have:&#xA;    &#xA;      &#xA;        &#xA;  &#xA;    &#xA;    &#xA;&#xA;        &#xA;&#xA;&#xA;  &#xA;  &#xA;  &#xA;    &#xA;&#xA;    &#xA;      This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.&#xA;      Learn more about bidirectional Unicode characters&#xA;    &#xA;&#xA;&#xA;              Show hidden characters&#xA;&#xA;&#xA;&#xA;&#xA;  &#xA;    &#xA;    &#xA;&#xA;&#xA;&#xA;  &#xA;        &#xA;          &#xA;          {&#xA;        &#xA;        &#xA;          &#xA;          &quot;2021-01-24T14:00:00&quot;: {  &#xA;        &#xA;        &#xA;          &#xA;            &quot;likes&quot;: {&#xA;        &#xA;        &#xA;          &#xA;              &quot;node-128&quot;: { &quot;v&quot;: 348, &quot;etag&quot;: 34 },&#xA;        &#xA;        &#xA;          &#xA;              &quot;node-493&quot;: { &quot;v&quot;: 493, &quot;etag&quot;: 1823 },&#xA;        &#xA;        &#xA;          &#xA;            },&#xA;        &#xA;        &#xA;          &#xA;            &quot;replies&quot;: {&#xA;        &#xA;        &#xA;          &#xA;              &quot;node-372&quot;: { &quot;v&quot;: 23, &quot;etag&quot;: 121 },&#xA;        &#xA;        &#xA;          &#xA;              &quot;node-023&quot;: { &quot;v&quot;: 2, &quot;etag&quot;: 788 },&#xA;        &#xA;        &#xA;          &#xA;            }&#xA;        &#xA;        &#xA;          &#xA;           },&#xA;        &#xA;        &#xA;          &#xA;           &quot;2021-01-24T15:00:00&quot;: {  &#xA;        &#xA;        &#xA;          &#xA;            &quot;likes&quot;: {&#xA;        &#xA;        &#xA;          &#xA;              &quot;node-382&quot;: { &quot;v&quot;: 8, &quot;etag&quot;: 231 },&#xA;        &#xA;        &#xA;          &#xA;              &quot;node-023&quot;: { &quot;v&quot;: -12, &quot;etag&quot;: 488 },&#xA;        &#xA;        &#xA;          &#xA;            },&#xA;        &#xA;        &#xA;          &#xA;            &quot;replies&quot;: {&#xA;        &#xA;        &#xA;          &#xA;              &quot;node-023&quot;: { &quot;v&quot;: 3, &quot;etag&quot;: 828 },&#xA;        &#xA;        &#xA;          &#xA;            }&#xA;        &#xA;        &#xA;          &#xA;           }&#xA;        &#xA;        &#xA;          &#xA;          }   &#xA;        &#xA;  &#xA;&#xA;&#xA;&#xA;    &#xA;&#xA;  &#xA;&#xA;&#xA;      &#xA;      &#xA;        view raw&#xA;        &#xA;          counters2.json&#xA;        &#xA;        hosted with &#x2764; by GitHub&#xA;      &#xA;    &#xA;&#xA;Here we have the last two hours of updates of operations on the post. Once every hour we&#x2019;ll consolidate all the updates from two hours ago and write them to the post itself. When we get to the point where we have no more updates in the post, we can safely delete the value. The reason you want to add this complexity is that there is a big difference between all the posts in a social media and the active working set. That tends to be far smaller value and can dramatically reduce the amount of data we need to keep and manage. Assuming that the working set is at 25 millions posts or so across the network seems reasonable, and that amount of data can be easily handle by any server instance you care to use. Managing 200 billion per year, on the other hand, puts us in a different class of problem, and we&#x2019;ll need more and more resources down the line.</p>
        </article>
        <article id="article-1155">
            <a href="https://andrewlock.net/finding-all-routable-components-in-a-webassembly-app/" target="_blank">
                <h2 class="title mb-6" id="article-1155">Finding all routable components in a Blazor App</h2>
            </a>
            <p class="mb-2">by Andrew Lock</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: February 02, 2021
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">In this post I show how to find all the routable components in a Blazor app, so that you can automate the static prerendering of a Blazor WebAssembly app.&#x2026;</p>
        </article>
        <article id="article-1156">
            <a href="https://ayende.com/blog/193058-A/building-a-social-media-platform-without-going-bankrupt-part-vi-dealing-with-edits-and-deletions" target="_blank">
                <h2 class="title mb-6" id="article-1156">Building a social media platform without going bankrupt</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: February 01, 2021
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">In the series so far, we talked about reading and writing posts, updating the timeline and distributing it, etc. I talked briefly about the challenges of caching data when we have to deal with updates in the background, but I haven&#x2019;t really touched on that. Edits and updates are a pain to handle, because they invalidate the cache, which is one of the primary ways we can scale cheaply. We need to consider a few different aspects of the problem. What sort of updates do we have in the context of a social media platform? We can easily disable editing of posts, of course, but you have to be able to support deletes. A user may post about Broccoli, which is verboten and we have to be able to remove that. And of course, users will want to be able to delete their own post, let&#x2019;s their salad tendencies come back to hunt them in the future. Another reason we need to handle updates is this:We keep track of the interactions of the post and we need to update them as they change. In fact, in many cases we want to update them &#x201C;live&#x201D;. How are we going to handle this? I discussed the caching aspects of this earlier, but the general idea is that we have two caching layers in place.An user getting data will first hit the CDN, which may cache the data (green icon) and then the API, which will get the data from the backend. The API endpoint will query its own local state and other pieces of the puzzle are responsible for the data distribution.When changes happen, we need to deal with them, like so:Each change means that we have to deal with a policy decision. For example, a deletion to a post means that we need to go and push an update to all the data centers to update the data. The same is relevant for updates to the post itself. In general, updating the content of the post or updating its view counts aren&#x2019;t really that different. We&#x2019;ll usually want to avoid editing the post content for non technical reasons, not for lack of ability to do so.Another important aspect to take into account is latency and updates. Depending on the interaction model with the CDN, we likely have it setup to cache data based on duration, so API requests are cached for a period of a few seconds to a minute or two. That is usually good enough to reduce the load on our servers and still retain good enough level of updates.Another advantage that we can use is the fact that when we get to high numbers, we can reduce the update rate. Consider:We now need to update the post only once every 100,000 likes or shares or once for 10,000 replies. Depending on the rate of change, we can skip that if this happened recently enough. That is the kind of thing that can reduce the load curve significantly.There is also the need to consider live updates. Typically, that means that we&#x2019;ll have the client connected via web socket to a server and we need to be able to tell it that a post has been updated. We can do that using the same cache update mechanism. The update cache command is placed on a queue and the web socket servers process messages from there. A client will indicate what post ids it is interested in and the web socket server will notify it about such changes. The idea is that we can completely separate the different pieces in the system. We have the posts storage and the timeline as one system and the live updates as a separate system. There is some complexity here about cache usage, but it is actually better to assume that stuff will not work than to try for cache coherency.For example, a client may get an update that a particular post was updated. When it query for the new post details, it gets a notice that it wasn&#x2019;t modified. This is a classic race condition issue which can case a lot of trouble for the backend people to eradicate. If we don&#x2019;t try, we can simply state that on the client side, getting a not modified response after an update note is not an error. Instead, we need to schedule (on the client) to query the post again after the cache period elapsed.A core design tenant in the system is to assume failure and timing issues, to avoid having to force a unified view of the system, because that is hard. Punting the problem even just a little bit allows us a much better architecture.</p>
        </article>
        <article id="article-1157">
            <a href="https://www.meziantou.net/automatically-generate-a-form-from-an-object-in-blazor.htm" target="_blank">
                <h2 class="title mb-6" id="article-1157">Automatically generate a form from an object in Blazor</h2>
            </a>
            <p class="mb-2">by G&#xE9;rald Barr&#xE9;</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: February 01, 2021
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">Blazor provides building blocks for creating forms. It includes editor components, model validation, and model binding. When you want to create a form, you need to create an object to store the form data and create the razor component with labels and editors for each property. This is tedious when</p>
        </article>
        <article id="article-1158">
            <a href="https://ayende.com/blog/193057-A/building-a-social-media-platform-without-going-bankrupt-part-v-handling-the-timeline" target="_blank">
                <h2 class="title mb-6" id="article-1158">Building a social media platform without going bankrupt</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: January 29, 2021
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">In the series of posts so far, when discussing reads, I punted the part where we know what to read. I mentioned that we get a whole batch of post ids from some where and discussed how that is going to work, but that was it. Now I want to talk exactly on how this works. The timeline concept is a fairly simple one. We have a list of posts ids that the user goes through. As they are browsing through the list, items are added at the top, etc. This is basically the Twitter model. Another alternative is that as you scroll, if there are new items in the list, you are shown them before older values (the Facebook approach), but that is more complex. Conceptually, the timeline is as simple as:&#xA;    &#xA;      &#xA;        &#xA;  &#xA;    &#xA;    &#xA;&#xA;        &#xA;&#xA;&#xA;  &#xA;  &#xA;  &#xA;    &#xA;&#xA;    &#xA;      This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.&#xA;      Learn more about bidirectional Unicode characters&#xA;    &#xA;&#xA;&#xA;              Show hidden characters&#xA;&#xA;&#xA;&#xA;&#xA;  &#xA;    &#xA;    &#xA;&#xA;&#xA;&#xA;  &#xA;        &#xA;          &#xA;          List&lt;long&gt; PostIds;&#xA;        &#xA;  &#xA;&#xA;&#xA;&#xA;    &#xA;&#xA;  &#xA;&#xA;&#xA;      &#xA;      &#xA;        view raw&#xA;        &#xA;          timeline.cs&#xA;        &#xA;        hosted with &#x2764; by GitHub&#xA;      &#xA;    &#xA;&#xA;In other words, we just have a list of post ids, we add items at the end and as we scroll we keep track of where we started. That is sufficient to get us pretty much all the features that we want, surprisingly enough.When you go to the home page and look at your timeline, you&#x2019;ll typically start with whatever the latest value there, we&#x2019;ll record the last position we saw and then start scrolling backward in time. In other words, if we have 10,000 items in the timeline, we&#x2019;ll record that the position we started at was 10,000 and then start going back toward zero. If there are new items, the size of the list will increase and we can jump back to the top, etc. That is simple enough, but how does this actually help us? That may be good if I wanted to see the public timeline of the entire network, but what about the actual features. I don&#x2019;t care that a restraint in Prague is now offering discounted deliveries, for example. I care about the accounts that I follow. The idea is that we don&#x2019;t have just a single such timeline, but many. In fact, pretty much all operations in the social platform can be represented using the timeline abstraction. Let&#x2019;s consider typical usage of an account. I&#x2019;m adding posts, but I also want to be able to see people talking to me or about tags that I follow. How is that going to work?Well, I&#x2019;m actually going to have two timelines:Public Timeline &#x2013; where we&#x2019;ll add all the posts from the user, and maybe posts that mention / reply, etc. Private Timeline &#x2013; where we&#x2019;ll add posts from users that you follow, mentions, replies to discussion the user took part of, etc.In both cases, the behavior of the system is identical. We simply go through the list.&#xA0; If you&#x2019;ll recall, I left a lot unsaid when I discussed writing posts. In particular, how do I publish them to interested parties. This is where we start to apply policies. Part of the process of adding a post is to figure out what timelines it should go to. By moving most of the cost to the write side, we drastically reduce the overall complexity. Furthermore, it also make a lot more sense, given that most posts aren&#x2019;t going to have a wide blast radius. When posting a message, we need to consider the following:Is this a high impact account? (Let&#x2019;s say, &gt; 50,000 followers) If so, we&#x2019;ll have special behaviors. Who is following this user? Is this a reply to another post?Are there mentions on this post? If so, need to apply the logic based on the mention policies. As you can imagine, this is quite involved, but in general, the way it will work is something like this:&#xA;    &#xA;      &#xA;        &#xA;  &#xA;    &#xA;    &#xA;&#xA;        &#xA;&#xA;&#xA;  &#xA;  &#xA;  &#xA;    &#xA;&#xA;    &#xA;      This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.&#xA;      Learn more about bidirectional Unicode characters&#xA;    &#xA;&#xA;&#xA;              Show hidden characters&#xA;&#xA;&#xA;&#xA;&#xA;  &#xA;    &#xA;    &#xA;&#xA;&#xA;&#xA;  &#xA;        &#xA;          &#xA;          async function handle_async(post){&#xA;        &#xA;        &#xA;          &#xA;              var author = await users.get(post.author);&#xA;        &#xA;        &#xA;          &#xA;              var promises = [];&#xA;        &#xA;        &#xA;          &#xA;              // always goes on my timeline&#xA;        &#xA;        &#xA;          &#xA;              promises.push(timelines.append(author.timelines[&quot;self&quot;], post.id));&#xA;        &#xA;        &#xA;          &#xA;              if (post.has_mentions() == false){&#xA;        &#xA;        &#xA;          &#xA;                  promises.push(timelines.append(author.timelines[&quot;posts_and_replies&quot;], post.id));&#xA;        &#xA;        &#xA;          &#xA;              }&#xA;        &#xA;        &#xA;          &#xA;              else{&#xA;        &#xA;        &#xA;          &#xA;                  var mentions = post.mentions().map(m =&gt; appendToMentionTimeline(m, post));&#xA;        &#xA;        &#xA;          &#xA;                  promises = promises.concat(mentions);&#xA;        &#xA;        &#xA;          &#xA;              }&#xA;        &#xA;        &#xA;          &#xA;              if(post.is_public()){&#xA;        &#xA;        &#xA;          &#xA;                  promises.push(timelines.append(author.timelines[&quot;public&quot;], post.id));&#xA;        &#xA;        &#xA;          &#xA;              }&#xA;        &#xA;        &#xA;          &#xA;              if(post.is_reply()){&#xA;        &#xA;        &#xA;          &#xA;                  promises.push(timelines.append(post.in_reply_to, post.id));&#xA;        &#xA;        &#xA;          &#xA;              }&#xA;        &#xA;        &#xA;          &#xA;              await Promise.all(promises);&#xA;        &#xA;        &#xA;          &#xA;          }&#xA;        &#xA;  &#xA;&#xA;&#xA;&#xA;    &#xA;&#xA;  &#xA;&#xA;&#xA;      &#xA;      &#xA;        view raw&#xA;        &#xA;          post.js&#xA;        &#xA;        hosted with &#x2764; by GitHub&#xA;      &#xA;    &#xA;&#xA;The key here is the whole manner in which this works is done via selecting what we&#x2019;ll publish to. Further more, you can see that a user have multiple timelines, and in the case of a mention, we can apply additional policies to see how the post gets routed. This is complex and often changing, but it also happens a lot less often than reads. So it is a net benefit to move all the costs to the write side. Another thing to notice in this case is how we handle a reply. A reply it just appended to the timeline of the post. In other words, it is timelines all the way down. We want to have a single simple abstraction to handle as much of the system as we want. In this case, we need to handle replies on a post and that can be anything from very few to hundreds of thousands. By generating a timeline for the post as well, we can reuse all the same behaviors and it just works.As for the timeline itself? It is merely a queue of post ids, and it allows you to set a position in it in an efficient manner. The list of post ids above is how it works conceptually, but we have to think about the numbers here. How big can a timeline get?The personal timeline of a user is limited to how many posts they can make. In general, even very heavy users will not top a few hundreds a day and low thousands a month. That means that we have a good reasonable upper bound to how big the personal timeline can grow. Ten years of posting 5,000 posts a month will get you over half a million, but that I would assume be the top rate for anything that isn&#x2019;t an automated system.Your Public Timeline is impact by how many people you follow and how prolific they are. There is a natural limit to how many people an account can follow, so there is a bound here, but assuming that you follow 1000 accounts that all post 1000 posts a month, that adds up to a million posts a month. Over a ten year span, that would be 120 million posts. That said, we&#x2019;ll discuss other properties of the public timeline below.Post&#x2019;s timeline is all the replies that were made to the post. Most posts have very few replies, but some will garner a lot. It took me a minute to find a Tweet on Twitter that had close to 400,000 replies, for example. So a timeline may be big, potentially very big. However, there is an interesting issue here, how much do we actually need to keep?The purpose of the Public Timeline, for example, is to show you the front page of the site, how much data back do we need to keep? Is there a reason to keep your timeline from three years ago? The answer is probably no. We can keep the public timeline at a certain size and likely benefit from a lot of space savings. On the other hand, the replies for a post can be quite interesting, and while they can grow very big, it probably make sense to never trim them. So we have the concept of a timeline, but what is it actually going to be? In terms of REST API, we are going to have the following endpoints:GET /timelines?id=1351081943163123854GET /timelines/sections/F4BE2048BF51F3DCC69EA4CA4ED08F12A36BD6524C9F12018BA0CE6F7C076BB2In other words, we can access the timeline or a section in that timeline. I would rather show the output and then discuss what it all means. The first endpoint gives us the timeline itself, and looks like this:&#xA;    &#xA;      &#xA;        &#xA;  &#xA;    &#xA;    &#xA;&#xA;        &#xA;&#xA;&#xA;  &#xA;  &#xA;  &#xA;    &#xA;&#xA;    &#xA;      This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.&#xA;      Learn more about bidirectional Unicode characters&#xA;    &#xA;&#xA;&#xA;              Show hidden characters&#xA;&#xA;&#xA;&#xA;&#xA;  &#xA;    &#xA;    &#xA;&#xA;&#xA;&#xA;  &#xA;        &#xA;          &#xA;          {&#xA;        &#xA;        &#xA;          &#xA;           &quot;timeline_id&quot;: 12383125732,&#xA;        &#xA;        &#xA;          &#xA;           // max of 64 items&#xA;        &#xA;        &#xA;          &#xA;           &quot;sections&quot;: [&quot;F4BE2048BF51F3DCC69EA4CA4ED08F12A36BD6524C9F12018BA0CE6F7C076BB2&quot;, &quot;6C8FB5FA6A92F965919C4AFB939CBBB6D1E4D24D35A1FDC91D9130B824FB0541&quot;],&#xA;        &#xA;        &#xA;          &#xA;           &quot;posts&quot;: [12383125472,12383125631,123831248711, 12383124214 ] // max of 128 items&#xA;        &#xA;        &#xA;          &#xA;          }&#xA;        &#xA;  &#xA;&#xA;&#xA;&#xA;    &#xA;&#xA;  &#xA;&#xA;&#xA;      &#xA;      &#xA;        view raw&#xA;        &#xA;          timeline.js&#xA;        &#xA;        hosted with &#x2764; by GitHub&#xA;      &#xA;    &#xA;&#xA;There are a few things to note here. When we ask for a timeline, we get the most recent posts in the timeline, as well as the past few sections. The way a timeline works, you can always append posts ids to it, which works great, except that at some point the sheer size that is involved is starting to be problematic.If we consider a big timeline, one with 400,000 posts in it, that comes to about 3.2 MB used, just to store the post ids (8 bytes each). In practice, due to concurrency and distribution concerns, we can&#x2019;t have an actual list of post ids, so we need some better management. Another factor is that you very rarely need or want to get the entire timeline, you want to start from the top and work your way down.We can handle that easily enough using two stage approach. First, all the new post ids appended to a timeline are written in a &#x201C;loose&#x201D; form. Each one with each own entry. Once we hit a certain limit (128, for example), we know that this is likely to grow bigger. We can grab the loose post ids in the timeline and gather them into a section. A section is an independently addressed part of the timeline. The idea is that we gather all of the post ids currently loose in the timeline, write them into a single object and compress that. Then we use the hash of the resulting object to as the key to an object store.Side note, timelines are immutable. Once the section is created, it cannot change. You can add additional filtering on the timeline on read, on the other hand. The timeline also should handle the case where posts in the timeline has been deleted, since we aren&#x2019;t cannot modify it. For ease of implementation, we&#x2019;ll also allow duplication in the posts ids. Clients are expected to handle and ignore duplicate post ids that happen within a certain time range. The reason that the timeline section is compressed is to reduce the size, obviously. In my testing, I was able to get 65% reduction in size without taking any special efforts. Throwing the compressed data into object storage (S3 and the like) also means that it is much easier to scale reads on them. If we have a user who is very popular, we can move that timeline to a compression section faster to reduce load. This design explicitly acknowledge the problems with distributed systems and concurrency. It is possible that a compressed section will have an id that also appears in the loose portion of the timeline. The responsibility to handle such a scenario is on the client code, which is able to do so far more easily than the server side portion. After compressing the loose posts in the timeline, we record the new section hash and allow clients to access it. It might be easier to see how that would work in the following image:Given a post id size of 8 bytes, and assuming that we can compress it by 65% (my na&#xEF;ve tests using Brotli &amp; GZip says yes, can probably do better than that) we can state that every thousand post ids or so we can generate a new section (meaning that it would be about 2KB in size, in the end). Even a very big timeline with hundreds of thousands of entries would end up with just a few hundreds of sections at the top.The entire mechanism is very limited, quite intentionally. The external operations we allow on a timeline are append and get, with the client expected to understand the manner in which they are going to go deeper into the timeline. The limitation and expectation from the client (like allowing duplicate post ids, handling post ids that point to deleted posts, etc) are all there to make it easy to handle scaling out the system.Consider a typical use case, I go into a popular account and look at their posts. Effectively, I&#x2019;m browsing their public timeline. My interactions with the server goes like this:Get a list of the post ids in the timeline. The first step is: GET /timelines?id=1351081943163123854This gets me the list of loose post ids and the recent segments.Notice that this API call is open for caching as well, so we can get the scaling benefit of that as well.Get the actual posts, which I can do with the batch post read API that we discussed earlier.In many cases, the cost of getting the timeline for the first time will be amortize over the reading time of many posts. The bulk read API gets me 128 posts at a time, so as the user is reading, I can get the next batch ready and give them the next part immediately.Once I&#x2019;m done with the loose posts, I can go into the compressed sections and do the same there. If each section has about 1000 posts ids, that will be sufficient for quite some time. And because I&#x2019;m driving this from the client, it is very easy for me to scale. Throwing the data into object store like S3 means that I can get both CDN support easily and my scaling issue is now: &#x201C;serve a lot of small files&#x201D;, which is a very well understood problem.I still have to take into account permissions, but that is already something that we handle in the batch read API. Notice that for the common case of public posts, pretty much the whole thing has caching and distribution baked and the amount of work that we can let the rest of the system handle is very high.Hot spots in the system are going to be handled by the infrastructure, not by our own code, big machines or clever algorithms. They are going to be handled by the architecture of the system making it simple to manage them, giving plenty of room for caching and CDN to take charge and reduce our costs. That is, after all, the whole point of this series of posts.</p>
        </article>
        <article id="article-1159">
            <a href="https://ayende.com/blog/193029-C/building-a-social-media-platform-without-going-bankrupt-part-iv-caching-and-distribution" target="_blank">
                <h2 class="title mb-6" id="article-1159">Building a social media platform without going bankrupt</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: January 28, 2021
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">In my previous post we looked at the process in which we process a request for a batch of posts and get their results. The code made no assumptions about where it is running and aside from specifying whatever it is okay or not to allow caching, did no such work.Caching is important, it matters a lot for performance. To the point where if you aren&#x2019;t using caching, you are past willful neglect and in the territory of intentional malpractice. The difference can be between needing 18,500 cores to serve a website and needing less than 400&#xA0; to serve a much busier website. Except that the difference will likely be more pronounced. Because it is so important, we need to take it into account at the architecture level. Another aspect we have to consider is the data distribution. Assuming we want to build a global social media platform, that means having to access it from multiple locations. Which mean, in turn, that we have to consider the fallacies of distributed computing in our system. Locality of reference is another key factor that you have to take into account. Which means that you have to consider the flow of data in the system.Let&#x2019;s assume that we have the following datacenters around the world:We are using geo routing and the relevant infrastructure to make sure that you&#x2019;ll always hit the nearest data center to you. Let&#x2019;s say that we have a Mr. Beat in our social platform, who is very popular and like to post controversial messages such as the need to abolish peppers from your menus. Mr. Beat is located in Australia, so when he is posting yet another &#x201C;peppers have no place in the kitchen&#x201D; post, the data center in Brisbane is going to be the one to field the request.A system like that would do best if we can avoid any and all required coordination between the different data centers, as such, we are going to be using gossip to share the results among all the data centers. In other words, the post will be written in the Brisbane data center and then replicated to the rest of the data centers. There are several ways that we can implement such a feature.The simplest way to replicate all the data to all the data centers. This way, we can always access it from the local store. However, that present two challenges:First, there is latency involved with replicating information across the glove. We may get a request for a post in the Odessa data center for a post originating in Brisbane. If the network devils decided to have a party, we may not have that particular post yet in Odessa. What do you do then? This is where the format of the post id come into play. If we can&#x2019;t find the post in our local storage, we can figure out who owns that (based on the machine id segment) and go ask the owning data center. The second problem is that in many cases, the data is purely local. For example, consider this Twitter account. For the most part, everyone that is following that is going to be located nearby. I assume you may have a very small minority of followers who left the area who are still interested in following up on what is going on, but for the most part, this is not that common. That means that replicating all the information to all the data centers is likely a waste.Given these two facts together, we are actually better off using a different model for data distribution and caching. The post id is generated using the following mechanism:The machine segment in the post id (10 bits long) gives us a good indication of where that post was created. This is originally used only for the purpose of generating unique ids, but we can make far greater usage of this. We decide that the data center that created the post is also the one that owns it. In other words, Mr. Beat&#x2019;s posts are &#x201C;owned&#x201D; by the Brisbane data center. The actual posts are held in a key/value store, but there is a difference in how we do lookup by id based on the ownership. Here is the relevant code:&#xA;    &#xA;      &#xA;        &#xA;  &#xA;    &#xA;    &#xA;&#xA;        &#xA;&#xA;&#xA;  &#xA;  &#xA;  &#xA;    &#xA;&#xA;    &#xA;      This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.&#xA;      Learn more about bidirectional Unicode characters&#xA;    &#xA;&#xA;&#xA;              Show hidden characters&#xA;&#xA;&#xA;&#xA;&#xA;  &#xA;    &#xA;    &#xA;&#xA;&#xA;&#xA;  &#xA;        &#xA;          &#xA;          async function get(id: int64){&#xA;        &#xA;        &#xA;          &#xA;              let [exists, value] = await kv.get(id);&#xA;        &#xA;        &#xA;          &#xA;              if(!exists){&#xA;        &#xA;        &#xA;          &#xA;                  let owner = identifier.get_owner(id);&#xA;        &#xA;        &#xA;          &#xA;                  if(owner == data_center.id()) {&#xA;        &#xA;        &#xA;          &#xA;                      return null; // missing and we are the owner, does not exists&#xA;        &#xA;        &#xA;          &#xA;                  }&#xA;        &#xA;        &#xA;          &#xA;                  // need to check other data center&#xA;        &#xA;        &#xA;          &#xA;                  let [_, remote_val] = await data_center.get_kv(owner).get(id);&#xA;        &#xA;        &#xA;          &#xA;                  await kv.put(id, remote_val); // always insert, even if we got null&#xA;        &#xA;        &#xA;          &#xA;                  value = remote_val;&#xA;        &#xA;        &#xA;          &#xA;              }&#xA;        &#xA;        &#xA;          &#xA;              return value;&#xA;        &#xA;        &#xA;          &#xA;          }&#xA;        &#xA;  &#xA;&#xA;&#xA;&#xA;    &#xA;&#xA;  &#xA;&#xA;&#xA;      &#xA;      &#xA;        view raw&#xA;        &#xA;          get.js&#xA;        &#xA;        hosted with &#x2764; by GitHub&#xA;      &#xA;    &#xA;&#xA;The idea is that we first check the key/value in the current data center for the post id. If it isn&#x2019;t found, we check if we are the owner of the post. If so, it doesn&#x2019;t exists or was removed. If it belongs to another data center, we&#x2019;ll go and fetch it from there. Note that we&#x2019;ll record a null if needed, so we&#x2019;ll not need to go and fetch a missing value each time it is requested. In other words, the first time that an item is requested from the owner data center, we&#x2019;ll place it in our own key/value for next time. We do so with an indication to the key/value that this is a remote value. Remote values may be purge early or be put on a least frequently viewed rotation, etc. There are other things that we need to deal with, of course. Concurrency, for example, if we have multiple concurrent requests to the same missing id. We&#x2019;ll have multiple chats between data centers to fetch the same value. I don&#x2019;t think that this is a problem. That is likely to only happen the first time, and then it is cached. We might want to monitor how often remote values are requested and only get them after a certain number of requests, but in all honesty, it probably doesn&#x2019;t matter.The key/value store architecture is already likely to cause us to use both disk and memory. We can take advantage of the fact that the remote key isn&#x2019;t important locally and not persist it to disk right away, or not in a durable format. When we need to remove the value from memory, we can see if it had enough hits to warrant writing locally or not.The most complex issue, however, is related to the cache itself. We have a strong ownership model, in which the data center that created a post is its owner. What happens when we need to update a post?Twitter, for example, doesn&#x2019;t have an Edit feature. That is a great reduction of the complexity we have to deal with, but it isn&#x2019;t all that simple. An update to the post can also be a delete. For example, let&#x2019;s say that Mr. Beat posted: &#x201C;eating steamed Br0ccoli&#x201D;. Such a violation of community standards cannot stand. Even though Mr. Beat cleverly disguised his broccoli tendencies by typo-ing the forbidden term. Mr. Beat is very popular and his posts have likely spread across many data centers. An admin marking the post as deleted also has to deal with the possibility that the post is located on other locations as well.We can try to keep track of this, but to be fair, it is easier to simply queue a delete command on all the data centers except the owner. That will ensure that they will remove the cached version and have to re-read it from the owner data center.Everything that I describe so far was about behavior, but we also ought to talk about policies. As part of the work we do when writing a post, we can apply all sort of interesting policies. For example, we may know that Mr. Beat is popular globally and as part of writing a post from him preemptively send that post to all the other data centers. If we have an update to a post, instead of sending a delete command to the rest of the data centers and let it refresh automatically, we can send the updated post content immediately.I intentionally don&#x2019;t want to dig too deeply into those policies, because they are important, but they aren&#x2019;t on the same level as the infrastructure I describe here is. Those are like the cherry on top, if you like such a thing, it can take something good and make it great. But given that those are policies that can be applied on a per item basis and modified as you go along, there isn&#x2019;t a reason to start going there yet.Finally, there is a last aspect to discuss: Expiry.&#xA0; In most social media, the now is important beyond all else. In other words, you are very unlikely to be seeing posts from two years ago and if you are, it matters a lot less if you have to deal with slightly higher latency. Expiring remote content that is over 3 months old, for example, and not placing that in our local key/value at all can be a great way to handle long tail issues. For that matter, given that old content is rarely accessed, we can also optimize our storage by compressing old posts instead of holding on to them directly. And after all of this discussion, I wanted to point out that you are also likely to want to have another layer of caching in place. The API calls you make in many cases may be good candidate for at least short term caching. In other words, if you put them behind something like Cloudflare, given that we explicitly state what post ids we want, we can set a cache duration of 1 &#x2013; 3 minutes without needing to worry too much about updates. That can massively reduce the number of requests that we actually have to handle, and it costs a lot less. Under what scenarios would that be useful?Consider people going to view a popular user, such as Mr. Beat&#x2019;s page. The list of posts there is going to be the same for anyone, and even a short duration on the cache would massively help our load.As you can see, the design of the system assume caching and actively work to make it possible for you to utilize the cache at multiple levels.</p>
        </article>
        <article id="article-1160">
            <a href="https://ayende.com/blog/193028-C/building-a-social-media-platform-without-going-bankrupt-part-iii-reading-posts" target="_blank">
                <h2 class="title mb-6" id="article-1160">Building a social media platform without going bankrupt</h2>
            </a>
            <p class="mb-2">by Oren Eini</p>
            <p class="mb-6 flex gap-1.5">
                    <span>
                        <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                             xmlns="http://www.w3.org/2000/svg"><path
                                xmlns="http://www.w3.org/2000/svg"
                                d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                    </span>
                posted on: January 27, 2021
            </p>
            <p class="max-w-full w-full line-clamp-5 text-justify mb-20">So far in this series of posts I looked into how we write posts. The media goes to S3 compatible API and the posts themselves will go to&#xA0; a key/value store. Reading them back, on the other hand, isn&#x2019;t that simple. For the media, I&#x201D;m going to assume that the S3 is connected to CDN and that is handled, but I want to focus on the manner in which we deal with reading posts. In particular, I&#x2019;m not talking here about how we can display the timeline for a user. That is going to be the subject on another post, right now, I&#x2019;m assuming that this is handled and talking about the next step. We have a list of post ids that we want to get and we need to manage that.&#xA;The endpoint in question would look like this:&#xA;&#xA;GET /api/v1/read?post=1352410889870336005&amp;post=1351081958063951875&#xA;&#xA;The result of this API is a JSON object with the keys as the posts ids and the values as the content of the post.&#xA;This simple API is about as simple as you can imagine, but even from this simple scenario you can see a few interesting details:&#xA;&#xA;The API is using GET, which means that there is a natural limit to the size of the URL. This is good and by design. We will likely limit this to a maximum of 128 items as a time anyway.&#xA;The API is inherently about dealing with batches of information.&#xA;The media is handled separately (generated directly from the client) so we can return far less information.&#xA;&#xA;In many cases, this is going to be a unique set of posts, for example, when you view your timeline, it is likely that you&#x2019;ll see a unique set of posts. However, in many other cases, you&#x2019;ll send a request that is similar or identical to what others will use.&#xA;When you are looking at a popular thread, for example, you&#x2019;ll be asking the same posts ids as everyone else, which means that there is a good chance to easily add caching for this via CDN or the like and benefit greatly as a result.&#xA;Internally, the implementation of this API is probably just going to issue direct reads by ids to the key/value store and just return the result. There should be a minimal amount of processing involved, usually, except for one factor, authorization.&#xA;Assuming that the key/value interface has a get(id) method, the backend code for this API critical API should be something like the code below. Note that this is&#xA0;server side code, I&#x27;m not showing any client side code in this series of posts. This is the backend code to handle address the reading of a batch of ids from the client.&#xA;&#xA;    &#xA;      &#xA;        &#xA;  &#xA;    &#xA;    &#xA;&#xA;        &#xA;&#xA;&#xA;  &#xA;  &#xA;  &#xA;    &#xA;&#xA;    &#xA;      This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.&#xA;      Learn more about bidirectional Unicode characters&#xA;    &#xA;&#xA;&#xA;              Show hidden characters&#xA;&#xA;&#xA;&#xA;&#xA;  &#xA;    &#xA;    &#xA;&#xA;&#xA;&#xA;  &#xA;        &#xA;          &#xA;          async function handle_async(user_id: string, post_ids: [int64]) {&#xA;        &#xA;        &#xA;          &#xA;              if(post_ids.length &gt; 128) throw &quot;too many post ids&quot;;&#xA;        &#xA;        &#xA;          &#xA;              &#xA;        &#xA;        &#xA;          &#xA;              let results = (await Promise.all(post_ids.map(id =&gt; kv.get(id))));&#xA;        &#xA;        &#xA;          &#xA;              let filter = null;&#xA;        &#xA;        &#xA;          &#xA;              for (let i = 0; i &lt; results.length; i&#x2B;&#x2B;) {&#xA;        &#xA;        &#xA;          &#xA;                  const post = results[i];&#xA;        &#xA;        &#xA;          &#xA;                  if (!post || post.is_public) continue;&#xA;        &#xA;        &#xA;          &#xA;                  if(!filter){&#xA;        &#xA;        &#xA;          &#xA;                      filter = {by_user: {}, promises: []};&#xA;        &#xA;        &#xA;          &#xA;                  }&#xA;        &#xA;        &#xA;          &#xA;                  let promise = filter.by_user[user_id] || auth.is_follower_async(post.author, user_id);&#xA;        &#xA;        &#xA;          &#xA;                  filter.by_user[user_id] = promise;&#xA;        &#xA;        &#xA;          &#xA;                  filter.promises.push(promise.then(r =&gt; results[i] = r ? post : null));&#xA;        &#xA;        &#xA;          &#xA;              }&#xA;        &#xA;        &#xA;          &#xA;              if(filter){&#xA;        &#xA;        &#xA;          &#xA;                  await Promise.all(filter.promises);&#xA;        &#xA;        &#xA;          &#xA;              }&#xA;        &#xA;        &#xA;          &#xA;              return { &#xA;        &#xA;        &#xA;          &#xA;                  cache: filter == null,&#xA;        &#xA;        &#xA;          &#xA;                  results: results.filter(x=&gt;x!=null)&#xA;        &#xA;        &#xA;          &#xA;              };&#xA;        &#xA;        &#xA;          &#xA;          }&#xA;        &#xA;  &#xA;&#xA;&#xA;&#xA;    &#xA;&#xA;  &#xA;&#xA;&#xA;      &#xA;      &#xA;        view raw&#xA;        &#xA;          fetch_posts.js&#xA;        &#xA;        hosted with &#x2764; by GitHub&#xA;      &#xA;    &#xA;&#xA;&#xA;The code itself assumes that there is no meaning to doing batch operation on the key/value itself, mind. That isn&#x2019;t always the case, but I&#x2019;ll assume that. We issue N async promises to the key/value and wait to get them all back. This assumes that the latency from the API node to the key/value servers is minimal and let us batch a lot of remote calls into near calls.&#xA;The vast majority of the function is dedicated to the auth behavior. A post can be marked as public or protected, and if it is the later, we need to ensure that only people that the author of the post follow will be able to see this. You&#x2019;ll note that I&#x2019;m doing a lot of stuff in an async manner here. For example, we&#x2019;ll only issue a single check per post author and we can safely assume that most posts are public anyway. I&#x2019;m including the &#x201C;full&#x201D; code here to give you an indication about the level of complexity that I would expect to see in the API.&#xA;You should also note that we indicate whatever we allow to cache the results or not. In the case of a request that include a protected post, we don&#x2019;t allow it. But for the most part, we can expect to see high percentage of posts that would be only public and can benefit from that.&#xA;Because we are running in a distributed system, we also have to take into account all sort of interesting race conditions. For example, you may be trying to read a post that has been removed. We explicitly clear all such null items from the results. Another way to handle that is to replace the content of the post and set a marker flag, but we&#x2019;ll touch that on another post.&#xA;Finally, the code above doesn&#x2019;t handle caching or distribution. That is going to be handled both above and below this code. I&#x2019;ll have a dedicated post around that tomorrow.</p>
        </article>
        <div class="button flex justify-between">
            <a href="115.html"><span class="back arrow"></span></a>

            <a href="117.html"><span class="next arrow"></span></a>
        </div>
    </section>
</main>
<footer
    class="mt-auto flex w-full flex-col items-center justify-center gap-y-2 pb-4 pt-20 text-center align-top font-semibold text-gray-600 dark:text-gray-400 sm:flex-row sm:justify-between sm:text-xs">
    <div class="me-0 sm:me-4">
        <div class="flex flex-wrap items-end gap-x-2">
            <ul class="flex flex-1 items-center gap-x-2 sm:flex-initial">
                <li class="flex">
                    <p class="flex items-end gap-2 justify-center flex-wrap	">Â© Relatively General
                        .NET 2025<span
                            class="inline-block">&nbsp;ðŸš€&nbsp;Theme: Astro Cactus</span>

                        <a class="inline-block sm:hover:text-link" href="https://github.com/chrismwilliams/astro-cactus"
                           rel="noopener noreferrer " target="_blank">
                            <svg width="1em" height="1em" viewBox="0 0 24 24" aria-hidden="true" class="h-6 w-6"
                                 focusable="false" data-icon="mdi:github">
                                <symbol id="ai:mdi:github">
                                    <path fill="currentColor"
                                          d="M12 2A10 10 0 0 0 2 12c0 4.42 2.87 8.17 6.84 9.5c.5.08.66-.23.66-.5v-1.69c-2.77.6-3.36-1.34-3.36-1.34c-.46-1.16-1.11-1.47-1.11-1.47c-.91-.62.07-.6.07-.6c1 .07 1.53 1.03 1.53 1.03c.87 1.52 2.34 1.07 2.91.83c.09-.65.35-1.09.63-1.34c-2.22-.25-4.55-1.11-4.55-4.92c0-1.11.38-2 1.03-2.71c-.1-.25-.45-1.29.1-2.64c0 0 .84-.27 2.75 1.02c.79-.22 1.65-.33 2.5-.33s1.71.11 2.5.33c1.91-1.29 2.75-1.02 2.75-1.02c.55 1.35.2 2.39.1 2.64c.65.71 1.03 1.6 1.03 2.71c0 3.82-2.34 4.66-4.57 4.91c.36.31.69.92.69 1.85V21c0 .27.16.59.67.5C19.14 20.16 22 16.42 22 12A10 10 0 0 0 12 2"></path>
                                </symbol>
                                <use xlink:href="#ai:mdi:github"></use>
                            </svg>
                            <span class="sr-only">Github</span>
                        </a>
                    </p>
                </li>
            </ul>
        </div>
    </div>
    <nav aria-label="More on this site" class="flex gap-x-2 sm:gap-x-0 sm:divide-x sm:divide-gray-500">
        <a class="px-4 py-2 sm:py-0 sm:hover:text-textColor sm:hover:underline" href="index.html"> Home </a><a
            class="px-4 py-2 sm:py-0 sm:hover:text-textColor sm:hover:underline" href="/about/"> About </a>
    </nav>
</footer>
<script src="js/script.js?id=af8f4559935e7bf5bf6015373793411d"></script>
<script src="pagefind/pagefind-ui.js"></script>
</body>
</html>