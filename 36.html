<!DOCTYPE html>
<html class="scroll-smooth" lang="en-US" data-theme="light">
<head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, shrink-to-fit=no" name="viewport">
    <meta content="IE=edge" http-equiv="X-UA-Compatible">
    <title>Home â€¢ Relatively General .NET</title>
    <link href="favicon.ico" rel="icon" sizes="any">
    <link href="images/icon.svg" rel="icon" type="image/svg+xml">
    <link href="images/apple-touch-icon.png" rel="apple-touch-icon">
    <link rel="stylesheet" href="css/_slug_.upexPNXs.css">
</head>
<body class="mx-auto flex min-h-screen max-w-3xl flex-col bg-bgColor px-4 pt-16 font-mono text-sm font-normal text-textColor antialiased sm:px-8">
<script>
    const lightModePref = window.matchMedia("(prefers-color-scheme: light)");

    function getUserPref() {
        const storedTheme = typeof localStorage !== "undefined" && localStorage.getItem("theme");
        return storedTheme || (lightModePref.matches ? "light" : "dark");
    }

    function setTheme(newTheme) {
        if (newTheme !== "light" && newTheme !== "dark") {
            return console.warn(
                `Invalid theme value '${newTheme}' received. Expected 'light' or 'dark'.`,
            );
        }

        const root = document.documentElement;

        // root already set to newTheme, exit early
        if (newTheme === root.getAttribute("data-theme")) {
            return;
        }

        root.setAttribute("data-theme", newTheme);

        const colorThemeMetaTag = document.querySelector("meta[name='theme-color']");
        const bgColour = getComputedStyle(document.body).getPropertyValue("--theme-bg");
        colorThemeMetaTag.setAttribute("content", `hsl(${bgColour})`);
        if (typeof localStorage !== "undefined") {
            localStorage.setItem("theme", newTheme);
        }
    }

    // initial setup
    setTheme(getUserPref());

    document.addEventListener("DOMContentLoaded", function () {
        document.getElementById("theme-toggle").addEventListener("click", () => {
            const theme = localStorage.getItem("theme");

            if (theme === "dark") {
                setTheme("light");
            } else {
                setTheme("dark");
            }
        });

        document.getElementById("toggle-navigation-menu").addEventListener("click", (e) => {
            const button = e.currentTarget;
            const ariaExpanded = button.getAttribute("aria-expanded");
            const header = document.getElementById("main-header");

            if (ariaExpanded === "true") {
                button.setAttribute("aria-expanded", "false");
                header.classList.remove("menu-open");
            } else {
                button.setAttribute("aria-expanded", "true");
                header.classList.add("menu-open");
            }
        });
    });
</script>

<a class="sr-only focus:not-sr-only focus:fixed focus:start-1 focus:top-1.5" href="#main">
    skip to content
</a>
<header class="group relative mb-28 flex items-center sm:ps-[4.5rem]" id="main-header">
    <div class="flex sm:flex-col">
        <a aria-current="page" class="inline-flex items-center hover:filter-none sm:relative sm:inline-block"
           href="index.html">
            <img class="me-3 sm:absolute sm:start-[-4.5rem] sm:me-0 sm:h-16 sm:w-16" src="images/giphy.gif" alt=""
                 style="width: 4rem"/>
            <span class="text-xl font-bold sm:text-2xl">Relatively General .NET</span>
        </a>
        <nav aria-label="Main menu"
             class="absolute -inset-x-4 top-14 hidden flex-col items-end gap-y-4 rounded-md bg-bgColor/[.85] py-4 text-accent shadow backdrop-blur group-[.menu-open]:z-50 group-[.menu-open]:flex sm:static sm:z-auto sm:-ms-4 sm:mt-1 sm:flex sm:flex-row sm:items-center sm:divide-x sm:divide-dashed sm:divide-accent sm:rounded-none sm:bg-transparent sm:py-0 sm:shadow-none sm:backdrop-blur-none"
             id="navigation-menu">
            <a aria-current="page" class="px-4 py-4 underline-offset-2 sm:py-0 sm:hover:underline"
               href="index.html"> Home </a><a
                class="px-4 py-4 underline-offset-2 sm:py-0 sm:hover:underline" href="/about/">
                About </a>
        </nav>
    </div>
    <theme-toggle class="ms-auto">
        <button id="theme-toggle" class="relative h-9 w-9 rounded-md p-2 ring-zinc-400 transition-all hover:ring-2"
                type="button">
            <span class="sr-only">Dark Theme</span>
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-7 w-7 -translate-x-1/2 -translate-y-1/2 scale-100 opacity-100 transition-all dark:scale-0 dark:opacity-0"
                 fill="none" focusable="false" id="sun-svg" stroke-width="1.5" viewBox="0 0 24 24"
                 xmlns="http://www.w3.org/2000/svg">
                <path
                    d="M12 18C15.3137 18 18 15.3137 18 12C18 8.68629 15.3137 6 12 6C8.68629 6 6 8.68629 6 12C6 15.3137 8.68629 18 12 18Z"
                    stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M22 12L23 12" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M12 2V1" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M12 23V22" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M20 20L19 19" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M20 4L19 5" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M4 20L5 19" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M4 4L5 5" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
                <path d="M1 12L2 12" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path>
            </svg>
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-7 w-7 -translate-x-1/2 -translate-y-1/2 scale-0 opacity-0 transition-all dark:scale-100 dark:opacity-100"
                 fill="none" focusable="false" id="moon-svg" stroke="currentColor" stroke-width="1.5"
                 viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M0 0h24v24H0z" fill="none" stroke="none"></path>
                <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z"></path>
                <path d="M17 4a2 2 0 0 0 2 2a2 2 0 0 0 -2 2a2 2 0 0 0 -2 -2a2 2 0 0 0 2 -2"></path>
                <path d="M19 11h2m-1 -1v2"></path>
            </svg>
        </button>
    </theme-toggle>
    <mobile-button>
        <button aria-expanded="false" aria-haspopup="menu" aria-label="Open main menu"
                class="group relative ms-4 h-7 w-7 sm:invisible sm:hidden" id="toggle-navigation-menu" type="button">
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-full w-full -translate-x-1/2 -translate-y-1/2 transition-all group-aria-expanded:scale-0 group-aria-expanded:opacity-0"
                 fill="none" focusable="false" id="line-svg" stroke="currentColor" stroke-width="1.5"
                 viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M3.75 9h16.5m-16.5 6.75h16.5" stroke-linecap="round" stroke-linejoin="round"></path>
            </svg>
            <svg aria-hidden="true"
                 class="absolute start-1/2 top-1/2 h-full w-full -translate-x-1/2 -translate-y-1/2 scale-0 text-accent opacity-0 transition-all group-aria-expanded:scale-100 group-aria-expanded:opacity-100"
                 fill="none" focusable="false" id="cross-svg" stroke="currentColor" stroke-width="1.5"
                 viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M6 18L18 6M6 6l12 12" stroke-linecap="round" stroke-linejoin="round"></path>
            </svg>
        </button>
    </mobile-button>
</header>
<main id="main">
    <section aria-label="Blog post list">
        <a href="https://www.stevejgordon.co.uk/asp-net-identity-core-under-the-hood-part-1" target="_blank"><h1 class="title mb-6">ASP.NET Identity Core 1.0.0 (Under the Hood)</h1></a>
        <p class="mb-2">by Steve Gordon</p>
        <p class="mb-6 flex" style="gap: 0.3rem">
                <span>
                                <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                                     xmlns="http://www.w3.org/2000/svg"><path
                                        xmlns="http://www.w3.org/2000/svg"
                                        d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                </span>
            posted on: January 24, 2016
        </p>
        <p class="max-w-full" style="margin-bottom: 5rem; text-align: justify">
            Let me prefix this post / series of blog posts with two important notes. I am not a security expert. This series of posts records my own dive into the ASP.NET Identity Core code, publicly available on GitHub which I&amp;#8217;ve done for my own self-interest to try and understand how it works and what is [&amp;hellip;]
        </p>
        <a href="https://www.stevejgordon.co.uk/introducing-my-blog" target="_blank"><h1 class="title mb-6">Introducing My Blog</h1></a>
        <p class="mb-2">by Steve Gordon</p>
        <p class="mb-6 flex" style="gap: 0.3rem">
                <span>
                                <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                                     xmlns="http://www.w3.org/2000/svg"><path
                                        xmlns="http://www.w3.org/2000/svg"
                                        d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                </span>
            posted on: January 21, 2016
        </p>
        <p class="max-w-full" style="margin-bottom: 5rem; text-align: justify">
            I&amp;#8217;ve been thinking about creating a blog on and off for a year or so. I like the idea of sharing what I learn&#xA0;and if nothing else it gives me somewhere to write stuff down that I might later need. I&amp;#8217;ve had a blog before and finding the time and discipline to keep it going [&amp;hellip;]
        </p>
        <a href="https://ayende.com/blog/201569-A/improving-ravendbs-node-js-bulk-insert-performance" target="_blank"><h1 class="title mb-6">Improving RavenDB&amp;#39;s Node.js bulk insert performance</h1></a>
        <p class="mb-2">by Oren Eini</p>
        <p class="mb-6 flex" style="gap: 0.3rem">
                <span>
                                <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                                     xmlns="http://www.w3.org/2000/svg"><path
                                        xmlns="http://www.w3.org/2000/svg"
                                        d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                </span>
            posted on: August 08, 2024
        </p>
        <p class="max-w-full" style="margin-bottom: 5rem; text-align: justify">
            &#xD;&#xA;&#x9;&#x9;&#x9;&#x9;During a performance evaluation internally, we ran into a strange situation. Our bulk insert performance using the node.js API was significantly&amp;nbsp;worse than the performance of other clients. In particular, when we compared that to the C# version, we saw that the numbers were significantly&amp;nbsp;worse than expected.To be fair, this comparison is made between our C# client, which has been through the wringer&amp;nbsp;in terms of optimization and attention to performance, and the Node.js client. The focus of the Node.js client was on correctness and usability. It isn&amp;rsquo;t fair to expect the same performance from Node.js and C#, after all. However, that difference in performance was annoying enough to make us take a deeper look into what was going on.Here is the relevant code:const store = new DocumentStore(&#x27;http://localhost:8080&#x27;, &#x27;bulk&#x27;);&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;store.initialize();&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;const bulk = store.bulkInsert();&#xD;&#xA;for (let i = 0; i &amp;lt; 100_000_000; i&#x2B;&#x2B;) {&#xD;&#xA;    await bulk.store(new User(&#x27;user&#x27; &#x2B; i));&#xD;&#xA;}&#xD;&#xA;await bulk.finish();As you can see, the Node.js numbers are respectable. Running at a rate of over 85,000 writes per second is nothing to sneeze at.But I also ran the exact same test with the C# client, and I got annoyed. The C# client was able to hit close to 100,000 more&amp;nbsp;writes per second than the Node.js client. And in both cases, the actual limit was on the client side, not on the server side. For fun, I ran a few clients and hit 250,000 writes/second without really doing much. The last time we properly tested ingest performance for RavenDB we achieved 150,000 writes/second. So it certainly looks like we are performing significantly better.Going back to the Node.js version, I wanted to know what exactly was the problem that we had there. Why are we so much slower than the C# version? It&amp;rsquo;s possible that this is just the limits of the node.js platform, but you gotta check to know. Node.js has an --inspect flag that you can use, and Chrome has a built-in profiler (chrome://inspect) that can plug into that. Using the DevTools, you can get a performance profile of a Node.js process.I did just that and go the following numbers:That is&amp;hellip; curious. Really&amp;nbsp;curious, isn&amp;rsquo;t it?Basically, none of my code appears here at&amp;nbsp;all, most of the time is spent dealing with the async machinery. If you look at the code above, you can see that we are issuing an await&amp;nbsp;for each document stored. &amp;nbsp;The idea with bulk insert is that under the covers, we split the writing to an in-memory buffer and the flushing of the buffer to the network. In the vast majority of cases, we&amp;rsquo;ll not&amp;nbsp;do any async operations in the store()&amp;nbsp;call. If the buffer is full, we&amp;rsquo;ll need to flush it to the network, and that may force us to do an actual await&amp;nbsp;operation. In Node.js, awaiting an async function that doesn&amp;rsquo;t actually perform any async operation appears to be super&amp;nbsp;expensive.We threw around a bunch of ideas on how to resolve this issue. The problem is that Node.js has no equivalent to C#&amp;rsquo;s ValueTask.&amp;nbsp;We also have a lot of existing code out there in the field that we must remain compatible with.Our solution to this dilemma was to add another function that you can call, like so:for (let i = 0; i &amp;lt; 100_000_000; i&#x2B;&#x2B;) {&#xD;&#xA;    const user = new User(&#x27;user&#x27; &#x2B; i);&#xD;&#xA;    const id = &quot;users/&quot; &#x2B; i;&#xD;&#xA;    if (bulk.tryStoreSync(user, id) == false) {&#xD;&#xA;        await bulk.store(user, id);&#xD;&#xA;    }&#xD;&#xA;}The idea is that if you call tryStoreSync()&amp;nbsp;we&amp;rsquo;ll try to do everything in memory, but it may not be possible (e.g. if we need to flush the buffer). In that case, you&amp;rsquo;ll need to call the async function store()&amp;nbsp;explicitly. Given that the usual reason for using the dedicated API for bulk insert is performance, this looks like a reasonable thing to ask. Especially when you can see the actual performance results. We are talking about over 55%(!!!)&amp;nbsp;improvement in the performance of bulk insert.It gets even better. That was just the mechanical fix to avoid generating a promise per operation. While we are addressing this performance issue, there are a few other low-hanging fruits that could improve the bulk insert performance in Node.js. For example, it turns out that we pay a hefty cost to generate the metadata for all those documents (runtime reflection cost, mostly). We can generate it once&amp;nbsp;and be done with it, like so:const bulk = store.bulkInsert();&#xD;&#xA;const metadata = {&#xD;&#xA;    &quot;@collection&quot;: &quot;Users&quot;,&#xD;&#xA;    &quot;Raven-Node-Type&quot;: &quot;User&quot;&#xD;&#xA;};&#xD;&#xA;for (let i = 0; i &amp;lt; 100_000_000; i&#x2B;&#x2B;) {&#xD;&#xA;    const user = new User(&#x27;user&#x27; &#x2B; i);&#xD;&#xA;    const id = &quot;users/&quot; &#x2B; i;&#xD;&#xA;    if (bulk.tryStoreSync(user, id, metadata) == false) {&#xD;&#xA;        await bulk.store(user, id, metadata);&#xD;&#xA;    }&#xD;&#xA;}&#xD;&#xA;await bulk.finish();And this code in particular gives us:That is basically near enough to the C#&amp;rsquo;s speed that I don&amp;rsquo;t think we need&amp;nbsp;to pay more attention to performance. Overall, that was time very well spent in making things go fast.&#xD;&#xA;&#xD;&#xA;&#x9;&#x9;&#x9;
        </p>
        <a href="https://ayende.com/blog/201537-A/legacy-code-with-really-good-tests-is-still-legacy-code" target="_blank"><h1 class="title mb-6">Legacy code with really good tests is still legacy code</h1></a>
        <p class="mb-2">by Oren Eini</p>
        <p class="mb-6 flex" style="gap: 0.3rem">
                <span>
                                <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                                     xmlns="http://www.w3.org/2000/svg"><path
                                        xmlns="http://www.w3.org/2000/svg"
                                        d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                </span>
            posted on: August 05, 2024
        </p>
        <p class="max-w-full" style="margin-bottom: 5rem; text-align: justify">
            &#xD;&#xA;&#x9;&#x9;&#x9;&#x9;I got into an interesting discussion on LinkedIn&amp;nbsp;about my previous post, talking about Code Rot. I was asked about Legacy Code defined as code without tests and how I reconcile code rot with having tests.I started to reply there, but it really got out of hand and became its own post.&amp;ldquo;To me, legacy code is simply code without tests.&amp;rdquo; Michael Feathers, Working Effectively with Legacy CodeI read Working Effectively with Legacy Code for the first time in 2005 or thereabout, I think. It left a massive&amp;nbsp;impression on me and on the industry at large. The book is one of the reasons I started rigorously writing tests for my code, it got me interested in mocking and eventually led me to writing Rhino Mocks. It is ironic that the point of this post is that I disagree with this statement by Michael because&amp;nbsp;of Rhino Mocks. Let&amp;rsquo;s start with numbers, last commit to the Rhino Mocks repository was about a decade ago. It has just under 1,000 tests and code coverage that ranges between 95% - 100%. I can modify this codebase with confidence, knowing that I will not break stuff unintentionally. The design of the code is very explicitly meant to aid&amp;nbsp;in testing and the entire project was developed with a Test First mindset. I haven&amp;rsquo;t touched the codebase in a decade (and it has been close to 15 years since I really delved into it). The code itself was written in .NET 1.1 around the 2006 timeframe. It literally predates generics in .NET. It compiles and runs all tests when I try to run it, which is great. But it is still very much a legacy codebase. It is a legacy codebase because changing this code is a big undertaking. This code will not&amp;nbsp;run on modern systems. We need to address issues related to dynamic code generation between .NET Framework and .NET. That in turn requires a high level of expertise and knowledge. I&amp;rsquo;m fairly certain that given enough time and effort, it is possible&amp;nbsp;to do so. The problem is that this will now require me to reconstitute my understanding of the code. The tests are going to be invaluable for actually making those changes, but the core issue is that a lot of knowledge has been lost. It will be a Project just to get it back to a normative state. This scenario is pretty interesting because I am actually looking back at my own project. Thinking about having to do the same to a similar project from someone else&amp;rsquo;s code is an even bigger challenge.Legacy code, in this context, means that there is a huge amount of effort required to start moving the project along. Note that if we had kept the knowledge and information within&amp;nbsp;the same codebase, the same process would be far cheaper and easier.Legacy code isn&amp;rsquo;t about the state of the codebase in my eyes, it is about the state of the team&amp;nbsp;maintaining it. The team, their knowledge, and expertise, are far more important than the code itself.An orphaned codebase, one that has no one to take care of, is a legacy project even if it has tests. Conversely, a project with no tests but with an actively knowledgeable team operating on it is not. Note that I absolutely agree that tests are crucial regardless. The distinction that I make between legacy projects and non-legacy projects is whether we can deliver a change to the system. Reminder: A codebase that isn&amp;rsquo;t being actively maintained and&amp;nbsp;has no tests is the worst thing of all. If you are in that situation, go read Working Effectively with Legacy Code, it will be a lifesaver.I need a feature with an ideal cost of X (time, materials, effort, cost, etc). A project with no tests but people familiar with it will be able to deliver it at a cost of 2-3X. A legacy project will need 10X or more. The second&amp;nbsp;feature may still require 2X from the maintained project, but only 5X from the legacy system. However, that initial cost to get things started is the killer.In other words, what matters here is the inertia, the ability to actually deliver updates to the system.&#xD;&#xA;&#xD;&#xA;&#x9;&#x9;&#x9;
        </p>
        <a href="https://ayende.com/blog/201505-B/optimizing-facets-query-performance-in-corax" target="_blank"><h1 class="title mb-6">Optimizing facets query performance in Corax</h1></a>
        <p class="mb-2">by Oren Eini</p>
        <p class="mb-6 flex" style="gap: 0.3rem">
                <span>
                                <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                                     xmlns="http://www.w3.org/2000/svg"><path
                                        xmlns="http://www.w3.org/2000/svg"
                                        d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                </span>
            posted on: July 31, 2024
        </p>
        <p class="max-w-full" style="margin-bottom: 5rem; text-align: justify">
            &#xD;&#xA;&#x9;&#x9;&#x9;&#x9;RavenDB allows you to query your data freely and cheaply. It is one of those things that makes or breaks a database, after all. After over a decade of working with Lucene as our backend indexing engine, we built Corax, a new querying &amp;amp; indexing engine that offers far better performance. Building an indexing engine is a humongous&amp;nbsp;task. It took us close to ten years from the first line of code to Corax actually shipping. But I&amp;rsquo;m really happy with the way it turned out. Building a query engine is a big task, and we focused primarily on making the most common queries fast. The issue at hand is that RavenDB has many features, and we don&amp;rsquo;t have infinite time. So for the less common features, we typically implemented them as a straightforward port from whatever Lucene is doing. One such feature is facets. Let&amp;rsquo;s say that I want to buy a jacket. There are way&amp;nbsp;too many choices, so I can use a faceted query to help me narrow it down. &amp;nbsp;Here is what this looks like in code:from Products&#xD;&#xA;where search(Description, &quot;suit jacket&quot;)&#xD;&#xA;select facet(Brand), &#xD;&#xA;       facet(Price &amp;lt; 200, &#xD;&#xA;             Price between 200 and 400, &#xD;&#xA;             Price between 400 and 800,&#xD;&#xA;             Price &gt; 800)And here is what this looks like as a website:I mentioned that we implemented some features as a straightforward port from Lucene, right?We did that because RavenDB offers very rich querying semantics, and we couldn&amp;rsquo;t spend the time to craft every single bit upfront. The idea was that we would get Corax out the door and be faster in most common scenarios, and at least at parity with everything else. It works for most scenarios, but not all of them. We recently got a query similar to the one above that was slower in Corax than in Lucene. That is usually good news since we have far more optimization opportunities in Corax. Lucene (and especially our usage of it) has already been through the wringer so many times that it is really hard to eke out any more meaningful performance gains. Corax&amp;rsquo;s architecture, on the other hand, gives us many more chances to do so.In the case of facets, the way Lucene handles that is roughly similar to this:def brand_facet(matches: List[int]):&#xD;&#xA;  facet = dict()&#xD;&#xA;  for term, docsForTerm in reader.terms(&quot;Brand&quot;):&#xD;&#xA;     facet[term] = count_intersect(matches,docsForTerm)Given the results of the query, run over all the terms for a particular field and intersect the documents for every term with the matches for the query. Lucene is able to do that efficiently because it materializes all its data into managed memory. That has costs associated with it:Higher managed memory usage (and associated GC costs)Slower initial queriesThe benefit of this approach is that many operations are simple, which is great. Corax, on the other hand, does not&amp;nbsp;materialize all its data into managed memory. It uses persistent data structures on disk (leading to reduced memory usage and faster responses on the first query).The advantage we have with Corax is that the architecture allows us to optimize a lot more deeply. In this case, however, it turned out to be unnecessary, as we are already keeping track of all the relevant information. We just needed to re-implement faceted search in a Corax-native manner.You can see the changes here. But here is the summary. For a dataset with 10,000,000 records, with hundreds of brands to facet on, we get:Yes, that isn&amp;rsquo;t a mistake. Corax is so fast here that you can barely observe it &amp;#128578;.&#xD;&#xA;&#xD;&#xA;&#x9;&#x9;&#x9;
        </p>
        <a href="https://ayende.com/blog/201473-A/with-bugs-failures-and-errors-ever-chugging-forward" target="_blank"><h1 class="title mb-6">With bugs, failures and errors: ever chugging forward</h1></a>
        <p class="mb-2">by Oren Eini</p>
        <p class="mb-6 flex" style="gap: 0.3rem">
                <span>
                                <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                                     xmlns="http://www.w3.org/2000/svg"><path
                                        xmlns="http://www.w3.org/2000/svg"
                                        d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                </span>
            posted on: July 29, 2024
        </p>
        <p class="max-w-full" style="margin-bottom: 5rem; text-align: justify">
            &#xD;&#xA;&#x9;&#x9;&#x9;&#x9;A customer called us about some pretty weird-looking numbers in their system:You&amp;rsquo;ll note that the total number of entries in the index across all the nodes does not match. Notice that node C has 1 less entry than the rest of the system. At the same time, all the indicators are green. As far as the administrator can tell, there is no issue, except for the number discrepancy. Why is it behaving in this manner? Well, let&amp;rsquo;s zoom out a bit. What are we actually&amp;nbsp;looking at here? We are looking at the state of a particular index&amp;nbsp;in a single database within a cluster of machines. When examining the index, there is no apparent problem. Indexing is running properly, after all.The actual problem was a replication issue, which prevented replication from proceeding to the third node. When looking at the index status, you can only see that the entry count is different. When we zoom out and look at the state of the cluster, we can see this:There are a few things that I want to point out in this scenario. The problem here is a pretty nasty one. All nodes are alive and well, they are communicating with each other, and any simple health check you run will give good results.However, there is a problem that prevents replication from properly flowing to node C. The actual details aren&amp;rsquo;t relevant (a bug that we fixed, to tell the complete story). The most important aspect is how RavenDB behaves in such a scenario.The cluster detected this as a problem, marked the node as problematic, and raised the appropriate alerts. As a result of this, clients would automatically be turned away from node C and use only the healthy nodes. From the customer&amp;rsquo;s perspective, the issue was never user-visible since the cluster isolated the problematic node. I had a hand in the design of this, and I wrote some of the relevant code. And I&amp;rsquo;m still looking at these screenshots with a big sense of accomplishment. This stuff isn&amp;rsquo;t easy or simple. But to an outside observer, the problem started from: why am I looking at funny numbers in the index state in the admin panel? And not at: why am I serving the wrong data to my users.The design of RavenDB is inherently paranoid. We go to a lot&amp;nbsp;of trouble to ensure that even if you run into problems, even if you encounter outright bugs (as in this case), the system as a whole would know how to deal with them and either recover or work around the issue.As you can see, live in production, it actually works and does the Right Thing for you. Thus, I can end this post by saying that this behavior makes me truly happy.&#xD;&#xA;&#xD;&#xA;&#x9;&#x9;&#x9;
        </p>
        <a href="https://ayende.com/blog/201442-B/indexing-only-recent-data-adventures-with-large-datasets-archiving" target="_blank"><h1 class="title mb-6">Indexing only recent data - adventures with large datasets &amp;amp; archiving</h1></a>
        <p class="mb-2">by Oren Eini</p>
        <p class="mb-6 flex" style="gap: 0.3rem">
                <span>
                                <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                                     xmlns="http://www.w3.org/2000/svg"><path
                                        xmlns="http://www.w3.org/2000/svg"
                                        d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                </span>
            posted on: July 26, 2024
        </p>
        <p class="max-w-full" style="margin-bottom: 5rem; text-align: justify">
            &#xD;&#xA;&#x9;&#x9;&#x9;&#x9;We recently got a support request from a user in which they had the following issue:We have an index that is using way too much disk space. We don&amp;rsquo;t need to search the entire dataset, just the most recent documents. Can we do something like this?from d in docs.Events&#xD;&#xA;where d.CreationDate &gt;= DateTime.UtcNow.AddMonths(-3)&#xD;&#xA;select new { d.CreationDate, d.Content };The idea is that only documents from the past 3 months would be indexed, while older documents would be purged from the index but still retained. The actual problem is that this is a full-text search index, and the actual data size required to perform a full-text search across the entire dataset is higher than just storing the documents (which can be easily compressed). This is a great example of an XY problem. The request was to allow access to the current date during the indexing process so the index could filter out old documents. However, that is actually something that we explicitly&amp;nbsp;prevent. The problem is that the current date isn&amp;rsquo;t really meaningful when we talk about indexing. The indexing time isn&amp;rsquo;t really relevant for filtering or operations, since it has no association with the actual data. The date of a document and the time it was indexed are completely unrelated. I might update a document (and thus re-index it) whose CreationDate is far in the past. That would filter it out from the index. However, if we didn&amp;rsquo;t&amp;nbsp;update the document, it would be retained indefinitely, since the filtering occurs only at indexing time.Going back to the XY problem, what is the user trying to solve? They don&amp;rsquo;t want to index all data, but they do want to retain it forever. So how can we achieve this with RavenDB?Data Archiving in RavenDBOne of the things we aim to do with RavenDB is ensure that we have a good fit for most common scenarios, and archiving is certainly one of them. In RavenDB 6.0 we added explicit support for Data Archiving.When you save a document, all you need to do is add a metadata element: @archive-at&amp;nbsp;and you are set. For example, take a look at the following document:{&#xD;&#xA;    &quot;Name&quot;: &quot;Wilman Kal&quot;,&#xD;&#xA;    &quot;Phone&quot;: &quot;90-224 8888&quot;,&#xD;&#xA;    &quot;@metadata&quot;: {&#xD;&#xA;        &quot;@archive-at&quot;: &quot;2024-11-01T12:00:00.000Z&quot;,&#xD;&#xA;        &quot;@collection&quot;: &quot;Companies&quot;,&#xD;&#xA;     }&#xD;&#xA;}This document is set to be archived on Nov 1st, 2024. What does that mean? From that day on, RavenDB will automatically mark it as an archived document, meaning it will be stored in a compressed format and excluded from indexing by default.In fact, this exact scenario is detailed&amp;nbsp;in the documentation. You can decide (on a per-index basis) whether to include archived documents in the index. This gives you a very high level of flexibility without requiring much manual effort. In short, for this scenario, you can simply tell RavenDB when to archive the document and let RavenDB handle the rest. RavenDB will do the right thing for you.&#xD;&#xA;&#xD;&#xA;&#x9;&#x9;&#x9;
        </p>
        <a href="https://ayende.com/blog/201441-B/cryptographically-impossible-bug-hunt" target="_blank"><h1 class="title mb-6">Cryptographically impossible bug hunt</h1></a>
        <p class="mb-2">by Oren Eini</p>
        <p class="mb-6 flex" style="gap: 0.3rem">
                <span>
                                <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                                     xmlns="http://www.w3.org/2000/svg"><path
                                        xmlns="http://www.w3.org/2000/svg"
                                        d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                </span>
            posted on: July 24, 2024
        </p>
        <p class="max-w-full" style="margin-bottom: 5rem; text-align: justify">
            &#xD;&#xA;&#x9;&#x9;&#x9;&#x9;I&amp;rsquo;m currently deep in the process of modifying the internals of Voron, trying to eke out more performance out of the system. I&amp;rsquo;m making great progress, but I&amp;rsquo;m also touching parts of the code that haven&amp;rsquo;t even been looked at for a long time. In other words, I&amp;rsquo;m mucking about with the most stable and most critical portions of the storage engine. It&amp;rsquo;s a lot of fun, and I&amp;rsquo;m actually seeing some great results, but it is also nerve-wracking. We have enough tests that I&amp;rsquo;ve great confidence I would catch any actual stability issues, but the drive back toward a fully green build has been a slog.The process is straightforward:Change something.Verify that it works better than before.Run the entire test suite (upward of 30K tests) to see if there are any breaks.The last part can be frustrating because it takes a while&amp;nbsp;to run this sort of test suite. That would be bad enough, but some of the changes I made were things like marking a piece of memory that used to be read/write as read-only. Now any access to that memory would result in an access violation. I fixed those in the code, of course, but we have a lot&amp;nbsp;of tests, including some tests that intentionally corrupt data to verify that RavenDB behaves properly under those conditions. One such test writes garbage to the RavenDB file, using read-write memory. The idea is to verify that the checksum matches on read and abort early. Because that test directly modifies what is now read-only memory, it generates a crash due to a memory access violation. That doesn&amp;rsquo;t just result in a test failure, it takes the whole process down.I&amp;rsquo;ve gotten pretty good at debugging those sorts of issues (--blame-crash&amp;nbsp;is fantastic) and was able to knock quite a few of them down and get them fixed. And then there was this test, which uses encryption-at-rest. That test started to fail after my changes, and I was pretty confused about exactly what was going on. When trying to read data from disk, it would follow up a pointer to an invalid location. That is not&amp;nbsp;supposed to happen, obviously. Looks like I have a little data corruption issue on my hands. The problem is that this shouldn&amp;rsquo;t be possible. Remember how we validate the checksum on read? When using encryption-at-rest, we are using a mechanism called AEAD (Authenticated Encryption with Associated Data). That means that in order to successfully decrypt a page of data from disk, it must have been cryptographically&amp;nbsp;verified to be valid.My test results showed, pretty conclusively, that I was generating valid data and then encrypting it. The next stage was to decrypt the data (verifying that it was valid), at which point I ended up with complete garbage.RavenDB trusts that since the data was properly decrypted, it is valid and tries to use it. Because the data is garbage, that leads to&amp;hellip; excitement. Once I realized what was going on, I was really confused. I&amp;rsquo;m pretty sure that I didn&amp;rsquo;t break 256-bit encryption, but I had a very clear chain of steps that led to valid data being decrypted (successfully!) to garbage. It was also quite frustrating to track because any small-stage test that I wrote would return the expected results. It was only when I ran the entire system and stressed&amp;nbsp;it that I got this weird scenario.I started practicing for my Fields medal acceptance speech while digging deeper. Something here had&amp;nbsp;to be wrong. It took me a while to figure out what was going on, but eventually, I tracked it down to registering to the TransactionCommit&amp;nbsp;event when we open a new file. The idea is that when we commit the transaction, we&amp;rsquo;ll encrypt all the data buffers and then write them to the file. We register for&amp;nbsp;an event to handle that, and we used to do that on a per-file basis. My changes, among other things, moved that logic to apply globally. As long as we were writing to a single file, everything just worked. When we had enough workload to need a second file, we would encrypt the data twice&amp;nbsp;and then write it to the file. Upon decryption, we would successfully decrypt the data but would end up with still encrypted data (looking like random fluff). The fix was simply moving the event registration to the transaction level, not the file level. I committed my changes and went back to the unexciting life of bug-fixing, rather than encryption-breaking and math-defying hacks.&#xD;&#xA;&#xD;&#xA;&#x9;&#x9;&#x9;
        </p>
        <a href="https://ayende.com/blog/201409-A/temporal-cattle-and-other-important-jargon" target="_blank"><h1 class="title mb-6">Temporal cattle and other important jargon</h1></a>
        <p class="mb-2">by Oren Eini</p>
        <p class="mb-6 flex" style="gap: 0.3rem">
                <span>
                                <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                                     xmlns="http://www.w3.org/2000/svg"><path
                                        xmlns="http://www.w3.org/2000/svg"
                                        d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                </span>
            posted on: July 15, 2024
        </p>
        <p class="max-w-full" style="margin-bottom: 5rem; text-align: justify">
            &#xD;&#xA;&#x9;&#x9;&#x9;&#x9;I was talking to a colleague about a particular problem we are trying to solve. He suggested that we solve the problem using a particular data structure from a recently published paper. As we were talking, he explained how this data structure works and how that should handle our problem.The solution was complex&amp;nbsp;and it took me a while to understand what it was trying to achieve and how it would fit our scenario. And then something clicked in my head and I said something like:Oh, that is just epoch-based, copy-on-write B&#x2B;Tree with single-producer/ concurrent-readers? If this sounds like nonsense to you, it is fine. Those are very&amp;nbsp;specific terms that we are using here. The point of such a discussion is that this sort of jargon serves a very important purpose. It allows us to talk with clarity and intent about fairly complex topics, knowing that both sides have the same understanding of what we are actually talking about.The idea is that we can elevate the conversation and focus on the differences&amp;nbsp;between what the jargon specifies and the topic at hand. This is abstraction at the logic level, where we can basically zoom out a lot of details and still keep high intent accuracy.Being able to discuss something at this level is hugely important because we can convey complex ideas easily. Once I managed to put what he was suggesting in context that I could understand, we were able to discuss the pros and cons of this data structure for the scenario. I do appreciate that the conversation basically stopped making sense to anyone who isn&amp;rsquo;t already well-versed in the topic as soon as we were able to (from my perspective) clearly and effectively communicate.&amp;ldquo;When I use a word,&amp;rsquo; Humpty Dumpty said in rather a scornful tone, &amp;lsquo;it means just what I choose it to mean &amp;mdash; neither more nor less.&amp;rdquo;Clarity of communication is a really important aspect of software engineering. Being able to explain, hopefully in a coherent fashion, why the software is built the way it is and why the code is structured just so can be really complex. Leaning on existing knowledge and understanding can make that a lot simpler.There is also another aspect. When using jargon like that, it is clear when you don&amp;rsquo;t&amp;nbsp;know something. You can go and research it. The mere fact that you can&amp;rsquo;t understand the text tells you both that you are missing information and where you can find it. For software, you need to consider two scenarios. Writing code today and explaining how it works to your colleagues, and looking at code that you wrote ten years ago and trying to figure out what was going on there.In both cases, I think that this sort of approach is a really useful way to convey information.&#xD;&#xA;&#xD;&#xA;&#x9;&#x9;&#x9;
        </p>
        <a href="https://ayende.com/blog/201377-A/does-code-rot-over-time" target="_blank"><h1 class="title mb-6">Does code rot over time?</h1></a>
        <p class="mb-2">by Oren Eini</p>
        <p class="mb-6 flex" style="gap: 0.3rem">
                <span>
                                <svg width="1.25rem" fill="currentColor" viewBox="0 0 24 24"
                                     xmlns="http://www.w3.org/2000/svg"><path
                                        xmlns="http://www.w3.org/2000/svg"
                                        d="M12 4C7.58172 4 4 7.58172 4 12C4 16.4183 7.58172 20 12 20C16.4183 20 20 16.4183 20 12C20 7.58172 16.4183 4 12 4ZM2 12C2 6.47715 6.47715 2 12 2C17.5228 2 22 6.47715 22 12C22 17.5228 17.5228 22 12 22C6.47715 22 2 17.5228 2 12ZM12 6C12.5523 6 13 6.44772 13 7V11.5858L15.7071 14.2929C16.0976 14.6834 16.0976 15.3166 15.7071 15.7071C15.3166 16.0976 14.6834 16.0976 14.2929 15.7071L11.2929 12.7071C11.1054 12.5196 11 12.2652 11 12V7C11 6.44772 11.4477 6 12 6Z"></path></svg>
                </span>
            posted on: July 10, 2024
        </p>
        <p class="max-w-full" style="margin-bottom: 5rem; text-align: justify">
            &#xD;&#xA;&#x9;&#x9;&#x9;&#x9;&amp;ldquo;This is Old Code&amp;rdquo; is a programmer&amp;rsquo;s idiom meaning &amp;ldquo;There Be Dragons&amp;rdquo;. &amp;nbsp;The term &amp;ldquo;Legacy Code&amp;rdquo; is a nice way to say &amp;ldquo;Don&amp;rsquo;t make me go there&amp;rdquo; Those are very strange statements when you think about it. &amp;nbsp;Code is code, just ones &amp;amp; zeros stored on a disk somewhere. It doesn&amp;rsquo;t go bad over time.When you write a line of code, it doesn&amp;rsquo;t have an expiration date, after all. For food, it makes sense, there are bacteria and such that would make it go bad. But what is it about old code that is so problematic?I want to take a look at a couple of examples of old code and examine how they stood the test of time. &amp;nbsp;I chose those two projects because there has been no activity on either project since about 2014 or so. No meaningful activity or changes for the past decade is a great place to start looking at code rots. Note that I&amp;rsquo;m not looking at the age&amp;nbsp;of a codebase, but whether it was left to pasture long enough to exhibit code rot issues.Rhino.Mocks&amp;nbsp;is a mocking framework for .NET that I spent close to a decade working on. It was highly&amp;nbsp;popular for several years and was quite capable. The vast majority of the code, written about 15 years ago, is now frozen, and I haven&amp;rsquo;t touched it since around 2016.I was able to clone the Rhino Mocks repository, run the build script and the tests in a single command. However&amp;hellip; trying to actually use this in a modern system would result in an error similar to this one:Method not found: &#x27;System.Reflection.Emit.AssemblyBuilder System.AppDomain.DefineDynamicAssembly(System.Reflection.AssemblyName, System.Reflection.Emit.AssemblyBuilderAccess)&#x27;.&#x27;Internally, Rhino Mocks does dynamic code generation, which relies on very low level APIs. Apparently, these APIs are not consistent between .NET Framework and .NET Core / the new .NET. To get Rhino Mocks working on the current version of .NET, we would need to actually fix those issues. That would require someone who understands how dynamic code generation and IL emitting work. I remember facing a lot&amp;nbsp;of InvalidProgramException in the past, so that isn&amp;rsquo;t a generally applicable skill. ALICE&amp;nbsp;is a tool for checking the crash correctness of databases and similar applications. It has a really interesting paper&amp;nbsp;associated with it and was used to find several consistency issues with many systems (from databases to Git and Mercurial). The code was last touched in 2015 but the actual work appears to have happened just over ten years ago. ALICE made quite a splash when it came out, and many projects tried to test it against themselves to see if there were any issues with their usage of the file system APIs. Trying to run ALICE today, you&amp;rsquo;ll run into several problems. It uses Python 2.x, which is no longer supported. Moving it to Python 3.x was not a big deal, but a much bigger problem is that ALICE is very closely tied to the syscalls of the kernel (it monitors them to see how the application uses the file system API). Since ALICE was released, new syscalls were introduced, and the actual I/O landscape has changed quite a bit (for example, with IO_Uring). Making it work, even for a relatively small test case, was not a trivial task.The most interesting aspect of this investigation was not the particular problems that I found, but actually figuring out what is the process of addressing them. Just updating the code to the latest version is a mechanical process that is pretty easy.Updating the actual behavior, however, would require a high degree of expertise. Furthermore, it would also need a good understanding and insight into the codebase and its intended effects. A codebase that hasn&amp;rsquo;t been touched in a long while is unlikely to have such insight. When we talk about a codebase rotting, we aren&amp;rsquo;t referring to the source files picking up viruses or the like, we are discussing the loss of information about what the code is actually doing. Even worse, even if we can follow what the code is doing, understanding how to modify it is a high-complexity task. What about ongoing projects? Projects that have continuous updates and dedicated team members associated with them. It turns out that they can rot as well. Here is an example taken from the RavenDB codebase. This is a pretty important method as it adds an item to a B&#x2B;Tree, which is quite a common (and important) operation in a database:You can see that this isn&amp;rsquo;t much of a function, most of the behavior happens elsewhere. However, you can see that this code has been around for a while. It was modified by four different people over the span of a decade. It is also pretty stable code, in terms of the number of changes that happened there.This is a small function, but you can see it pretty clearly when you are looking at the code at large. There are whole sections that are just&amp;hellip; there. They are functional and work well, and no one needs to touch them for a very long period of time. Occasionally, we make minor changes, but for the most part, they are not touched much at all. How does that&amp;nbsp;play into the notion of code rot? The code wouldn&amp;rsquo;t suffer as badly as the examples above, of course, since it is still being run and executed on an ongoing basis. However, the understanding of the code is definitely diminished. The question is, do we care? Those are the stable&amp;nbsp;parts, the ones we don&amp;rsquo;t need to touch. Until we do&amp;hellip; that is, and what happens then?Just making changes in our codebase for the sake of making changes is a bad idea. But going into the codebase and leaving it in a better state than before is a good practice. This helps ensure it doesn&amp;rsquo;t become a daunting &amp;lsquo;there be dragons&amp;rsquo; scenario.&#xD;&#xA;&#xD;&#xA;&#x9;&#x9;&#x9;
        </p>
        <div class="button flex justify-between">
            <a href="35.html"><span class="back arrow"></span></a>

            <a href="37.html"><span class="next arrow"></span></a>
        </div>
    </section>
</main>
<footer
    class="mt-auto flex w-full flex-col items-center justify-center gap-y-2 pb-4 pt-20 text-center align-top font-semibold text-gray-600 dark:text-gray-400 sm:flex-row sm:justify-between sm:text-xs">
    <div class="me-0 sm:me-4">
        <div class="flex flex-wrap items-end gap-x-2">
            <ul class="flex flex-1 items-center gap-x-2 sm:flex-initial">
                <li class="flex">
                    <p class="flex items-end gap-2 justify-center flex-wrap	">Â© Relatively General
                        .NET 2024<span
                            class="inline-block">&nbsp;ðŸš€&nbsp;Theme: Astro Cactus</span>

                        <a class="inline-block sm:hover:text-link" href="https://github.com/chrismwilliams/astro-cactus"
                           rel="noopener noreferrer " target="_blank">
                            <svg width="1em" height="1em" viewBox="0 0 24 24" aria-hidden="true" class="h-6 w-6"
                                 focusable="false" data-icon="mdi:github">
                                <symbol id="ai:mdi:github">
                                    <path fill="currentColor"
                                          d="M12 2A10 10 0 0 0 2 12c0 4.42 2.87 8.17 6.84 9.5c.5.08.66-.23.66-.5v-1.69c-2.77.6-3.36-1.34-3.36-1.34c-.46-1.16-1.11-1.47-1.11-1.47c-.91-.62.07-.6.07-.6c1 .07 1.53 1.03 1.53 1.03c.87 1.52 2.34 1.07 2.91.83c.09-.65.35-1.09.63-1.34c-2.22-.25-4.55-1.11-4.55-4.92c0-1.11.38-2 1.03-2.71c-.1-.25-.45-1.29.1-2.64c0 0 .84-.27 2.75 1.02c.79-.22 1.65-.33 2.5-.33s1.71.11 2.5.33c1.91-1.29 2.75-1.02 2.75-1.02c.55 1.35.2 2.39.1 2.64c.65.71 1.03 1.6 1.03 2.71c0 3.82-2.34 4.66-4.57 4.91c.36.31.69.92.69 1.85V21c0 .27.16.59.67.5C19.14 20.16 22 16.42 22 12A10 10 0 0 0 12 2"></path>
                                </symbol>
                                <use xlink:href="#ai:mdi:github"></use>
                            </svg>
                            <span class="sr-only">Github</span>
                        </a>
                    </p>
                </li>
            </ul>
        </div>
    </div>
    <nav aria-label="More on this site" class="flex gap-x-2 sm:gap-x-0 sm:divide-x sm:divide-gray-500">
        <a class="px-4 py-2 sm:py-0 sm:hover:text-textColor sm:hover:underline" href="index.html"> Home </a><a
            class="px-4 py-2 sm:py-0 sm:hover:text-textColor sm:hover:underline" href="/about/"> About </a>
    </nav>
</footer>
</body>
</html>